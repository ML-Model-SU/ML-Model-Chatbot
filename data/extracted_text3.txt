INTRODUCTION TO MACHINE LEARNING

Introduction to Machine Learning Alex Smola and S.V.N. Vishwanathan Yahoo! Labs Santa Clara {and{ Departments of Statistics and Computer Science Purdue University {and{ College of Engineering and Computer Science Australian National University

published by the press syndicate of the university of cambridge The Pitt Building, Trumpington Street, Cambridge, United Kingdom cambridge university press The Edinburgh Building, Cambridge CB2 2RU, UK 40 West 20th Street, New York, NY 10011{4211, USA 477 Williamstown Road, Port Melbourne, VIC 3207, Australia Ruiz de Alarc on 13, 28014 Madrid, Spain Dock House, The Waterfront, Cape Town 8001, South Africa http://www.cambridge.org cCambridge University Press 2008 This book is in copyright. Subject to statutory exception and to the provisions of relevant collective licensing agreements, no reproduction of any part may take place without the written permission of Cambridge University Press. First published 2008 Printed in the United Kingdom at the University Press, Cambridge Typeface Monotype Times 10/13pt System LATEX 2"[Alexander J. Smola and S.V.N. Vishwanathan] A catalogue record for this book is available from the British Library Library of Congress Cataloguing in Publication data available ISBN 0 521 82583 0 hardback Author: vishy Revision: 252 Timestamp: October 1, 2010 URL: svn://smola@repos.stat.purdue.edu/thebook/trunk/Book/thebook.tex

Contents Preface page 1 1 Introduction 3 1.1 A Taste of Machine Learning 3 1.1.1 Applications 3 1.1.2 Data 7 1.1.3 Problems 9 1.2 Probability Theory 12 1.2.1 Random Variables 12 1.2.2 Distributions 13 1.2.3 Mean and Variance 15 1.2.4 Marginalization, Independence, Conditioning, and Bayes Rule 16 1.3 Basic Algorithms 20 1.3.1 Naive Bayes 22 1.3.2 Nearest Neighbor Estimators 24 1.3.3 A Simple Classier 27 1.3.4 Perceptron 29 1.3.5 K-Means 32 2 Density Estimation 37 2.1 Limit Theorems 37 2.1.1 Fundamental Laws 38 2.1.2 The Characteristic Function 42 2.1.3 Tail Bounds 45 2.1.4 An Example 48 2.2 Parzen Windows 51 2.2.1 Discrete Density Estimation 51 2.2.2 Smoothing Kernel 52 2.2.3 Parameter Estimation 54 2.2.4 Silverman's Rule 57 2.2.5 Watson-Nadaraya Estimator 59 2.3 Exponential Families 60 2.3.1 Basics 60 v

vi 0 Contents 2.3.2 Examples 62 2.4 Estimation 66 2.4.1 Maximum Likelihood Estimation 66 2.4.2 Bias, Variance and Consistency 68 2.4.3 A Bayesian Approach 71 2.4.4 An Example 75 2.5 Sampling 77 2.5.1 Inverse Transformation 78 2.5.2 Rejection Sampler 82 3 Optimization 91 3.1 Preliminaries 91 3.1.1 Convex Sets 92 3.1.2 Convex Functions 92 3.1.3 Subgradients 96 3.1.4 Strongly Convex Functions 97 3.1.5 Convex Functions with Lipschitz Continous Gradient 98 3.1.6 Fenchel Duality 98 3.1.7 Bregman Divergence 100 3.2 Unconstrained Smooth Convex Minimization 102 3.2.1 Minimizing a One-Dimensional Convex Function 102 3.2.2 Coordinate Descent 104 3.2.3 Gradient Descent 104 3.2.4 Mirror Descent 108 3.2.5 Conjugate Gradient 111 3.2.6 Higher Order Methods 115 3.2.7 Bundle Methods 121 3.3 Constrained Optimization 125 3.3.1 Projection Based Methods 125 3.3.2 Lagrange Duality 127 3.3.3 Linear and Quadratic Programs 131 3.4 Stochastic Optimization 135 3.4.1 Stochastic Gradient Descent 136 3.5 Nonconvex Optimization 137 3.5.1 Concave-Convex Procedure 137 3.6 Some Practical Advice 139 4 Online Learning and Boosting 143 4.1 Halving Algorithm 143 4.2 Weighted Majority 144

Contents vii 5 Conditional Densities 149 5.1 Logistic Regression 150 5.2 Regression 151 5.2.1 Conditionally Normal Models 151 5.2.2 Posterior Distribution 151 5.2.3 Heteroscedastic Estimation 151 5.3 Multiclass Classication 151 5.3.1 Conditionally Multinomial Models 151 5.4 What is a CRF? 152 5.4.1 Linear Chain CRFs 152 5.4.2 Higher Order CRFs 152 5.4.3 Kernelized CRFs 152 5.5 Optimization Strategies 152 5.5.1 Getting Started 152 5.5.2 Optimization Algorithms 152 5.5.3 Handling Higher order CRFs 152 5.6 Hidden Markov Models 153 5.7 Further Reading 153 5.7.1 Optimization 153 6 Kernels and Function Spaces 155 6.1 The Basics 155 6.1.1 Examples 156 6.2 Kernels 161 6.2.1 Feature Maps 161 6.2.2 The Kernel Trick 161 6.2.3 Examples of Kernels 161 6.3 Algorithms 161 6.3.1 Kernel Perceptron 161 6.3.2 Trivial Classier 161 6.3.3 Kernel Principal Component Analysis 161 6.4 Reproducing Kernel Hilbert Spaces 161 6.4.1 Hilbert Spaces 163 6.4.2 Theoretical Properties 163 6.4.3 Regularization 163 6.5 Banach Spaces 164 6.5.1 Properties 164 6.5.2 Norms and Convex Sets 164 7 Linear Models 165 7.1 Support Vector Classication 165

viii 0 Contents 7.1.1 A Regularized Risk Minimization Viewpoint 170 7.1.2 An Exponential Family Interpretation 170 7.1.3 Specialized Algorithms for Training SVMs 172 7.2 Extensions 177 7.2.1 The trick 177 7.2.2 Squared Hinge Loss 179 7.2.3 Ramp Loss 180 7.3 Support Vector Regression 181 7.3.1 Incorporating General Loss Functions 184 7.3.2 Incorporating the Trick 186 7.4 Novelty Detection 186 7.5 Margins and Probability 189 7.6 Beyond Binary Classication 189 7.6.1 Multiclass Classication 190 7.6.2 Multilabel Classication 191 7.6.3 Ordinal Regression and Ranking 192 7.7 Large Margin Classiers with Structure 193 7.7.1 Margin 193 7.7.2 Penalized Margin 193 7.7.3 Nonconvex Losses 193 7.8 Applications 193 7.8.1 Sequence Annotation 193 7.8.2 Matching 193 7.8.3 Ranking 193 7.8.4 Shortest Path Planning 193 7.8.5 Image Annotation 193 7.8.6 Contingency Table Loss 193 7.9 Optimization 193 7.9.1 Column Generation 193 7.9.2 Bundle Methods 193 7.9.3 Overrelaxation in the Dual 193 7.10 CRFs vs Structured Large Margin Models 194 7.10.1 Loss Function 194 7.10.2 Dual Connections 194 7.10.3 Optimization 194 Appendix 1 Linear Algebra and Functional Analysis 197 Appendix 2 Conjugate Distributions 201 Appendix 3 Loss Functions 203 Bibliography 221

Preface Since this is a textbook we biased our selection of references towards easily accessible work rather than the original references. While this may not be in the interest of the inventors of these concepts, it greatly simplies access to those topics. Hence we encourage the reader to follow the references in the cited works should they be interested in nding out who may claim intellectual ownership of certain key ideas. 1

2 0 Preface Structure of the Book IntroductionDensity EstimationGraphical ModelsKernelsOptimizationConditional DensitiesConditional Random FieldsLinear ModelsStructured EstimationDuality and EstimationMomentMethodsReinforcement Learning IntroductionDensity EstimationGraphical ModelsKernelsOptimizationConditional DensitiesConditional Random FieldsLinear ModelsStructured EstimationDuality and EstimationMomentMethodsReinforcement Learning IntroductionDensity EstimationGraphical ModelsKernelsOptimizationConditional DensitiesConditional Random FieldsLinear ModelsStructured EstimationDuality and EstimationMomentMethodsReinforcement Learning Canberra, August 2008

1 Introduction Over the past two decades Machine Learning has become one of the main- stays of information technology and with that, a rather central, albeit usually hidden, part of our life. With the ever increasing amounts of data becoming available there is good reason to believe that smart data analysis will become even more pervasive as a necessary ingredient for technological progress. The purpose of this chapter is to provide the reader with an overview over the vast range of applications which have at their heart a machine learning problem and to bring some degree of order to the zoo of problems. After that, we will discuss some basic tools from statistics and probability theory, since they form the language in which many machine learning problems must be phrased to become amenable to solving. Finally, we will outline a set of fairly basic yet eective algorithms to solve an important problem, namely that of classication. More sophisticated tools, a discussion of more general problems and a detailed analysis will follow in later parts of the book. 1.1 A Taste of Machine Learning Machine learning can appear in many guises. We now discuss a number of applications, the types of data they deal with, and nally, we formalize the problems in a somewhat more stylized fashion. The latter is key if we want to avoid reinventing the wheel for every new application. Instead, much of the artof machine learning is to reduce a range of fairly disparate problems to a set of fairly narrow prototypes. Much of the science of machine learning is then to solve those problems and provide good guarantees for the solutions. 1.1.1 Applications Most readers will be familiar with the concept of web page ranking . That is, the process of submitting a query to a search engine, which then nds webpages relevant to the query and which returns them in their order of relevance. See e.g. Figure 1.1 for an example of the query results for \ma- chine learning". That is, the search engine returns a sorted list of webpages given a query. To achieve this goal, a search engine needs to `know' which 3

4 1 Introduction Web Images Maps News Shopping Gmail more !     Sponsored LinksMachine LearningGoogle Sydney needs machinelearning experts. Apply today!www.google.com.au/jobsSign in  Search  Advanced Search  Preferences Web    Scholar   Results 1 - 10 of about 10,500,000 for machine learning. (0.06 seconds) Machine learning - Wikipedia, the free encyclopediaAs a broad subfield of artificial intelligence, machine learning is concerned with the designand development of algorithms and techniques that allow ...en.wikipedia.org/wiki/Machine_learning - 43k - Cached - Similar pagesMachine Learning textbookMachine Learning is the study of computer algorithms that improve automatically throughexperience. Applications range from datamining programs that ...www.cs.cmu.edu/~tom/mlbook.html - 4k - Cached - Similar pagesmachine learningwww.aaai.org/AITopics/html/machine.html - Similar pagesMachine LearningA list of links to papers and other resources on machine learning.www.machinelearning.net/ - 14k - Cached - Similar pagesIntroduction to Machine LearningThis page has pointers to my draft book on Machine Learning and to its individualchapters. They can be downloaded in Adobe Acrobat format. ...ai.stanford.edu/~nilsson/mlbook.html - 15k - Cached - Similar pagesMachine Learning - Artificial Intelligence (incl. Robotics ...Machine Learning - Artificial Intelligence. Machine Learning is an international forum forresearch on computational approaches to learning.www.springer.com/computer/artificial/journal/10994 - 39k - Cached - Similar pagesMachine Learning (Theory)Graduating students in Statistics appear to be at a substantial handicap compared tograduating students in Machine Learning, despite being in substantially ...hunch.net/ - 94k - Cached - Similar pagesAmazon.com: Machine Learning: Tom M. Mitchell: BooksAmazon.com: Machine Learning: Tom M. Mitchell: Books.www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077 - 210k -Cached - Similar pagesMachine Learning JournalMachine Learning publishes articles on the mechanisms through which intelligent systemsimprove their performance over time. We invite authors to submit ...pages.stern.nyu.edu/~fprovost/MLJ/ - 3k - Cached - Similar pagesCS 229: Machine LearningSTANFORD. CS229 Machine Learning Autumn 2007. Announcements. Final reports fromthis year's class projects have been posted here. ...cs229.stanford.edu/ - 10k - Cached - Similar pages12345678910Next   SearchSearch within results | Language Tools | Search Tips | Dissatisfied? Help us improve | Try Google ExperimentalÂ©2008 Google - Google Home - Advertising Programs - Business Solutions - About Googlemachine learning machine learningGoogle Fig. 1.1. The 5 top scoring webpages for the query \machine learning" pages are relevant and which pages match the query. Such knowledge can be gained from several sources: the link structure of webpages, their content, the frequency with which users will follow the suggested links in a query, or from examples of queries in combination with manually ranked webpages. Increasingly machine learning rather than guesswork and clever engineering is used to automate the process of designing a good search engine [RPB06]. A rather related application is collaborative ltering . Internet book- stores such as Amazon, or video rental sites such as Netix use this informa- tion extensively to entice users to purchase additional goods (or rent more movies). The problem is quite similar to the one of web page ranking. As before, we want to obtain a sorted list (in this case of articles). The key dif- ference is that an explicit query is missing and instead we can only use past purchase and viewing decisions of the user to predict future viewing and purchase habits. The key side information here are the decisions made by similar users, hence the collaborative nature of the process. See Figure 1.2 for an example. It is clearly desirable to have an automatic system to solve this problem, thereby avoiding guesswork and time [BK07]. An equally ill-dened problem is that of automatic translation of doc- uments. At one extreme, we could aim at fully understanding a text before translating it using a curated set of rules crafted by a computational linguist well versed in the two languages we would like to translate. This is a rather arduous task, in particular given that text is not always grammatically cor- rect, nor is the document understanding part itself a trivial one. Instead, we could simply use examples of translated documents, such as the proceedings of the Canadian parliament or other multilingual entities (United Nations, European Union, Switzerland) to learn how to translate between the two

1.1 A Taste of Machine Learning 5 languages. In other words, we could use examples of translations to learn how to translate. This machine learning approach proved quite successful [?]. Many security applications, e.g. for access control, use face recognition as one of its components. That is, given the photo (or video recording) of a person, recognize who this person is. In other words, the system needs to classify the faces into one of many categories (Alice, Bob, Charlie, . . . ) or decide that it is an unknown face. A similar, yet conceptually quite dierent problem is that of verication. Here the goal is to verify whether the person in question is who he claims to be. Note that dierently to before, this is now a yes/no question. To deal with dierent lighting conditions, facial expressions, whether a person is wearing glasses, hairstyle, etc., it is desirable to have a system which learns which features are relevant for identifying a person. Another application where learning helps is the problem of named entity recognition (see Figure 1.4). That is, the problem of identifying entities, such as places, titles, names, actions, etc. from documents. Such steps are crucial in the automatic digestion and understanding of documents. Some modern e-mail clients, such as Apple's Mail.app nowadays ship with the ability to identify addresses in mails and ling them automatically in an address book. While systems using hand-crafted rules can lead to satisfac- tory results, it is far more ecient to use examples of marked-up documents to learn such dependencies automatically, in particular if we want to de- ploy our system in many languages. For instance, while 'bush' and 'rice' Your Amazon.com  Today's DealsGifts & Wish Lists Gift Cards Your Account  |  Help Advertise on Amazon 5 star: (23)4 star: (2)3 star: (3)2 star: (2)1 star:  (0)    Quantity:  1  orSign in to turn on 1-Click ordering.    More Buying Choices16 used & new from$52.00Have one to sell?         Share your own customer imagesSearch inside another edition of this bookAre You an Author orPublisher? Find out how to publishyour own Kindle Books   Hello. Sign in to get personalized recommendations. New customer? Start here.    Books   BooksAdvanced SearchBrowse SubjectsHot New ReleasesBestsellersThe New York TimesÂ® Best SellersLibros En EspaÃ±olBargain BooksTextbooksJoin Amazon Prime and ship Two-Day for free and Overnight for $3.99. Already a member? Sign in.Machine Learning (Mcgraw-Hill International Edit)(Paperback)by Thomas Mitchell (Author) "Ever since computers were invented, we have wondered whetherthey might be made to learn..." (more)      (30 customer reviews)  List Price:$87.47Price:$87.47 & this item ships for FREE with Super Saver Shipping.DetailsAvailability: Usually ships within 4 to 7 weeks. Ships from and sold by Amazon.com. Gift-wrap available.16 used & new available from $52.00Also Available in:List Price:Our Price:Other Offers:Hardcover (1)$153.44$153.4434 used & new from $67.00   Better TogetherBuy this book with Introduction to Machine Learning (Adaptive Computation and Machine Learning) by Ethem Alpaydin today!Buy Together Today: $130.87 Customers Who Bought This Item Also Bought Pattern Recognition andMachine Learning(Information Science andStatistics) by ChristopherM. Bishop  (30)  $60.50 Artificial Intelligence: AModern Approach (2ndEdition) (Prentice HallSeries in ArtificialIntelligence) by StuartRussell  (76)  $115.00 The Elements of StatisticalLearning by T. Hastie  (25)  $72.20 Pattern Classification (2ndEdition) by Richard O.Duda  (25)  $115.00 Data Mining: PracticalMachine Learning Toolsand Techniques, SecondEdition (Morgan KaufmannSeries in DataManagement Systems) byIan H. Witten  (21)  $39.66â€º Explore similar items : Books (50)Editorial ReviewsBook DescriptionThis exciting addition to the McGraw-Hill Series in Computer Science focuses on the concepts and techniques that contribute to the rapidlychanging field of machine learning--including probability and statistics, artificial intelligence, and neural networks--unifying them all in a logicaland coherent manner. Machine Learning serves as a useful reference tool for software developers and researchers, as well as an outstanding textfor college students. --This text refers to the Hardcover edition. Book InfoPresents the key algorithms and theory that form the core of machine learning. Discusses such theoretical issues as How does learningperformance vary with the number of training examples presented? and Which learning algorithms are most appropriate for various types oflearning tasks? DLC: Computer algorithms. --This text refers to the Hardcover edition.Product DetailsPaperback: 352 pagesPublisher: McGraw-Hill Education (ISE Editions); 1st edition (October 1, 1997)Language: EnglishISBN-10: 0071154671ISBN-13: 978-0071154673Product Dimensions: 9 x 5.9 x 1.1 inchesShipping Weight: 1.2 pounds (View shipping rates and policies)Average Customer Review:     (30 customer reviews)Amazon.com Sales Rank: #104,460 in Books (See Bestsellers in Books)Popular in this category: (What's this?)#11 in Books > Computers & Internet > Computer Science > Artificial Intelligence > Machine Learning(Publishers and authors: Improve Your Sales)In-Print Editions: Hardcover (1) |  All Editions Would you like to update product info or give feedback on images? (We'll ask you to sign in so we can get back to you)Inside This Book (learn more) Browse and search another edition of this book.First Sentence:Ever since computers were invented, we have wondered whether they might be made to learn. Read the first pageBrowse Sample Pages:Front Cover | Copyright | Table of Contents | Excerpt | Index | Back Cover | Surprise Me!Search Inside This Book:  Customers viewing this page may be interested in these Sponsored Links (What's this?)Online Law Degreehttp://www.edu-onlinedegree.org Juris Doctor JD & LLM Masters Low tuition, Free Textbooks Learning CDswww.mindperk.com Save on powerful mind-boosting CDs & DVDs. Huge Selection Video Edit Magicwww.deskshare.com/download Video Editing Software trim, modify color, and merge video Tags Customers Associate with This Product (What's this?)Click on a tag to find related items, discussions, and people.machine learning (6)artificial intelligence (2)computer science (1)pattern recognition (1)Your tags: Add your first tagHelp others find this product - tag it for Amazon searchNo one has tagged this product for Amazon search yet. Why not be the first tosuggest a search for which it should appear?Search Products Tagged with  Are you the publisher or author? Learn how Amazon can help you make this book an eBook. If you are a publisher or author and hold the digital rights to a book, you can make it available as an eBook on Amazon.com. Learn moreRate This Item to Improve Your RecommendationsI own itNot ratedYour ratingDon't like it < > I love it!Save yourrating   ? 1 2 3 4 5  Customer Reviews30 Reviews  Average Customer Review (30 customer reviews)    Share your thoughts with other customers: Most Helpful Customer Reviews 44 of 44 people found the following review helpful:  An excellent overview for the adv. undergrad or beg. grad,September 30, 2002By Todd Ebert (Long Beach California) - See all my reviewsThis review is from: Machine Learning (Hardcover)I agree with some of the previous reviews which criticize the book for its lack ofdepth, but I believe this to be an asset rather than a liability given its targetaudience (seniors and beginning grad. students). The average college senior typicallyknows very little about subjects like neural networks, genetic algorithms, or Baysiannetworks, and this book goes a long way in demystifying these subjects in a veryclear, concise, and understandable way. Moreover, the first-year grad. student who isinterested in possibly doing research in this field needs more of an overview than todive deeply into one of the many branches which themselves have had entire books written aboutthem. This is one of the few if only books where one will find diverse areas oflearning (e.g. analytical, reinforcment, Bayesian, neural-network, genetic-algorithmic)all within the same cover.But more than just an encyclopedic introduction, the author makes a number ofconnections between the different paradigms. For example, he explains thatassociated with each paradigm is the notion of an inductive-learning bias, i.e. theunderlying assumptions that lend validity to a given learning approach. These end-of-chapter discussions on bias seem very interesting and unique to this book.Finally, I used this book for part of the reading material for an intro. AI class, andreceived much positive feedback from the students, although some did find thepresentation a bit too abstract for their undergraduate tastes  Comment | Permalink | Was this review helpful to you?    (Report this)  22 of 27 people found the following review helpful:  Great compilation, May 18, 2001By Steven Burns (-) - See all my reviewsThis review is from: Machine Learning (Hardcover)This book is completely worth the price, and worth the hardcover to take care of it.The main chapters of the book are independent, so you can read them in any order.The way it explains the different learning approaches is beautiful because: 1)itexplains them nicely 2)it gives examples and 3)it presents pseudocode summaries ofthe algorithms. As a software developer, what else could I possibly ask for?  Comment | Permalink | Was this review helpful to you?    (Report this)  23 of 23 people found the following review helpful:  Venerable, in both senses, April 4, 2004By eldil (Albuquerque NM) - See all my reviewsThis review is from: Machine Learning (Hardcover)It's pretty well done, it covers theory and core areas but - maybe it was more thestate of the field when it was written - I found it unsatisfyingly un-synthesized,unconnected, and short of detail (but this is subjective). I found the 2nd edition ofRussell and Norvig to be a better introduction where it covers the same topic, whichit does for everything I can think of, except VC dimension.The book sorely needs an update, it was written in 1997 and the field has movedfast. A comparison with Mitchell's current course (materials generously availableonline) shows that about 1/4 of the topics taught have arisen since the book waspublished; Boosting, Support Vector Machines and Hidden Markov Models to namethe best-known. The book also does not cover statistical or data mining methods.Despite the subjective complaint about lack of depth it does give the theoreticalroots and many fundamental techniques decently and readably. For many purposesthough it may have been superceded by R&N 2nd ed.  Comment | Permalink | Was this review helpful to you?    (Report this) Share your thoughts with other customers:  â€º See all 30 customer reviews...  Most Recent Customer Reviews  OutstandingI read this book about 7 years ago while inthe PhD program at Stanford University. Iconsider this book not only the bestMachine Learning book, but one of the bestbooks in all... Read morePublished 6 months ago by Husam Abu-Haimed  Great Start to Machine LearningI have used this book during my mastersand found it to be an extremely helpful anda gentle introduction to the thick and thingsof machine learning applications.Read morePublished 6 months ago by Subrat Nanda  Best book I've seen on topicI have this book listed as one of the bestand most interesting I've ever read. I lovedthe book just as much as I loved the coursewe used it in. Read morePublished 13 months ago by Lars Kristensson  too expensive I would saygreat book if you wanna start sth anywherein machine learning, but it is tooooooexpensive.Published 17 months ago by X. Wu  Excellent book, concise andreadableThis is a great book if you're starting outwith machine learning. It's rare to comeacross a book like this that is very wellwritten and has technical depth. Read morePublished 20 months ago by Part Time Reader  great bookThis is a great book because it focuses onmachine learning techniques. It has beenused as textbook in my class.Published on November 11, 2005 by Jay  Great introduction book forstudents in data mining and machinelearning classAlthough this text book is not required inmy data mining class, but I found it is veryhelpful for my study. Read morePublished on October 24, 2005 by Thanh Doan  Excellently writtenI am using this textbook for a MachineLearning class. While my professor isexcellent, I must say that this book is awelcome addition to class. Read morePublished on October 12, 2005 by Gregor Kronenberger  Just a brief introduction to ML...First of all, the statistical part of machinelearning is JUST a real subset ofmathematical statisitcs, whatever Bayesianor frequentist. Read morePublished on September 12, 2005 by supercutepig  Excellent reference bookI liked the book. But I think author mustprovide more figures in the book like Dudaand Hart's Pattern Classification book.Read morePublished on December 25, 2004 by Fatih NarSearch Customer Reviews  Only search this product's reviewsâ€º See all 30 customer reviews...  Customer Discussions Beta (What's this?)New! See recommended Discussions for YouThis product's forum (0 discussions)DiscussionRepliesLatest PostNo discussions yetAsk questions, Share opinions, Gain insightStart a new discussionTopic:   Related forumsmachine learning (start the discussion)artificial intelligence  (1 discussion)Product Information from the Amapedia Community Beta (What's this?)Be the first person to add an article about this item at Amapedia.com.  â€º See featured Amapedia.com articles  Listmania!  Machine Learning and Graphs: A list by J. Chan "PhD Student (ComputerScience)"  Bayesian Network Books: A list by Tincture Of Iodine "TOI"  Books on Algorithms on a variety of topics: A list by calvinnme "Texan refugee"Create a Listmania! listSearch Listmania! So You'd Like to...  Learn Advanced Mathematics on Your Own: A guide by Gal Gross "Wir mÃ¼ssenwissen, wir werden wissen. - David Hilbert"  Learn more about Artificial Intelligence (AI) and Games: A guide by John Funge  study curriculum of B.S. computer science (honors mode): A guide by"josie_roberts"Create a guideSearch Guides Look for Similar Items by CategoryComputers & Internet > Computer Science > Artificial Intelligence > Machine LearningLook for Similar Items by Subject  Machine learning  Computer Books: General Find books matching ALL checked subjects i.e., each book must be in subject 1 AND subject 2 AND ... Harry Potter StoreOur HarryPotterStorefeaturesall thingsHarry,includingbooks, audio CDs andcassettes, DVDs,soundtracks, and more. Got Your Neti Pot?Give yoursinuses abath withone of the many netipots in our Health &Personal Care Store.â€ºSee more Drop It Like It'sWaterproofAndshockproof,crushproof,andfreezeproof. All that, inaddition to 7-megapixelresolution and BrightCapture technology,makes the OlympusStylus 770SW theperfect vacationcompanion. Plus, it's nowavailable for only$289.94 fromAmazon.com. Editors' Faves inBooksSave40%on TheSignificant 7, our favoritepicks for the month.     Feedback   If you need help or have a question for Customer Service, contact us.  Would you like to update product info or give feedback on images? (We'll ask you to sign in so we can get back to you)  Is there any other feedback you would like to provide? Click hereWhere's My Stuff?Track your recent orders.View or change your orders in Your Account.Shipping & ReturnsSee our shipping rates & policies.Return an item (here's our Returns Policy).Need Help?Forgot your password? Click here.Redeem or buy a gift certificate/card.Visit our Help department.Search  Amazon.com     Your Recent History (What's this?) Recently Viewed ProductsAfter viewing product detail pages or search results, look here to find an easy way to navigate back to pages you are interested in.Look to the right column to find helpful suggestions for your shopping session.â€º View & edit Your Browsing History     Amazon.com Home  |   Directory of All StoresInternational Sites:  Canada  |  United Kingdom  |  Germany  |  Japan  |  France  |  ChinaHelp  |  View Cart  |  Your Account  |  Sell Items  |  1-Click Settings           Fig. 1.2. Books recommended by Amazon.com when viewing Tom Mitchell's Ma- chine Learning Book [Mit97]. It is desirable for the vendor to recommend relevant books which a user might purchase. Fig. 1.3. 11 Pictures of the same person taken from the Yale face recognition database. The challenge is to recognize that we are dealing with the same per- son in all 11 cases.

6 1 Introduction HAVANA (Reuters) - The European Union's top development aid official left Cuba on Sunday convinced that EU diplomatic sanctions against the communist island should be dropped after Fidel Castro's retirement, his main aide said. <TYPE="ORGANIZATION" >HAVANA</>(<TYPE="ORGANIZATION" >Reuters</>) - The <TYPE="ORGANIZATION" >European Union </>'s top development aid official left <TYPE="ORGANIZATION" >Cuba</>on Sunday convinced that EU diplomatic sanctions against the communist <TYPE="LOCATION" >island</>should be dropped after <TYPE="PERSON" >Fidel Castro </>'s retirement, his main aide said. Fig. 1.4. Named entity tagging of a news article (using LingPipe). The relevant locations, organizations and persons are tagged for further information extraction. are clearly terms from agriculture, it is equally clear that in the context of contemporary politics they refer to members of the Republican Party. Other applications which take advantage of learning are speech recog- nition (annotate an audio sequence with text, such as the system shipping with Microsoft Vista), the recognition of handwriting (annotate a sequence of strokes with text, a feature common to many PDAs), trackpads of com- puters (e.g. Synaptics, a major manufacturer of such pads derives its name from the synapses of a neural network), the detection of failure in jet en- gines, avatar behavior in computer games (e.g. Black and White), direct marketing (companies use past purchase behavior to guesstimate whether you might be willing to purchase even more) and oor cleaning robots (such as iRobot's Roomba). The overarching theme of learning problems is that there exists a nontrivial dependence between some observations, which we will commonly refer to as xand a desired response, which we refer to as y, for which a simple set of deterministic rules is not known. By using learning we can infer such a dependency between xandyin a systematic fashion. We conclude this section by discussing the problem of classication , since it will serve as a prototypical problem for a signicant part of this book. It occurs frequently in practice: for instance, when performing spam ltering, we are interested in a yes/no answer as to whether an e-mail con- tains relevant information or not. Note that this issue is quite user depen- dent: for a frequent traveller e-mails from an airline informing him about recent discounts might prove valuable information, whereas for many other recipients this might prove more of an nuisance (e.g. when the e-mail relates to products available only overseas). Moreover, the nature of annoying e- mails might change over time, e.g. through the availability of new products (Viagra, Cialis, Levitra, . . . ), dierent opportunities for fraud (the Nigerian 419 scam which took a new twist after the Iraq war), or dierent data types (e.g. spam which consists mainly of images). To combat these problems we

1.1 A Taste of Machine Learning 7 Fig. 1.5. Binary classication; separate stars from diamonds. In this example we are able to do so by drawing a straight line which separates both sets. We will see later that this is an important example of what is called a linear classier . want to build a system which is able to learn how to classify new e-mails. A seemingly unrelated problem, that of cancer diagnosis shares a common structure: given histological data (e.g. from a microarray analysis of a pa- tient's tissue) infer whether a patient is healthy or not. Again, we are asked to generate a yes/no answer given a set of observations. See Figure 1.5 for an example. 1.1.2 Data It is useful to characterize learning problems according to the type of data they use. This is a great help when encountering new challenges, since quite often problems on similar data types can be solved with very similar tech- niques. For instance natural language processing and bioinformatics use very similar tools for strings of natural language text and for DNA sequences. Vectors constitute the most basic entity we might encounter in our work. For instance, a life insurance company might be interesting in obtaining the vector of variables (blood pressure, heart rate, height, weight, cholesterol level, smoker, gender) to infer the life expectancy of a potential customer. A farmer might be interested in determining the ripeness of fruit based on (size, weight, spectral data). An engineer might want to nd dependencies in (voltage, current) pairs. Likewise one might want to represent documents by a vector of counts which describe the occurrence of words. The latter is commonly referred to as bag of words features. One of the challenges in dealing with vectors is that the scales and units of dierent coordinates may vary widely. For instance, we could measure the height in kilograms, pounds, grams, tons, stones, all of which would amount to multiplicative changes. Likewise, when representing temperatures, we have a full class of ane transformations, depending on whether we rep- resent them in terms of Celsius, Kelvin or Farenheit. One way of dealing

8 1 Introduction with those issues in an automatic fashion is to normalize the data. We will discuss means of doing so in an automatic fashion. Lists: In some cases the vectors we obtain may contain a variable number of features. For instance, a physician might not necessarily decide to perform a full battery of diagnostic tests if the patient appears to be healthy. Sets may appear in learning problems whenever there is a large number of potential causes of an eect, which are not well determined. For instance, it is relatively easy to obtain data concerning the toxicity of mushrooms. It would be desirable to use such data to infer the toxicity of a new mushroom given information about its chemical compounds. However, mushrooms contain a cocktail of compounds out of which one or more may be toxic. Consequently we need to infer the properties of an object given a setof features, whose composition and number may vary considerably. Matrices are a convenient means of representing pairwise relationships. For instance, in collaborative ltering applications the rows of the matrix may represent users whereas the columns correspond to products. Only in some cases we will have knowledge about a given (user, product) combina- tion, such as the rating of the product by a user. A related situation occurs whenever we only have similarity information between observations, as implemented by a semi-empirical distance mea- sure. Some homology searches in bioinformatics, e.g. variants of BLAST [AGML90], only return a similarity score which does not necessarily satisfy the requirements of a metric. Images could be thought of as two dimensional arrays of numbers, that is, matrices. This representation is very crude, though, since they exhibit spa- tial coherence (lines, shapes) and (natural images exhibit) a multiresolution structure. That is, downsampling an image leads to an object which has very similar statistics to the original image. Computer vision and psychooptics have created a raft of tools for describing these phenomena. Video adds a temporal dimension to images. Again, we could represent them as a three dimensional array. Good algorithms, however, take the tem- poral coherence of the image sequence into account. Trees and Graphs are often used to describe relations between collec- tions of objects. For instance the ontology of webpages of the DMOZ project (www.dmoz.org ) has the form of a tree with topics becoming increasingly rened as we traverse from the root to one of the leaves (Arts !Animation !Anime!General Fan Pages !Ocial Sites). In the case of gene ontol- ogy the relationships form a directed acyclic graph, also referred to as the GO-DAG [ABB+00]. Both examples above describe estimation problems where our observations

1.1 A Taste of Machine Learning 9 are vertices of a tree or graph. However, graphs themselves may be the observations. For instance, the DOM-tree of a webpage, the call-graph of a computer program, or the protein-protein interaction networks may form the basis upon which we may want to perform inference. Strings occur frequently, mainly in the area of bioinformatics and natural language processing. They may be the input to our estimation problems, e.g. when classifying an e-mail as spam, when attempting to locate all names of persons and organizations in a text, or when modeling the topic structure of a document. Equally well they may constitute the output of a system. For instance, we may want to perform document summarization, automatic translation, or attempt to answer natural language queries. Compound structures are the most commonly occurring object. That is, in most situations we will have a structured mix of dierent data types. For instance, a webpage might contain images, text, tables, which in turn contain numbers, and lists, all of which might constitute nodes on a graph of webpages linked among each other. Good statistical modelling takes such de- pendencies and structures into account in order to tailor suciently exible models. 1.1.3 Problems The range of learning problems is clearly large, as we saw when discussing applications. That said, researchers have identied an ever growing number of templates which can be used to address a large set of situations. It is those templates which make deployment of machine learning in practice easy and our discussion will largely focus on a choice set of such problems. We now give a by no means complete list of templates. Binary Classication is probably the most frequently studied problem in machine learning and it has led to a large number of important algorithmic and theoretic developments over the past century. In its simplest form it reduces to the question: given a pattern xdrawn from a domain X, estimate which value an associated binary random variable y2f 1gwill assume. For instance, given pictures of apples and oranges, we might want to state whether the object in question is an apple or an orange. Equally well, we might want to predict whether a home owner might default on his loan, given income data, his credit history, or whether a given e-mail is spam or ham. The ability to solve this basic problem already allows us to address a large variety of practical settings. There are many variants exist with regard to the protocol in which we are required to make our estimation:

10 1 Introduction Fig. 1.6. Left: binary classication. Right: 3-class classication. Note that in the latter case we have much more degree for ambiguity. For instance, being able to distinguish stars from diamonds may not suce to identify either of them correctly, since we also need to distinguish both of them from triangles. We might see a sequence of ( xi;yi) pairs for which yineeds to be estimated in an instantaneous online fashion. This is commonly referred to as online learning. We might observe a collection X:=fx1;:::xmgandY:=fy1;:::ymgof pairs (xi;yi) which are then used to estimate yfor a (set of) so-far unseen X0= x0 1;:::;x0 m0	 . This is commonly referred to as batch learning. We might be allowed to know X0already at the time of constructing the model. This is commonly referred to as transduction. We might be allowed to choose Xfor the purpose of model building. This is known as active learning. We might not have full information about X, e.g. some of the coordinates of theximight be missing, leading to the problem of estimation with missing variables. The sets XandX0might come from dierent data sources, leading to the problem of covariate shift correction. We might be given observations stemming from two problems at the same time with the side information that both problems are somehow related. This is known as co-training. Mistakes of estimation might be penalized dierently depending on the type of error, e.g. when trying to distinguish diamonds from rocks a very asymmetric loss applies. Multiclass Classication is the logical extension of binary classica- tion. The main dierence is that now y2f1;:::;ngmay assume a range of dierent values. For instance, we might want to classify a document ac- cording to the language it was written in (English, French, German, Spanish, Hindi, Japanese, Chinese, . . . ). See Figure 1.6 for an example. The main dif- ference to before is that the cost of error may heavily depend on the type of

1.1 A Taste of Machine Learning 11 Fig. 1.7. Regression estimation. We are given a number of instances (indicated by black dots) and would like to nd some function fmapping the observations Xto Rsuch thatf(x) is close to the observed values. error we make. For instance, in the problem of assessing the risk of cancer, it makes a signicant dierence whether we mis-classify an early stage of can- cer as healthy (in which case the patient is likely to die) or as an advanced stage of cancer (in which case the patient is likely to be inconvenienced from overly aggressive treatment). Structured Estimation goes beyond simple multiclass estimation by assuming that the labels yhave some additional structure which can be used in the estimation process. For instance, ymight be a path in an ontology, when attempting to classify webpages, ymight be a permutation, when attempting to match objects, to perform collaborative ltering, or to rank documents in a retrieval setting. Equally well, ymight be an annotation of a text, when performing named entity recognition. Each of those problems has its own properties in terms of the set of ywhich we might consider admissible, or how to search this space. We will discuss a number of those problems in Chapter ??. Regression is another prototypical application. Here the goal is to esti- mate a real-valued variable y2Rgiven a pattern x(see e.g. Figure 1.7). For instance, we might want to estimate the value of a stock the next day, the yield of a semiconductor fab given the current process, the iron content of ore given mass spectroscopy measurements, or the heart rate of an athlete, given accelerometer data. One of the key issues in which regression problems dier from each other is the choice of a loss. For instance, when estimating stock values our loss for a put option will be decidedly one-sided. On the other hand, a hobby athlete might only care that our estimate of the heart rate matches the actual on average. Novelty Detection is a rather ill-dened problem. It describes the issue of determining \unusual" observations given a set of past measurements. Clearly, the choice of what is to be considered unusual is very subjective. A commonly accepted notion is that unusual events occur rarely. Hence a possible goal is to design a system which assigns to each observation a rating

12 1 Introduction Fig. 1.8. Left: typical digits contained in the database of the US Postal Service. Right: unusual digits found by a novelty detection algorithm [SPST+01] (for a description of the algorithm see Section 7.4). The score below the digits indicates the degree of novelty. The numbers on the lower right indicate the class associated with the digit. as to how novel it is. Readers familiar with density estimation might contend that the latter would be a reasonable solution. However, we neither need a score which sums up to 1 on the entire domain, nor do we care particularly much about novelty scores for typical observations. We will later see how this somewhat easier goal can be achieved directly. Figure 1.8 has an example of novelty detection when applied to an optical character recognition database. 1.2 Probability Theory In order to deal with the instances of where machine learning can be used, we need to develop an adequate language which is able to describe the problems concisely. Below we begin with a fairly informal overview over probability theory. For more details and a very gentle and detailed discussion see the excellent book of [BT03]. 1.2.1 Random Variables Assume that we cast a dice and we would like to know our chances whether we would see 1 rather than another digit. If the dice is fair all six outcomes X=f1;:::; 6gare equally likely to occur, hence we would see a 1 in roughly 1 out of 6 cases. Probability theory allows us to model uncertainty in the out- come of such experiments. Formally we state that 1 occurs with probability 1 6. In many experiments, such as the roll of a dice, the outcomes are of a numerical nature and we can handle them easily. In other cases, the outcomes may not be numerical, e.g., if we toss a coin and observe heads or tails. In these cases, it is useful to associate numerical values to the outcomes. This is done via a random variable. For instance, we can let a random variable

1.2 Probability Theory 13 Xtake on a value +1 whenever the coin lands heads and a value of  1 otherwise. Our notational convention will be to use uppercase letters, e.g., X,Yetc to denote random variables and lower case letters, e.g.,x,yetc to denote the values they take. XweightheightÎ¾(x)x Fig. 1.9. The random variable maps from the set of outcomes of an experiment (denoted here by X) to real numbers. As an illustration here Xconsists of the patients a physician might encounter, and they are mapped via to their weight and height. 1.2.2 Distributions Perhaps the most important way to characterize a random variable is to associate probabilities with the values it can take. If the random variable is discrete, i.e.,it takes on a nite number of values, then this assignment of probabilities is called a probability mass function or PMF for short. A PMF must be, by denition, non-negative and must sum to one. For instance, if the coin is fair, i.e., heads and tails are equally likely, then the random variableXdescribed above takes on values of +1 and  1 with probability 0:5. This can be written as Pr(X= +1) = 0:5 andPr(X= 1) = 0:5: (1.1) When there is no danger of confusion we will use the slightly informal no- tationp(x) :=Pr(X=x). In case of a continuous random variable the assignment of probabilities results in a probability density function or PDF for short. With some abuse of terminology, but keeping in line with convention, we will often use density or distribution instead of probability density function. As in the case of the PMF, a PDF must also be non-negative and integrate to one. Figure 1.10 shows two distributions: the uniform distribution p(x) =( 1 b aifx2[a;b] 0 otherwise ;(1.2)

14 1 Introduction -4  -2  0  2  4 0.0 0.1 0.2 0.3 0.4 0.5 -4  -2  0  2  4 0.0 0.1 0.2 0.3 0.4 0.5 Fig. 1.10. Two common densities. Left: uniform distribution over the interval [ 1;1]. Right: Normal distribution with zero mean and unit variance. and the Gaussian distribution (also called normal distribution) p(x) =1p 22exp  (x )2 22 : (1.3) Closely associated with a PDF is the indenite integral over p. It is com- monly referred to as the cumulative distribution function (CDF). Denition 1.1 (Cumulative Distribution Function) For a real valued random variable Xwith PDFpthe associated Cumulative Distribution Func- tionFis given by F(x0) := Pr Xx0	 =Zx0  1dp(x): (1.4) The CDFF(x0) allows us to perform range queries on peciently. For instance, by integral calculus we obtain Pr(aXb) =Zb adp(x) =F(b) F(a): (1.5) The values of x0for whichF(x0) assumes a specic value, such as 0 :1 or 0:5 have a special name. They are called the quantiles of the distribution p. Denition 1.2 (Quantiles) Letq2(0;1). Then the value of x0for which Pr(X <x0)qandPr(X >x0)1 qis theq-quantile of the distribution p. Moreover, the value x0associated with q= 0:5is called the median.

1.2 Probability Theory 15 p(x) Fig. 1.11. Quantiles of a distribution correspond to the area under the integral of the density p(x) for which the integral takes on a pre-specied value. Illustrated are the 0.1, 0.5 and 0.9 quantiles respectively. 1.2.3 Mean and Variance A common question to ask about a random variable is what its expected value might be. For instance, when measuring the voltage of a device, we might ask what its typical values might be. When deciding whether to ad- minister a growth hormone to a child a doctor might ask what a sensible range of height should be. For those purposes we need to dene expectations and related quantities of distributions. Denition 1.3 (Mean) We dene the mean of a random variable Xas E[X] :=Z xdp(x) (1.6) More generally, if f:R!Ris a function, then f(X)is also a random variable. Its mean is mean given by E[f(X)] :=Z f(x)dp(x): (1.7) WheneverXis a discrete random variable the integral in (1.6) can be re- placed by a summation: E[X] =X xxp(x): (1.8) For instance, in the case of a dice we have equal probabilities of 1 =6 for all 6 possible outcomes. It is easy to see that this translates into a mean of (1 + 2 + 3 + 4 + 5 + 6) =6 = 3:5. The mean of a random variable is useful in assessing expected losses and benets. For instance, as a stock broker we might be interested in the ex- pected value of our investment in a year's time. In addition to that, however, we also might want to investigate the riskof our investment. That is, how likely it is that the value of the investment might deviate from its expecta- tion since this might be more relevant for our decisions. This means that we

16 1 Introduction need a variable to quantify the risk inherent in a random variable. One such measure is the variance of a random variable. Denition 1.4 (Variance) We dene the variance of a random variable Xas Var[X] :=Eh (X E[X])2i : (1.9) As before, if f:R!Ris a function, then the variance of f(X)is given by Var[f(X)] :=Eh (f(X) E[f(X)])2i : (1.10) The variance measures by how much on average f(X) deviates from its ex- pected value. As we shall see in Section 2.1, an upper bound on the variance can be used to give guarantees on the probability that f(X) will be within of its expected value. This is one of the reasons why the variance is often associated with the risk of a random variable. Note that often one discusses properties of a random variable in terms of its standard deviation , which is dened as the square root of the variance. 1.2.4 Marginalization, Independence, Conditioning, and Bayes Rule Given two random variables XandY, one can write their joint density p(x;y). Given the joint density, one can recover p(x) by integrating out y. This operation is called marginalization: p(x) =Z ydp(x;y): (1.11) IfYis a discrete random variable, then we can replace the integration with a summation: p(x) =X yp(x;y): (1.12) We say that XandYare independent, i.e.,the values that Xtakes does not depend on the values that Ytakes whenever p(x;y) =p(x)p(y): (1.13) Independence is useful when it comes to dealing with large numbers of ran- dom variables whose behavior we want to estimate jointly. For instance, whenever we perform repeated measurements of a quantity, such as when

1.2 Probability Theory 17 -0.5  0.0  0.5  1.0  1.5 2.0-0.5 0.0 0.5 1.0 1.52.0 -0.5  0.0  0.5  1.0  1.5 2.0-0.5 0.0 0.5 1.0 1.52.0 Fig. 1.12. Left: a sample from two dependent random variables. Knowing about rst coordinate allows us to improve our guess about the second coordinate. Right: a sample drawn from two independent random variables, obtained by randomly permuting the dependent sample. measuring the voltage of a device, we will typically assume that the individ- ual measurements are drawn from the same distribution and that they are independent of each other. That is, having measured the voltage a number of times will not aect the value of the next measurement. We will call such random variables to be independently and identically distributed , or in short, iidrandom variables. See Figure 1.12 for an example of a pair of random variables drawn from dependent and independent distributions respectively. Conversely, dependence can be vital in classication and regression prob- lems. For instance, the trac lights at an intersection are dependent of each other. This allows a driver to perform the inference that when the lights are green in his direction there will be no trac crossing his path, i.e. the other lights will indeed be red. Likewise, whenever we are given a picture xof a digit, we hope that there will be dependence between xand its label y. Especially in the case of dependent random variables, we are interested in conditional probabilities, i.e., probability that Xtakes on a particular value given the value of Y. ClearlyPr(X=rainjY=cloudy ) is higher than Pr(X=rainjY=sunny ). In other words, knowledge about the value of Y signicantly inuences the distribution of X. This is captured via conditional probabilities: p(xjy) :=p(x;y) p(y): (1.14) Equation 1.14 leads to one of the key tools in statistical inference. Theorem 1.5 (Bayes Rule) Denote byXandYrandom variables then

18 1 Introduction the following holds p(yjx) =p(xjy)p(y) p(x): (1.15) This follows from the fact that p(x;y) =p(xjy)p(y) =p(yjx)p(x). The key consequence of (1.15) is that we may reverse the conditioning between a pair of random variables. 1.2.4.1 An Example We illustrate our reasoning by means of a simple example | inference using an AIDS test. Assume that a patient would like to have such a test carried out on him. The physician recommends a test which is guaranteed to detect HIV-positive whenever a patient is infected. On the other hand, for healthy patients it has a 1% error rate. That is, with probability 0.01 it diagnoses a patient as HIV-positive even when he is, in fact, HIV-negative. Moreover, assume that 0.15% of the population is infected. Now assume that the patient has the test carried out and the test re- turns 'HIV-negative'. In this case, logic implies that he is healthy, since the test has 100% detection rate. In the converse case things are not quite as straightforward. Denote by XandTthe random variables associated with the health status of the patient and the outcome of the test respectively. We are interested in p(X= HIV+jT= HIV+). By Bayes rule we may write p(X= HIV+jT= HIV+) =p(T= HIV+jX= HIV+)p(X= HIV+) p(T= HIV+) While we know all terms in the numerator, p(T= HIV+) itself is unknown. That said, it can be computed via p(T= HIV+) =X x2fHIV+;HIV-gp(T= HIV+;x) =X x2fHIV+;HIV-gp(T= HIV+jx)p(x) = 1:00:0015 + 0:010:9985: Substituting back into the conditional expression yields p(X= HIV+jT= HIV+) =1:00:0015 1:00:0015 + 0:010:9985= 0:1306: In other words, even though our test is quite reliable, there is such a low prior probability of having been infected with AIDS that there is not much evidence to accept the hypothesis even after this test.

1.2 Probability Theory 19 agextest 1test 2 Fig. 1.13. A graphical description of our HIV testing scenario. Knowing the age of the patient inuences our prior on whether the patient is HIV positive (the random variableX). The outcomes of the tests 1 and 2 are independent of each other given the statusX. We observe the shaded random variables (age, test 1, test 2) and would like to infer the un-shaded random variable X. This is a special case of a graphical model which we will discuss in Chapter ??. Let us now think how we could improve the diagnosis. One way is to ob- tain further information about the patient and to use this in the diagnosis. For instance, information about his age is quite useful. Suppose the patient is 35 years old. In this case we would want to compute p(X= HIV+jT= HIV+;A= 35) where the random variable Adenotes the age. The corre- sponding expression yields: p(T= HIV+jX= HIV+;A)p(X= HIV+jA) p(T= HIV+jA) Here we simply conditioned all random variables on Ain order to take addi- tional information into account. We may assume that the test is independent of the age of the patient, i.e. p(tjx;a) =p(tjx): What remains therefore is p(X= HIV+jA). Recent US census data pegs this number at approximately 0.9%. Plugging all data back into the conditional expression yields10:009 10:009+0:010:991= 0:48. What has happened here is that by including additional observed random variables our estimate has become more reliable. Combination of evidence is a powerful tool. In our case it helped us make the classication problem of whether the patient is HIV- positive or not more reliable. A second tool in our arsenal is the use of multiple measurements. After the rst test the physician is likely to carry out a second test to conrm the diagnosis. We denote by T1andT2(andt1;t2respectively) the two tests. Obviously, what we want is that T2will give us an \independent" second opinion of the situation. In other words, we want to ensure that T2does not make the same mistakes as T1. For instance, it is probably a bad idea to repeatT1without changes, since it might perform the same diagnostic

20 1 Introduction mistake as before. What we want is that the diagnosis of T2is independent of that ofT2given the health status Xof the patient. This is expressed as p(t1;t2jx) =p(t1jx)p(t2jx): (1.16) See Figure 1.13 for a graphical illustration of the setting. Random variables satisfying the condition (1.16) are commonly referred to as conditionally independent . In shorthand we write T1;T2? ?X. For the sake of the argument we assume that the statistics for T2are given by p(t2jx)x= HIV-x= HIV+ t2= HIV- 0.95 0.01 t2= HIV+ 0.05 0.99 Clearly this test is less reliable than the rst one. However, we may now combine both estimates to obtain a very reliable estimate based on the combination of both events. For instance, for t1=t2= HIV+ we have p(X= HIV+jT1= HIV+;T2= HIV+) =1:00:990:009 1:00:990:009 + 0:010:050:991= 0:95: In other words, by combining two tests we can now conrm with very high condence that the patient is indeed diseased. What we have carried out is a combination of evidence. Strong experimental evidence of two positive tests eectively overcame an initially very strong prior which suggested that the patient might be healthy. Tests such as in the example we just discussed are fairly common. For instance, we might need to decide which manufacturing procedure is prefer- able, which choice of parameters will give better results in a regression es- timator, or whether to administer a certain drug. Note that often our tests may not be conditionally independent and we would need to take this into account. 1.3 Basic Algorithms We conclude our introduction to machine learning by discussing four simple algorithms, namely Naive Bayes, Nearest Neighbors, the Mean Classier, and the Perceptron, which can be used to solve a binary classication prob- lem such as that described in Figure 1.5. We will also introduce the K-means algorithm which can be employed when labeled data is not available. All these algorithms are readily usable and easily implemented from scratch in their most basic form. For the sake of concreteness assume that we are interested in spam lter- ing. That is, we are given a set of me-mailsxi, denoted by X:=fx1;:::;xmg

1.3 Basic Algorithms 21 From: "LucindaParkison497072" <LucindaParkison497072@hotmail.com> To: <kargr@earthlink.net> Subject: we think ACGU is our next winner Date: Mon, 25 Feb 2008 00:01:01 -0500 MIME-Version: 1.0 X-OriginalArrivalTime: 25 Feb 2008 05:01:01.0329 (UTC) FILETIME=[6A931810:01C8776B] Return-Path: lucindaparkison497072@hotmail.com (ACGU) .045 UP 104.5% I do think that (ACGU) at it's current levels looks extremely attractive. Asset Capital Group, Inc., (ACGU) announced that it is expanding the marketing of bio-remediation fluids and cleaning equipment. After its recent acquisition of interest in American Bio-Clean Corporation and an 80 News is expected to be released next week on this growing company and could drive the price even higher. Buy (ACGU) Monday at open. I believe those involved at this stage could enjoy a nice ride up. Fig. 1.14. Example of a spam e-mail x1:The quick brown fox jumped over the lazy dog. x2:The dog hunts a fox. the quick brown fox jumped over lazy dog hunts a x12 1 1 1 1 1 1 1 0 0 x21 0 0 1 0 0 0 1 1 1 Fig. 1.15. Vector space representation of strings. and associated labels yi, denoted by Y:=fy1;:::;ymg. Here the labels sat- isfyyi2fspam;hamg. The key assumption we make here is that the pairs (xi;yi) are drawn jointly from some distribution p(x;y) which represents the e-mail generating process for a user. Moreover, we assume that there is suciently strong dependence between xandythat we will be able to estimateygivenxand a set of labeled instances X;Y. Before we do so we need to address the fact that e-mails such as Figure 1.14 aretext, whereas the three algorithms we present will require data to be represented in a vectorial fashion. One way of converting text into a vector is by using the so-called bag of words representation [Mar61, Lew98]. In its simplest version it works as follows: Assume we have a list of all possible words occurring in X, that is a dictionary, then we are able to assign a unique number with each of those words (e.g. the position in the dictionary). Now we may simply count for each document xithe number of times a given wordjis occurring. This is then used as the value of the j-th coordinate ofxi. Figure 1.15 gives an example of such a representation. Once we have the latter it is easy to compute distances, similarities, and other statistics directly from the vectorial representation.

22 1 Introduction 1.3.1 Naive Bayes In the example of the AIDS test we used the outcomes of the test to infer whether the patient is diseased. In the context of spam ltering the actual text of the e-mail xcorresponds to the test and the label yis equivalent to the diagnosis. Recall Bayes Rule (1.15). We could use the latter to infer p(yjx) =p(xjy)p(y) p(x): We may have a good estimate of p(y), that is, the probability of receiving a spam or ham mail. Denote by mhamandmspam the number of ham and spam e-mails in X. In this case we can estimate p(ham)mham mandp(spam)mspam m: The key problem, however, is that we do not know p(xjy) orp(x). We may dispose of the requirement of knowing p(x) by settling for a likelihood ratio L(x) :=p(spamjx) p(hamjx)=p(xjspam)p(spam) p(xjham)p(ham): (1.17) WheneverL(x) exceeds a given threshold cwe decide that xis spam and consequently reject the e-mail. If cis large then our algorithm is conservative and classies an email as spam only if p(spamjx)p(hamjx). On the other hand, ifcis small then the algorithm aggressively classies emails as spam. The key obstacle is that we have no access to p(xjy). This is where we make our key approximation. Recall Figure 1.13. In order to model the distribution of the test outcomes T1andT2we made the assumption that they are conditionally independent of each other given the diagnosis. Analogously, we may now treat the occurrence of each word in a document as a separate test and combine the outcomes in a naive fashion by assuming that p(xjy) =# of words in xY j=1p(wjjy); (1.18) wherewjdenotes the j-th word in document x. This amounts to the as- sumption that the probability of occurrence of a word in a document is independent of all other words given the category of the document. Even though this assumption does not hold in general { for instance, the word \York" is much more likely to after the word \New" { it suces for our purposes (see Figure 1.16). This assumption reduces the diculty of knowing p(xjy) to that of esti- mating the probabilities of occurrence of individual words w. Estimates for

1.3 Basic Algorithms 23 yword 1word 2...word nword 3 Fig. 1.16. Naive Bayes model. The occurrence of individual words is independent of each other, given the category of the text. For instance, the word Viagra is fairly frequent if y= spam but it is considerably less frequent if y= ham, except when considering the mailbox of a Pzer sales representative. p(wjy) can be obtained, for instance, by simply counting the frequency oc- currence of the word within documents of a given class. That is, we estimate p(wjspam)Pm i=1P# of words in xi j=1n yi= spam and wj i=wo Pm i=1P# of words in xi j=1fyi= spamg Heren yi= spam and wj i=wo equals 1 if and only if xiis labeled as spam andwoccurs as the j-th word in xi. The denominator is simply the total number of words in spam documents. Similarly one can compute p(wjham). In principle we could perform the above summation whenever we see a new documentx. This would be terribly inecient, since each such computation requires a full pass through XandY. Instead, we can perform a single pass through XandYand store the resulting statistics as a good estimate of the conditional probabilities. Algorithm 1.1 has details of an implementation. Note that we performed a number of optimizations: Firstly, the normaliza- tion bym 1 spam andm 1 hamrespectively is independent of x, hence we incor- porate it as a xed oset. Secondly, since we are computing a product over a large number of factors the numbers might lead to numerical overow or underow. This can be addressed by summing over the logarithm of terms rather than computing products. Thirdly, we need to address the issue of estimating p(wjy) for words wwhich we might not have seen before. One way of dealing with this is to increment all counts by 1. This method is commonly referred to as Laplace smoothing. We will encounter a theoretical justication for this heuristic in Section 2.3. This simple algorithm is known to perform surprisingly well, and variants of it can be found in most modern spam lters. It amounts to what is commonly known as \Bayesian spam ltering". Obviously, we may apply it to problems other than document categorization, too.

24 1 Introduction Algorithm 1.1 Naive Bayes Train( X;Y)freads documents Xand labels Yg Compute dictionary DofXwithnwords. Computem;m hamandmspam. Initializeb:= logc+logmham logmspam to oset the rejection threshold Initializep2R2nwithpij= 1,wspam =n,wham=n. fCount occurrence of each word g fHerexj idenotes the number of times word joccurs in document xig fori= 1 tomdo ifyi= spam then forj= 1 tondo p0;j p0;j+xj i wspam wspam+xj i end for else forj= 1 tondo p1;j p1;j+xj i wham wham+xj i end for end if end for fNormalize counts to yield word probabilities g forj= 1 tondo p0;j p0;j=wspam p1;j p1;j=wham end for Classify(x)fclassies document xg Initialize score threshold t= b forj= 1 tondo t t+xj(logp0;j logp1;j) end for ift>0return spam else return ham 1.3.2 Nearest Neighbor Estimators An even simpler estimator than Naive Bayes is nearest neighbors. In its most basic form it assigns the label of its nearest neighbor to an observation x (see Figure 1.17). Hence, all we need to implement it is a distance measure d(x;x0) between pairs of observations. Note that this distance need not even be symmetric. This means that nearest neighbor classiers can be extremely

1.3 Basic Algorithms 25 Fig. 1.17. 1 nearest neighbor classier. Depending on whether the query point xis closest to the star, diamond or triangles, it uses one of the three labels for it. Fig. 1.18.k-Nearest neighbor classiers using Euclidean distances. Left: decision boundaries obtained from a 1-nearest neighbor classier. Middle: color-coded sets of where the number of red / blue points ranges between 7 and 0. Right: decision boundary determining where the blue or red dots are in the majority. exible. For instance, we could use string edit distances to compare two documents or information theory based measures. However, the problem with nearest neighbor classication is that the esti- mates can be very noisy whenever the data itself is very noisy. For instance, if a spam email is erroneously labeled as nonspam then all emails which are similar to this email will share the same fate. See Figure 1.18 for an example. In this case it is benecial to pool together a number of neighbors, say thek-nearest neighbors of xand use a majority vote to decide the class membership of x. Algorithm 1.2 has a description of the algorithm. Note that nearest neighbor algorithms can yield excellent performance when used with a good distance measure. For instance, the technology underlying the Netix progress prize [BK07] was essentially nearest neighbours based. Note that it is trivial to extend the algorithm to regression. All we need to change in Algorithm 1.2 is to return the average of the values yiinstead of their majority vote. Figure 1.19 has an example. Note that the distance computation d(xi;x) for all observations can be-

26 1 Introduction Algorithm 1.2 k-Nearest Neighbor Classication Classify( X;Y;x)freads documents X, labels Yand queryxg fori= 1tomdo Compute distance d(xi;x) end for Compute set Icontaining indices for the ksmallest distances d(xi;x). return majority label of fyiwherei2Ig. Fig. 1.19.k-Nearest neighbor regression estimator using Euclidean distances. Left: some points ( x;y) drawn from a joint distribution. Middle: 1-nearest neighbour classier. Right: 7-nearest neighbour classier. Note that the regression estimate is much more smooth. come extremely costly, in particular whenever the number of observations is large or whenever the observations xilive in a very high dimensional space. Random projections are a technique that can alleviate the high computa- tional cost of Nearest Neighbor classiers. A celebrated lemma by Johnson and Lindenstrauss [DG03] asserts that a set of mpoints in high dimensional Euclidean space can be projected into a O(logm=2) dimensional Euclidean space such that the distance between any two points changes only by a fac- tor of (1). Since Euclidean distances are preserved, running the Nearest Neighbor classier on this mapped data yields the same results but at a lower computational cost [GIM99]. The surprising fact is that the projection relies on a simple randomized algorithm: to obtain a d-dimensional representation of n-dimensional ran- dom observations we pick a matrix R2Rdnwhere each element is drawn independently from a normal distribution with n 1 2variance and zero mean. Multiplying xwith this projection matrix can be shown to achieve this prop- erty with high probability. For details see [DG03].

1.3 Basic Algorithms 27 wÎ¼-Î¼+x Fig. 1.20. A trivial classier. Classication is carried out in accordance to which of the two means  or+is closer to the test point x. Note that the sets of positive and negative labels respectively form a half space. 1.3.3 A Simple Classier We can use geometry to design another simple classication algorithm [SS02] for our problem. For simplicity we assume that the observations x2Rd, such as the bag-of-words representation of e-mails. We dene the means +and  to correspond to the classes y2f 1gvia  :=1 m X yi= 1xiand+:=1 m+X yi=1xi: Here we used m andm+to denote the number of observations with label yi= 1 andyi= +1 respectively. An even simpler approach than using the nearest neighbor classier would be to use the class label which corresponds to the mean closest to a new query x, as described in Figure 1.20. For Euclidean distances we have k  xk2=k k2+kxk2 2h ;xiand (1.19) k+ xk2=k+k2+kxk2 2h+;xi: (1.20) Hereh;idenotes the standard dot product between vectors. Taking dier- ences between the two distances yields f(x) :=k+ xk2 k  xk2= 2h  +;xi+k k2 k+k2: (1.21) This is a linear function in xand its sign corresponds to the labels we esti- mate forx. Our algorithm sports an important property: The classication rule can be expressed via dot products. This follows from k+k2=h+;+i=m 2 +X yi=yj=1hxi;xjiandh+;xi=m 1 +X yi=1hxi;xi:

28 1 Introduction XÏ†(x)xH Fig. 1.21. The feature map maps observations xfromXinto a feature space H. The mapis a convenient way of encoding pre-processing steps systematically. Analogous expressions can be computed for  . Consequently we may ex- press the classication rule (1.21) as f(x) =mX i=1ihxi;xi+b (1.22) whereb=m 2  P yi=yj= 1hxi;xji m 2 +P yi=yj=1hxi;xjiandi=yi=myi. This oers a number of interesting extensions. Recall that when dealing with documents we needed to perform pre-processing to map e-mails into a vector space. In general, we may pick arbitrary maps :X!Hmapping the space of observations into a feature space H, as long as the latter is endowed with a dot product (see Figure 1.21). This means that instead of dealing withhx;x0iwe will be dealing with h(x);(x0)i. As we will see in Chapter 6, whenever His a so-called Reproducing Kernel Hilbert Space, the inner product can be abbreviated in the form of a kernel functionk(x;x0) which satises k(x;x0) := (x);(x0) : (1.23) This small modication leads to a number of very powerful algorithm and it is at the foundation of an area of research called kernel methods. We will encounter a number of such algorithms for regression, classication, segmentation, and density estimation over the course of the book. Examples of suitablekare the polynomial kernel k(x;x0) =hx;x0idford2Nand the Gaussian RBF kernel k(x;x0) =e kx x0k2for >0. The upshot of (1.23) is that our basic algorithm can be kernelized . That is, we may rewrite (1.21) as f(x) =mX i=1ik(xi;x) +b (1.24) where as before i=yi=myiand the oset bis computed analogously. As

1.3 Basic Algorithms 29 Algorithm 1.3 The Perceptron Perceptron( X;Y)freads stream of observations ( xi;yi)g Initializew= 0 andb= 0 while There exists some ( xi;yi) withyi(hw;xii+b)0do w w+yixiandb b+yi end while Algorithm 1.4 The Kernel Perceptron KernelPerceptron( X;Y)freads stream of observations ( xi;yi)g Initializef= 0 while There exists some ( xi;yi) withyif(xi)0do f f+yik(xi;) +yi end while a consequence we have now moved from a fairly simple and pedestrian lin- ear classier to one which yields a nonlinear function f(x) with a rather nontrivial decision boundary. 1.3.4 Perceptron In the previous sections we assumed that our classier had access to a train- ing set of spam and non-spam emails. In real life, such a set might be dicult to obtain all at once. Instead, a user might want to have instant results when- ever a new e-mail arrives and he would like the system to learn immediately from any corrections to mistakes the system makes. To overcome both these diculties one could envisage working with the following protocol: As emails arrive our algorithm classies them as spam or non-spam, and the user provides feedback as to whether the classication is correct or incorrect. This feedback is then used to improve the performance of the classier over a period of time. This intuition can be formalized as follows: Our classier maintains a parameter vector. At the t-th time instance it receives a data point xt, to which it assigns a label ^ ytusing its current parameter vector. The true label ytis then revealed, and used to update the parameter vector of the classier. Such algorithms are said to be online . We will now describe perhaps the simplest classier of this kind namely the Perceptron [Heb49, Ros58]. Let us assume that the data points xt2Rd, and labels yt2f 1g. As before we represent an email as a bag-of-words vector and we assign +1 to spam emails and 1 to non-spam emails. The Perceptron maintains a weight

30 1 Introduction w*wtw*wt+1xtxt Fig. 1.22. The Perceptron without bias. Left: at time twe have a weight vector wt denoted by the dashed arrow with corresponding separating plane (also dashed). For reference we include the linear separator wand its separating plane (both denoted by a solid line). As a new observation xtarrives which happens to be mis-classied by the current weight vector wtwe perform an update. Also note the margin between the point xtand the separating hyperplane dened by w. Right: This leads to the weight vector wt+1which is more aligned with w. vectorw2Rdand classies xtaccording to the rule ^yt:= signfhw;xti+bg; (1.25) wherehw;xtidenotes the usual Euclidean dot product and bis an oset. Note the similarity of (1.25) to (1.21) of the simple classier. Just as the latter, the Perceptron is a linear classier which separates its domain Rdinto two halfspaces, namely fxjhw;xi+b>0gand its complement. If ^ yt=ytthen no updates are made. On the other hand, if ^ yt6=ytthe weight vector is updated as w w+ytxtandb b+yt: (1.26) Figure 1.22 shows an update step of the Perceptron algorithm. For simplicity we illustrate the case without bias, that is, where b= 0 and where it remains unchanged. A detailed description of the algorithm is given in Algorithm 1.3. An important property of the algorithm is that it performs updates on w by multiples of the observations xion which it makes a mistake. Hence we may express wasw=P i2Erroryixi. Just as before, we can replace xiandx by(xi) and(x) to obtain a kernelized version of the Perceptron algorithm [FS99] (Algorithm 1.4). If the dataset ( X;Y) is linearly separable, then the Perceptron algorithm

1.3 Basic Algorithms 31 eventually converges and correctly classies all the points in X. The rate of convergence however depends on the margin. Roughly speaking, the margin quanties how linearly separable a dataset is, and hence how easy it is to solve a given classication problem. Denition 1.6 (Margin) Letw2Rdbe a weight vector and let b2Rbe an oset. The margin of an observation x2Rdwith associated label yis (x;y) :=y(hw;xi+b): (1.27) Moreover, the margin of an entire set of observations Xwith labels Yis (X;Y) := min i(xi;yi): (1.28) Geometrically speaking (see Figure 1.22) the margin measures the distance ofxfrom the hyperplane dened by fxjhw;xi+b= 0g. Larger the margin, the more well separated the data and hence easier it is to nd a hyperplane with correctly classies the dataset. The following theorem asserts that if there exists a linear classier which can classify a dataset with a large mar- gin, then the Perceptron will also correctly classify the same dataset after making a small number of mistakes. Theorem 1.7 (Noviko's theorem) Let(X;Y)be a dataset with at least one example labeled +1and one example labeled  1. LetR:= maxtkxtk, and assume that there exists (w;b)such thatkwk= 1andt:=yt(hw;xti+ b)for allt. Then, the Perceptron will make at most(1+R2)(1+(b)2) 2 mistakes. This result is remarkable since it does notdepend on the dimensionality of the problem. Instead, it only depends on the geometry of the setting, as quantied via the margin and the radius Rof a ball enclosing the observations. Interestingly, a similar bound can be shown for Support Vector Machines [Vap95] which we will be discussing in Chapter 7. Proof We can safely ignore the iterations where no mistakes were made and hence no updates were carried out. Therefore, without loss of generality assume that the t-th update was made after seeing the t-th observation and letwtdenote the weight vector after the update. Furthermore, for simplicity assume that the algorithm started with w0= 0 andb0= 0. By the update equation (1.26) we have hwt;wi+btb=hwt 1;wi+bt 1b+yt(hxt;wi+b) hwt 1;wi+bt 1b+:

32 1 Introduction By induction it follows that hwt;wi+btbt. On the other hand we made an update because yt(hxt;wt 1i+bt 1)<0. By using ytyt= 1, kwtk2+b2 t=kwt 1k2+b2 t 1+y2 tkxtk2+ 1 + 2yt(hwt 1;xti+bt 1) kwt 1k2+b2 t 1+kxtk2+ 1 Sincekxtk2=R2we can again apply induction to conclude that kwtk2+b2 t t R2+ 1 . Combining the upper and the lower bounds, using the Cauchy- Schwartz inequality, and kwk= 1 yields thwt;wi+btb=wt bt ;w b wt btw b=q kwtk2+b2 tp 1 + (b)2 p t(R2+ 1)p 1 + (b)2: Squaring both sides of the inequality and rearranging the terms yields an upper bound on the number of updates and hence the number of mistakes. The Perceptron was the building block of research on Neural Networks [Hay98, Bis95]. The key insight was to combine large numbers of such net- works, often in a cascading fashion, to larger objects and to fashion opti- mization algorithms which would lead to classiers with desirable properties. In this book we will take a complementary route. Instead of increasing the number of nodes we will investigate what happens when increasing the com- plexity of the feature map and its associated kernel k. The advantage of doing so is that we will reap the benets from convex analysis and linear models, possibly at the expense of a slightly more costly function evaluation. 1.3.5 K-Means All the algorithms we discussed so far are supervised, that is, they assume that labeled training data is available. In many applications this is too much to hope for; labeling may be expensive, error prone, or sometimes impossi- ble. For instance, it is very easy to crawl and collect every page within the www.purdue.edu domain, but rather time consuming to assign a topic to each page based on its contents. In such cases, one has to resort to unsuper- vised learning. A prototypical unsupervised learning algorithm is K-means, which is clustering algorithm. Given X=fx1;:::;xmgthe goal of K-means is to partition it into kclusters such that each point in a cluster is similar to points from its own cluster than with points from some other cluster.

1.3 Basic Algorithms 33 Towards this end, dene prototype vectors 1;:::;kand an indicator vectorrijwhich is 1 if, and only if, xiis assigned to cluster j. To cluster our dataset we will minimize the following distortion measure, which minimizes the distance of each point from the prototype vector: J(r;) :=1 2mX i=1kX j=1rijkxi jk2; (1.29) wherer=frijg,=fjg, andkk2denotes the usual Euclidean square norm. Our goal is to nd rand, but since it is not easy to jointly minimize J with respect to both rand, we will adapt a two stage strategy: Stage 1 Keep thexed and determine r. In this case, it is easy to see that the minimization decomposes into mindependent problems. The solution for the i-th data point xican be found by setting: rij= 1 ifj= argmin j0kxi j0k2; (1.30) and 0 otherwise. Stage 2 Keep therxed and determine . Since the r's are xed, Jis an quadratic function of . It can be minimized by setting the derivative with respect to jto be 0: mX i=1rij(xi j) = 0 for all j: (1.31) Rearranging obtains j=P irijxiP irij: (1.32) SinceP irijcounts the number of points assigned to cluster j, we are essentially setting jto be the sample mean of the points assigned to clusterj. The algorithm stops when the cluster assignments do not change signi- cantly. Detailed pseudo-code can be found in Algorithm 1.5. Two issues with K-Means are worth noting. First, it is sensitive to the choice of the initial cluster centers . A number of practical heuristics have been developed. For instance, one could randomly choose kpoints from the given dataset as cluster centers. Other methods try to pick kpoints from X which are farthest away from each other. Second, it makes a hard assignment of every point to a cluster center. Variants which we will encounter later in

34 1 Introduction Algorithm 1.5 K-Means Cluster( X)fCluster dataset Xg Initialize cluster centers jforj= 1;:::;k randomly repeat fori= 1tomdo Computej0= argminj=1;:::;kd(xi;j) Setrij0= 1 andrij= 0 for allj06=j end for forj= 1tokdo Computej=P irijxiP irij end for until Cluster assignments rijare unchanged returnf1;:::;kgandrij the book will relax this. Instead of letting rij2f0;1gthese softvariants will replace it with the probability that a given xibelongs to cluster j. The K-Means algorithm concludes our discussion of a set of basic machine learning methods for classication and regression. They provide a useful starting point for an aspiring machine learning researcher. In this book we will see many more such algorithms as well as connections between these basic algorithms and their more advanced counterparts. Problems Problem 1.1 (Eyewitness) Assume that an eyewitness is 90% certain that a given person committed a crime in a bar. Moreover, assume that there were 50 people in the restaurant at the time of the crime. What is the posterior probability of the person actually having committed the crime. Problem 1.2 (DNA Test) Assume the police have a DNA library of 10 million records. Moreover, assume that the false recognition probability is below 0:00001% per record . Suppose a match is found after a database search for an individual. What are the chances that the identication is correct? You can assume that the total population is 100 million people. Hint: compute the probability of no match occurring rst. Problem 1.3 (Bomb Threat) Suppose that the probability that one of a thousand passengers on a plane has a bomb is 1 : 1;000;000. Assuming that the probability to have a bomb is evenly distributed among the passengers,

1.3 Basic Algorithms 35 the probability that two passengers have a bomb is roughly equal to 10 12. Therefore, one might decide to take a bomb on a plane to decrease chances that somebody else has a bomb. What is wrong with this argument? Problem 1.4 (Monty-Hall Problem) Assume that in a TV show the candidate is given the choice between three doors. Behind two of the doors there is a pencil and behind one there is the grand prize, a car. The candi- date chooses one door. After that , the showmaster opens another door behind which there is a pencil. Should the candidate switch doors after that? What is the probability of winning the car? Problem 1.5 (Mean and Variance for Random Variables) Denote by Xirandom variables. Prove that in this case EX1;:::XN"X ixi# =X iEXi[xi]andVarX1;:::XN"X ixi# =X iVarXi[xi] To show the second equality assume independence of the Xi. Problem 1.6 (Two Dices) Assume you have a game which uses the max- imum of two dices. Compute the probability of seeing any of the events f1;:::; 6g. Hint: prove rst that the cumulative distribution function of the maximum of a pair of random variables is the square of the original cumu- lative distribution function. Problem 1.7 (Matching Coins) Consider the following game: two play- ers bring a coin each. the rst player bets that when tossing the coins both will match and the second one bets that they will not match. Show that even if one of the players were to bring a tainted coin, the game still would be fair. Show that it is in the interest of each player to bring a fair coin to the game. Hint: assume that the second player knows that the rst coin favors heads over tails. Problem 1.8 (Randomized Maximization) How many observations do you need to draw from a distribution to ensure that the maximum over them is larger than 95% of all observations with at least 95% probability? Hint: generalize the result from Problem 1.6 to the maximum over nrandom vari- ables. Application: Assume we have 1000 computers performing MapReduce [DG08] and the Reducers have to wait until all 1000 Mappers are nished with their job. Compute the quantile of the typical time to completion.

36 1 Introduction Problem 1.9 Prove that the Normal distribution (1.3) has mean and variance2. Hint: exploit the fact that pis symmetric around . Problem 1.10 (Cauchy Distribution) Prove that for the density p(x) =1 (1 +x2)(1.33) mean and variance are undened. Hint: show that the integral diverges. Problem 1.11 (Quantiles) Find a distribution for which the mean ex- ceeds the median. Hint: the mean depends on the value of the high-quantile terms, whereas the median does not. Problem 1.12 (Multicategory Naive Bayes) Prove that for multicate- gory Naive Bayes the optimal decision is given by y(x) := argmax yp(y)nY i=1p([x]ijy) (1.34) wherey2Yis the class label of the observation x. Problem 1.13 (Bayes Optimal Decisions) Denote byy(x) = argmaxyp(yjx) the label associated with the largest conditional class probability. Prove that fory(x)the probability of choosing the wrong label yis given by l(x) := 1 p(y(x)jx): Moreover, show that y(x)is the label incurring the smallest misclassication error. Problem 1.14 (Nearest Neighbor Loss) Show that the expected loss in- curred by the nearest neighbor classier does not exceed twice the loss of the Bayes optimal decision.

2 Density Estimation 2.1 Limit Theorems Assume you are a gambler and go to a casino to play a game of dice. As it happens, it is your unlucky day and among the 100 times you toss the dice, you only see '6' eleven times. For a fair dice we know that each face should occur with equal probability1 6. Hence the expected value over 100 draws is100 617, which is considerably more than the eleven times that we observed. Before crying foul you decide that some mathematical analysis is in order. The probability of seeing a particular sequence of mtrials out of which n are a '6' is given by1 6n5 6m n. Moreover, there are m n =m! n!(m n)!dierent sequences of '6' and 'not 6' with proportions nandm nrespectively. Hence we may compute the probability of seeing a '6' only 11 or less via Pr(X11) =11X i=0p(i) =11X i=0100 i1 6i5 6100 i 7:0% (2.1) After looking at this gure you decide that things are probably reasonable. And, in fact, they are consistent with the convergence behavior of a sim- ulated dice in Figure 2.1. In computing (2.1) we have learned something useful: the expansion is a special case of a binomial series. The rst term 1234560.00.10.20.3m=10 1234560.00.10.20.3m=20 1234560.00.10.20.3m=50 1234560.00.10.20.3m=100 1234560.00.10.20.3m=200 1234560.00.10.20.3m=500 Fig. 2.1. Convergence of empirical means to expectations. From left to right: em- pirical frequencies of occurrence obtained by casting a dice 10, 20, 50, 100, 200, and 500 times respectively. Note that after 20 throws we still have not observed a single '6', an event which occurs with only5 6202:6% probability. 37

38 2 Density Estimation counts the number of congurations in which we could observe itimes '6' in a sequence of 100 dice throws. The second and third term are the probabilities of seeing one particular instance of such a sequence. Note that in general we may not be as lucky, since we may have con- siderably less information about the setting we are studying. For instance, we might not know the actual probabilities for each face of the dice, which would be a likely assumption when gambling at a casino of questionable reputation. Often the outcomes of the system we are dealing with may be continuous valued random variables rather than binary ones, possibly even with unknown range. For instance, when trying to determine the average wage through a questionnaire we need to determine how many people we need to ask in order to obtain a certain level of condence. To answer such questions we need to discuss limit theorems. They tell us by how much averages over a set of observations may deviate from the corresponding expectations and how many observations we need to draw to estimate a number of probabilities reliably. For completeness we will present proofs for some of the more fundamental theorems in Section 2.1.2. They are useful albeit non-essential for the understanding of the remainder of the book and may be omitted. 2.1.1 Fundamental Laws The Law of Large Numbers developed by Bernoulli in 1713 is one of the fundamental building blocks of statistical analysis. It states that averages over a number of observations converge to their expectations given a su- ciently large number of observations and given certain assumptions on the independence of these observations. It comes in two avors: the weak and the strong law. Theorem 2.1 (Weak Law of Large Numbers) Denote by X1;:::;Xm random variables drawn from p(x)with mean=EXi[xi]for alli. Moreover let Xm:=1 mmX i=1Xi (2.2) be the empirical average over the random variables Xi. Then for any >0 the following holds lim m!1Pr Xm  = 1: (2.3)

2.1 Limit Theorems 39 101102103123456 Fig. 2.2. The mean of a number of casts of a dice. The horizontal straight line denotes the mean 3.5. The uneven solid line denotes the actual mean Xnas a function of the number of draws, given as a semilogarithmic plot. The crosses denote the outcomes of the dice. Note how Xnever more closely approaches the mean 3 :5 are we obtain an increasing number of observations. This establishes that, indeed, for large enough sample sizes, the average will converge to the expectation. The strong law strengthens this as follows: Theorem 2.2 (Strong Law of Large Numbers) Under the conditions of Theorem 2.1 we have Pr  limm!1Xm= = 1. The strong law implies that almost surely (in a measure theoretic sense) Xm converges to , whereas the weak law only states that for every the random variable Xmwill be within the interval [  ;+]. Clearly the strong implies the weak law since the measure of the events Xm=converges to 1, hence any-ball around would capture this. Both laws justify that we may take sample averages, e.g. over a number of events such as the outcomes of a dice and use the latter to estimate their means, their probabilities (here we treat the indicator variable of the event as af0; 1g-valued random variable), their variances or related quantities. We postpone a proof until Section 2.1.2, since an eective way of proving Theo- rem 2.1 relies on the theory of characteristic functions which we will discuss in the next section. For the moment, we only give a pictorial illustration in Figure 2.2. Once we established that the random variable Xm=m 1Pm i=1Xicon- verges to its mean , a natural second question is to establish how quickly it converges and what the properties of the limiting distribution of Xm are. Note in Figure 2.2 that the initial deviation from the mean is large whereas as we observe more data the empirical mean approaches the true one.

40 2 Density Estimation 101102103123456 Fig. 2.3. Five instantiations of a running average over outcomes of a toss of a dice. Note that all of them converge to the mean 3 :5. Moreover note that they all are well contained within the upper and lower envelopes given by p VarX[x]=m. The central limit theorem answers this question exactly by addressing a slightly more general question, namely whether the sum over a number of independent random variables where each of them arises from a dierent distribution might also have a well behaved limiting distribution. This is the case as long as the variance of each of the random variables is bounded. The limiting distribution of such a sum is Gaussian. This arms the pivotal role of the Gaussian distribution. Theorem 2.3 (Central Limit Theorem) Denote byXiindependent ran- dom variables with means iand standard deviation i. Then Zm:="mX i=12 i# 1 2"mX i=1Xi i# (2.4) converges to a Normal Distribution with zero mean and unit variance. Note that just like the law of large numbers the central limit theorem (CLT) is an asymptotic result. That is, only in the limit of an innite number of observations will it become exact. That said, it often provides an excellent approximation even for nite numbers of observations, as illustrated in Fig- ure 2.4. In fact, the central limit theorem and related limit theorems build the foundation of what is known as asymptotic statistics. Example 2.1 (Dice) If we are interested in computing the mean of the values returned by a dice we may apply the CLT to the sum over mvariables

2.1 Limit Theorems 41 which have all mean = 3:5and variance (see Problem 2.1) VarX[x] =EX[x2] EX[x]2= (1 + 4 + 9 + 16 + 25 + 36) =6 3:522:92: We now study the random variable Wm:=m 1Pm i=1[Xi 3:5]. Since each of the terms in the sum has zero mean, also Wm's mean vanishes. Moreover, Wmis a multiple of Zmof (2.4). Hence we have that Wmconverges to a normal distribution with zero mean and standard deviation 2:92m 1 2. Consequently the average of mtosses of the dice yields a random vari- able with mean 3:5and it will approach a normal distribution with variance m 1 22:92. In other words, the empirical mean converges to its average at rateO(m 1 2). Figure 2.3 gives an illustration of the quality of the bounds implied by the CLT. One remarkable property of functions of random variables is that in many conditions convergence properties of the random variables are bestowed upon the functions, too. This is manifest in the following two results: a variant of Slutsky's theorem and the so-called delta method. The former deals with limit behavior whereas the latter deals with an extension of the central limit theorem. Theorem 2.4 (Slutsky's Theorem) Denote byXi;Yisequences of ran- dom variables with Xi!XandYi!cforc2Rin probability. Moreover, denote byg(x;y)a function which is continuous for all (x;c). In this case the random variable g(Xi;Yi)converges in probability to g(X;c). For a proof see e.g. [Bil68]. Theorem 2.4 is often referred to as the continuous mapping theorem (Slutsky only proved the result for ane functions). It means that for functions of random variables it is possible to pull the limiting procedure into the function. Such a device is useful when trying to prove asymptotic normality and in order to obtain characterizations of the limiting distribution. Theorem 2.5 (Delta Method) Assume that Xn2Rdis asymptotically normal with a 2 n(Xn b)!N(0;)fora2 n!0. Moreover, assume that g:Rd!Rlis a mapping which is continuously dierentiable at b. In this case the random variable g(Xn)converges a 2 n(g(Xn) g(b))!N(0;[rxg(b)][rxg(b)]>): (2.5) Proof Via a Taylor expansion we see that a 2 n[g(Xn) g(b)] = [rxg(n)]>a 2 n(Xn b) (2.6)

42 2 Density Estimation Herenlies on the line segment [ b;Xn]. SinceXn!bwe have that n!b, too. Sincegis continuously dierentiable at bwe may apply Slutsky's the- orem to see that a 2 n[g(Xn) g(b)]![rxg(b)]>a 2 n(Xn b). As a con- sequence, the transformed random variable is asymptotically normal with covariance [rxg(b)][rxg(b)]>. We will use the delta method when it comes to investigating properties of maximum likelihood estimators in exponential families. There gwill play the role of a mapping between expectations and the natural parametrization of a distribution. 2.1.2 The Characteristic Function The Fourier transform plays a crucial role in many areas of mathematical analysis and engineering. This is equally true in statistics. For historic rea- sons its applications to distributions is called the characteristic function, which we will discuss in this section. At its foundations lie standard tools from functional analysis and signal processing [Rud73, Pap62]. We begin by recalling the basic properties: Denition 2.6 (Fourier Transform) Denote byf:Rn!Ca function dened on a d-dimensional Euclidean space. Moreover, let x;!2Rn. Then the Fourier transform Fand its inverse F 1are given by F[f](!) := (2) d 2Z Rnf(x) exp( ih!;xi)dx (2.7) F 1[g](x) := (2) d 2Z Rng(!) exp(ih!;xi)d!: (2.8) The key insight is that F 1F=FF 1= Id. In other words, Fand F 1are inverses to each other for all functions which are L2integrable on Rd, which includes probability distributions. One of the key advantages of Fourier transforms is that derivatives and convolutions on ftranslate into multiplications. That is F[fg] = (2)d 2F[f]F[g]. The same rule applies to the inverse transform, i.e. F 1[fg] = (2)d 2F 1[f]F 1[g]. The benet for statistical analysis is that often problems are more easily expressed in the Fourier domain and it is easier to prove convergence results there. These results then carry over to the original domain. We will be exploiting this fact in the proof of the law of large numbers and the central limit theorem. Note that the denition of Fourier transforms can be extended to more general domains such as groups. See e.g. [BCR84] for further details.

2.1 Limit Theorems 43 We next introduce the notion of a characteristic function of a distribution.1 Denition 2.7 (Characteristic Function) Denote byp(x)a distribution of a random variable X2Rd. Then the characteristic function X(!)with !2Rdis given by X(!) := (2)d 2F 1[p(x)] =Z exp(ih!;xi)dp(x): (2.9) In other words, X(!) is the inverse Fourier transform applied to the prob- ability measure p(x). Consequently X(!)uniquely characterizes p(x) and moreover,p(x) can be recovered from X(!) via the forward Fourier trans- form. One of the key utilities of characteristic functions is that they allow us to deal in easy ways with sums of random variables. Theorem 2.8 (Sums of random variables and convolutions) Denote byX;Y2Rtwo independent random variables. Moreover, denote by Z:= X+Ythe sum of both random variables. Then the distribution over Zsat- isesp(z) =p(x)p(y). Moreover, the characteristic function yields: Z(!) =X(!)Y(!): (2.10) ProofZis given by Z=X+Y. Hence, for a given Z=zwe have the freedom to choose X=xfreely provided that Y=z x. In terms of distributions this means that the joint distribution p(z;x) is given by p(z;x) =p(Y=z x)p(x) and hencep(z) =Z p(Y=z x)dp(x) = [p(x)p(y)](z): The result for characteristic functions follows form the property of the Fourier transform. For sums of several random variables the characteristic function is the prod- uct of the individual characteristic functions. This allows us to prove both the weak law of large numbers and the central limit theorem (see Figure 2.4 for an illustration) by proving convergence in the Fourier domain. Proof [Weak Law of Large Numbers] At the heart of our analysis lies a Taylor expansion of the exponential into exp(iwx) = 1 +ihw;xi+o(jwj) and henceX(!) = 1 +iwEX[x] +o(jwj): 1In Chapter ??we will discuss more general descriptions of distributions of which Xis a special case. In particular, we will replace the exponential exp( ih!;xi) by a kernel function k(x;x0).

44 2 Density Estimation -5050.00.51.0 -5050.00.51.0 -5050.00.51.0 -5050.00.51.0 -5050.00.51.0 -1010.00.51.01.5 -1010.00.51.01.5 -1010.00.51.01.5 -1010.00.51.01.5 -1010.00.51.01.5 Fig. 2.4. A working example of the central limit theorem. The top row contains distributions of sums of uniformly distributed random variables on the interval [0:5;0:5]. From left to right we have sums of 1 ;2;4;8 and 16 random variables. The bottom row contains the same distribution with the means rescaled bypm, where mis the number of observations. Note how the distribution converges increasingly to the normal distribution. Givenmrandom variables Xiwith mean EX[x] =this means that their average Xm:=1 mPm i=1Xihas the characteristic function Xm(!) = 1 +i mw+o(m 1jwj)m (2.11) In the limit of m!1 this converges to exp( iw), the characteristic func- tion of the constant distribution with mean . This proves the claim that in the large sample limit Xmis essentially constant with mean . Proof [Central Limit Theorem] We use the same idea as above to prove the CLT. The main dierence, though, is that we need to assume that the second moments of the random variables Xiexist. To avoid clutter we only prove the case of constant mean EXi[xi] =and variance Var Xi[xi] =2.

2.1 Limit Theorems 45 LetZm:=1p m2Pm i=1(Xi ). Our proof relies on showing convergence of the characteristic function of Zm, i.e.Zmto that of a normally dis- tributed random variable Wwith zero mean and unit variance. Expanding the exponential to second order yields: exp(iwx) = 1 +iwx 1 2w2x2+o(jwj2) and henceX(!) = 1 +iwEX[x] 1 2w2VarX[x] +o(jwj2) Since the mean of Zmvanishes by centering ( Xi ) and the variance per variable ism 1we may write the characteristic function of Zmvia Zm(!) = 1 1 2mw2+o(m 1jwj2)m As before, taking limits m!1 yields the exponential function. We have that limm!1Zm(!) = exp( 1 2!2) which is the characteristic function of the normal distribution with zero mean and variance 1. Since the character- istic function transform is injective this proves our claim. Note that the characteristic function has a number of useful properties. For instance, it can also be used as moment generating function via the identity: rn !X(0) =i nEX[xn]: (2.12) Its proof is left as an exercise. See Problem 2.2 for details. This connection also implies (subject to regularity conditions) that if we know the moments of a distribution we are able to reconstruct it directly since it allows us to reconstruct its characteristic function. This idea has been exploited in density estimation [Cra46] in the form of Edgeworth and Gram-Charlier expansions [Hal92]. 2.1.3 Tail Bounds In practice we never have access to an innite number of observations. Hence the central limit theorem does not apply but is just an approximation to the real situation. For instance, in the case of the dice, we might want to state worst case bounds fornite sums of random variables to determine by how much the empirical mean may deviate from its expectation. Those bounds will not only be useful for simple averages but to quantify the behavior of more sophisticated estimators based on a set of observations. The bounds we discuss below dier in the amount of knowledge they assume about the random variables in question. For instance, we might only

46 2 Density Estimation know their mean. This leads to the Gauss-Markov inequality. If we know their mean and their variance we are able to state a stronger bound, the Chebyshev inequality. For an even stronger setting, when we know that each variable has bounded range, we will be able to state a Cherno bound. Those bounds are progressively more tight and also more dicult to prove. We state them in order of technical sophistication. Theorem 2.9 (Gauss-Markov) Denote byX0a random variable and letbe its mean. Then for any >0we have Pr(X) : (2.13) Proof We use the fact that for nonnegative random variables Pr(X) =Z1 dp(x)Z1 x dp(x) 1Z1 0xdp(x) = : This means that for random variables with a small mean, the proportion of samples with large value has to be small. Consequently deviations from the mean are O( 1). However, note that this bound does notdepend on the number of observations. A useful application of the Gauss-Markov inequality is Chebyshev's inequality. It is a statement on the range of random variables using its variance. Theorem 2.10 (Chebyshev) Denote byXa random variable with mean and variance 2. Then the following holds for >0: Pr(jx j)2 2: (2.14) Proof Denote by Y:=jX j2the random variable quantifying the deviation of Xfrom its mean . By construction we know that EY[y] =2. Next let:=2. Applying Theorem 2.9 to Yandyields Pr(Y > )2= which proves the claim. Note the improvement to the Gauss-Markov inequality. Where before we had bounds whose condence improved with O( 1) we can now state O( 2) bounds for deviations from the mean. Example 2.2 (Chebyshev bound) Assume that Xm:=m 1Pm i=1Xiis the average over mrandom variables with mean and variance 2. Hence Xmalso has mean . Its variance is given by Var Xm[xm] =mX i=1m 2VarXi[xi] =m 12:

2.1 Limit Theorems 47 Applying Chebyshev's inequality yields that the probability of a deviation offrom the mean is bounded by2 m2. For xed failure probability = Pr(jXm j>)we have 2m 1 2and equivalently =p m: This bound is quite reasonable for large but it means that for high levels of condence we need a huge number of observations. Much stronger results can be obtained if we are able to bound the range of the random variables. Using the latter, we reap an exponential improve- ment in the quality of the bounds in the form of the McDiarmid [McD89] inequality. We state the latter without proof: Theorem 2.11 (McDiarmid) Denote by f:Xm!Ra function on X and letXibe independent random variables. In this case the following holds: Pr (jf(x1;:::;xm) EX1;:::;Xm[f(x1;:::;xm)]j>)2 exp   22C 2 : Here the constant C2is given by C2=Pm i=1c2 iwhere f(x1;:::;xi;:::;xm) f(x1;:::;x0 i;:::;xm)ci for allx1;:::;xm;x0 iand for all i. This bound can be used for averages of a number of observations when they are computed according to some algorithm as long as the latter can be encoded in f. In particular, we have the following bound [Hoe63]: Theorem 2.12 (Hoeding) Denote byXiiid random variables with bounded rangeXi2[a;b]and mean. Let Xm:=m 1Pm i=1Xibe their average. Then the following bound holds: Pr Xm > 2 exp  2m2 (b a)2 : (2.15) Proof This is a corollary of Theorem 2.11. In Xmeach individual random variable has range [ a=m;b=m ] and we set f(X1;:::;Xm) := Xm. Straight- forward algebra shows that C2=m 2(b a)2. Plugging this back into McDiarmid's theorem proves the claim. Note that (2.15) is exponentially better than the previous bounds. With increasing sample size the condence level also increases exponentially. Example 2.3 (Hoeding bound) As in example 2.2 assume that Xiare iid random variables and let Xmbe their average. Moreover, assume that

48 2 Density Estimation Xi2[a;b]for alli. As before we want to obtain guarantees on the probability thatjXm j>. For a given level of condence 1 we need to solve 2 exp  2m2 (b a)2 (2.16) for. Straightforward algebra shows that in this case needs to satisfy jb ajp [log 2 log]=2m (2.17) In other words, while the condence level only enters logarithmically into the inequality, the sample size mimproves our condence only with =O(m 1 2). That is, in order to improve our condence interval from = 0:1to= 0:01 we need 100 times as many observations. While this bound is tight (see Problem 2.5 for details), it is possible to ob- tain better bounds if we know additional information. In particular knowing a bound on the variance of a random variable in addition to knowing that it has bounded range would allow us to strengthen the statement considerably. The Bernstein inequality captures this connection. For details see [BBL05] or works on empirical process theory [vdVW96, SW86, Vap82]. 2.1.4 An Example It is probably easiest to illustrate the various bounds using a concrete exam- ple. In a semiconductor fab processors are produced on a wafer. A typical 300mm wafer holds about 400 chips. A large number of processing steps are required to produce a nished microprocessor and often it is impossible to assess the eect of a design decision until the nished product has been produced. Assume that the production manager wants to change some step from process 'A' to some other process 'B'. The goal is to increase the yield of the process, that is, the number of chips of the 400 potential chips on the wafer which can be sold. Unfortunately this number is a random variable, i.e. the number of working chips per wafer can vary widely between dierent wafers. Since process 'A' has been running in the factory for a very long time we may assume that the yield is well known, say it is A= 350 out of 400 processors on average. It is our goal to determine whether process 'B' is better and what its yield may be. Obviously, since production runs are expensive we want to be able to determine this number as quickly as possible, i.e. using as few wafers as possible. The production manager is risk averse and wants to ensure that the new process is really better. Hence he requires a condence level of 95% before he will change the production.

2.1 Limit Theorems 49 A rst step is to formalize the problem. Since we know process 'A' exactly we only need to concern ourselves with 'B'. We associate the random variable Xiwith wafer i. A reasonable (and somewhat simplifying) assumption is to posit that all Xiare independent and identically distributed where all Xi have the mean B. Obviously we do not know B| otherwise there would be no reason for testing! We denote by Xmthe average of the yields of m wafers using process 'B'. What we are interested in is the accuracy for which the probability = Pr(jXm Bj>) satises0:05: Let us now discuss how the various bounds behave. For the sake of the argument assume that B A= 20, i.e. the new process produces on average 20 additional usable chips. Chebyshev In order to apply the Chebyshev inequality we need to bound the variance of the random variables Xi. The worst possible variance would occur ifXi2f0; 400gwhere both events occur with equal probability. In other words, with equal probability the wafer if fully usable or it is entirely broken. This amounts to 2= 0:5(200 0)2+ 0:5(200 400)2= 40;000. Since for Chebyshev bounds we have 2m 1 2(2.18) we can solve for m=2=2= 40;000=(0:05400) = 20;000. In other words, we would typically need 20,000 wafers to assess with reasonable condence whether process 'B' is better than process 'A'. This is completely unrealistic. Slightly better bounds can be obtained if we are able to make better assumptions on the variance. For instance, if we can be sure that the yield of process 'B' is at least 300, then the largest possible variance is 0 :25(300  0)2+ 0:75(300 400)2= 30;000, leading to a minimum of 15,000 wafers which is not much better. Hoeding Since the yields are in the interval f0;:::; 400gwe have an ex- plicit bound on the range of observations. Recall the inequality (2.16) which bounds the failure probably = 0:05 by an exponential term. Solving this formyields m0:5jb aj2 2log(2=)737:8 (2.19) In other words, we need at lest 738 wafers to determine whether process 'B' is better. While this is a signicant improvement of almost two orders of magnitude, it still seems wasteful and we would like to do better.

50 2 Density Estimation Central Limit Theorem The central limit theorem is an approximation . This means that our reasoning is not accurate any more. That said, for large enough sample sizes, the approximation is good enough to use it for practical predictions. Assume for the moment that we knew the variance 2 exactly. In this case we know that Xmis approximately normal with mean Band variance m 12. We are interested in the interval [  ;+] which contains 95% of the probability mass of a normal distribution. That is, we need to solve the integral 1 22Z+  exp  (x )2 22 dx= 0:95 (2.20) This can be solved eciently using the cumulative distribution function of a normal distribution (see Problem 2.3 for more details). One can check that (2.20) is solved for = 2:96. In other words, an interval of 2:96 contains 95% of the probability mass of a normal distribution. The number of observations is therefore determined by = 2:96=pmand hencem= 8:762 2(2.21) Again, our problem is that we do notknow the variance of the distribution. Using the worst-case bound on the variance, i.e. 2= 40;000 would lead to a requirement of at least m= 876 wafers for testing. However, while we do notknow the variance, we may estimate it along with the mean and use the empirical estimate, possibly plus some small constant to ensure we do not underestimate the variance, instead of the upper bound. Assuming that uctuations turn out to be in the order of 50 processors, i.e.2= 2500, we are able to reduce our requirement to approximately 55 wafers. This is probably an acceptable number for a practical test. Rates and Constants The astute reader will have noticed that all three condence bounds had scaling behavior m=O( 2). That is, in all cases the number of observations was a fairly ill behaved function of the amount of condence required. If we were just interested in convergence per se, a statement like that of the Chebyshev inequality would have been entirely sucient. The various laws and bounds can often be used to obtain con- siderably better constants for statistical condence guarantees. For more complex estimators, such as methods to classify, rank, or annotate data, a reasoning such as the one above can become highly nontrivial. See e.g. [MYA94, Vap98] for further details.

2.2 Parzen Windows 51 2.2 Parzen Windows 2.2.1 Discrete Density Estimation The convergence theorems discussed so far mean that we can use empir- ical observations for the purpose of density estimation. Recall the case of the Naive Bayes classier of Section 1.3.1. One of the key ingredients was the ability to use information about word counts for dierent document classes to estimate the probability p(wjjy), wherewjdenoted the number of occurrences of word jin document x, given that it was labeled y. In the following we discuss an extremely simple and crude method for estimating probabilities. It relies on the fact that for random variables Xidrawn from distribution p(x) with discrete values Xi2Xwe have lim m!1^pX(x) =p(x) (2.22) where ^pX(x) :=m 1mX i=1fxi=xgfor allx2X: (2.23) Let us discuss a concrete case. We assume that we have 12 documents and would like to estimate the probability of occurrence of the word 'dog' from it. As raw data we have: Document ID 1 2 3 4 5 6 7 8 9 10 11 12 Occurrences of `dog' 1 0 2 0 4 6 3 0 6 2 0 1 This means that the word `dog' occurs the following number of times: Occurrences of `dog' 0 1 2 3 4 5 6 Number of documents 4 2 2 1 1 0 2 Something unusual is happening here: for some reason we never observed 5 instances of the word dog in our documents, only 4 and less, or alter- natively 6 times. So what about 5 times? It is reasonable to assume that the corresponding value should not be 0 either. Maybe we did not sample enough. One possible strategy is to add pseudo-counts to the observations. This amounts to the following estimate: ^pX(x) := (m+jXj) 1h 1 +mX i=1fxi=xg=p(x)i (2.24) Clearly the limit for m!1 is stillp(x). Hence, asymptotically we do not lose anything. This prescription is what we used in Algorithm 1.1 used a method called Laplace smoothing. Below we contrast the two methods:

52 2 Density Estimation Occurrences of `dog' 0 1 2 3 4 5 6 Number of documents 4 2 2 1 1 0 2 Frequency of occurrence 0.33 0.17 0.17 0.083 0.083 0 0.17 Laplace smoothing 0.26 0.16 0.16 0.11 0.11 0.05 0.16 The problem with this method is that as jXjincreases we need increasingly more observations to obtain even a modicum of precision. On average, we will need at least one observation for every x2X. This can be infeasible for large domains as the following example shows. Example 2.4 (Curse of Dimensionality) Assume that X=f0;1gd, i.e. xconsists of binary bit vectors of dimensionality d. Asdincreases the size of Xincreases exponentially, requiring an exponential number of observations to perform density estimation. For instance, if we work with images, a 100  100 black and white picture would require in the order of 103010observations to model such fairly low-resolution images accurately. This is clearly utterly infeasible | the number of particles in the known universe is in the order of1080. Bellman [Bel61] was one of the rst to formalize this dilemma by coining the term 'curse of dimensionality'. This example clearly shows that we need better tools to deal with high- dimensional data. We will present one of such tools in the next section. 2.2.2 Smoothing Kernel We now proceed to proper density estimation. Assume that we want to estimate the distribution of weights of a population. Sample data from a population might look as follows: X=f57, 88, 54, 84, 83, 59, 56, 43, 70, 63, 90, 98, 102, 97, 106, 99, 103, 112 g. We could use this to perform a density estimate by placing discrete components at the locations xi2Xwith weight 1=jXjas what is done in Figure 2.5. There is no reason to believe that weights are quantized in kilograms, or grams, or miligrams (or pounds and stones). And even if it were, we would expect that similar weights would have similar densities associated with it. Indeed, as the right diagram of Figure 2.5 shows, the corresponding density is continuous. The key question arising is how we may transform Xinto a realistic estimate of the density p(x). Starting with a 'density estimate' with only discrete terms ^p(x) =1 mmX i=1(x xi) (2.25)

2.2 Parzen Windows 53 we may choose to smooth it out by a smoothing kernel h(x) such that the probability mass becomes somewhat more spread out. For a density estimate onXRdthis is achieved by ^p(x) =1 mmX i=1r dh x xi r : (2.26) This expansion is commonly known as the Parzen windows estimate. Note that obviously hmust be chosen such that h(x)0 for allx2Xand moreover thatR h(x)dx= 1 in order to ensure that (2.26) is a proper prob- ability distribution. We now formally justify this smoothing. Let Rbe a small region such that q=Z Rp(x)dx: Out of the msamples drawn from p(x), the probability that kof them fall in regionRis given by the binomial distribution m k qk(1 q)m k: The expected fraction of points falling inside the region can easily be com- puted from the expected value of the Binomial distribution: E[k=m] =q. Similarly, the variance can be computed as Var[ k=m] =q(1 q)=m. As m!1 the variance goes to 0 and hence the estimate peaks around the expectation. We can therefore set kmq: If we assume that Ris so small that p(x) is constant over R, then qp(x)V; whereVis the volume of R. Rearranging we obtain p(x)k mV: (2.27) Let us now set Rto be a cube with side length r, and dene a function h(u) =( 1 ifjuij1 2 0 otherwise : Observe that h x xi r is 1 if and only if xilies inside a cube of size rcentered

54 2 Density Estimation aroundx. If we let k=mX i=1hx xi r ; then one can use (2.27) to estimate pvia ^p(x) =1 mmX i=1r dhx xi r ; whererdis the volume of the hypercube of size rinddimensions. By symme- try, we can interpret this equation as the sum over mcubes centered around mdata points xn. If we replace the cube by any smooth kernel function h() this recovers (2.26). There exists a large variety of dierent kernels which can be used for the kernel density estimate. [Sil86] has a detailed description of the properties of a number of kernels. Popular choices are h(x) = (2) 1 2e 1 2x2Gaussian kernel (2.28) h(x) =1 2e jxjLaplace kernel (2.29) h(x) =3 4max(0;1 x2) Epanechnikov kernel (2.30) h(x) =1 2[ 1;1](x) Uniform kernel (2.31) h(x) = max(0;1 jxj) Triangle kernel : (2.32) Further kernels are the triweight and the quartic kernel which are basically powers of the Epanechnikov kernel. For practical purposes the Gaussian ker- nel (2.28) or the Epanechnikov kernel (2.30) are most suitable. In particular, the latter has the attractive property of compact support. This means that for any given density estimate at location xwe will only need to evaluate termsh(xi x) for which the distance kxi xkis less than r. Such expan- sions are computationally much cheaper, in particular when we make use of fast nearest neighbor search algorithms [GIM99, IM98]. Figure 2.7 has some examples of kernels. 2.2.3 Parameter Estimation So far we have not discussed the issue of parameter selection. It should be evident from Figure 2.6, though, that it is quite crucial to choose a good kernel width. Clearly, a kernel that is overly wide will oversmooth any ne detail that there might be in the density. On the other hand, a very narrow kernel will not be very useful, since it will be able to make statements only about the locations where we actually observed data.

2.2 Parzen Windows 55 40  50  60  70  80  90  100 1100.00 0.050.10 40  50  60  70  80  90  100 1100.00 0.01 0.02 0.03 0.040.05 Fig. 2.5. Left: a naive density estimate given a sample of the weight of 18 persons. Right: the underlying weight distribution. 4060801000.0000.0250.0504060801000.0000.0250.0504060801000.0000.0250.0504060801000.0000.0250.050 Fig. 2.6. Parzen windows density estimate associated with the 18 observations of the Figure above. From left to right: Gaussian kernel density estimate with kernel of width 0:3;1;3, and 10 respectively. -2-10120.00.51.0-2-10120.00.51.0-2-10120.00.51.0-2-10120.00.51.0 Fig. 2.7. Some kernels for Parzen windows density estimation. From left to right: Gaussian kernel, Laplace kernel, Epanechikov kernel, and uniform density. Moreover, there is the issue of choosing a suitable kernel function. The fact that a large variety of them exists might suggest that this is a crucial issue. In practice, this turns out not to be the case and instead, the choice of a suitable kernel width is much more vital for good estimates. In other words, size matters, shape is secondary. The problem is that we do not know which kernel width is best for the data. If the problem is one-dimensional, we might hope to be able to eyeball the size ofr. Obviously, in higher dimensions this approach fails. A second

56 2 Density Estimation option would be to choose rsuch that the log-likelihood of the data is maximized. It is given by logmY i=1p(xi) = mlogm+mX i=1logmX j=1r dhxi xj r (2.33) Remark 2.13 (Log-likelihood) We consider the logarithm of the likeli- hood for reasons of computational stability to prevent numerical underow. While each term p(xi)might be within a suitable range, say 10 2, the prod- uct of 1000 of such terms will easily exceed the exponent of oating point representations on a computer. Summing over the logarithm, on the other hand, is perfectly feasible even for large numbers of observations. Unfortunately computing the log-likelihood is equally infeasible: for decreas- ingrthe only surviving terms in (2.33) are the functions h((xi xi)=r) = h(0), since the arguments of all other kernel functions diverge. In other words, the log-likelihood is maximized when p(x) is peaked exactly at the locations where we observed the data. The graph on the left of Figure 2.6 shows what happens in such a situation. What we just experienced is a case of overtting where our model is too exible. This led to a situation where our model was able to explain the observed data \unreasonably well", simply because we were able to adjust our parameters given the data. We will encounter this situation throughout the book. There exist a number of ways to address this problem. Validation Set: We could use a subset of our set of observations as an estimate of the log-likelihood. That is, we could partition the obser- vations into X:=fx1;:::;xngandX0:=fxn+1;:::;xmgand use the second part for a likelihood score according to (2.33). The second set is typically called a validation set . n-fold Cross-validation: Taking this idea further, note that there is no particular reason why any given xishould belong to XorX0respec- tively. In fact, we could use all splits of the observations into sets XandX0to infer the quality of our estimate. While this is compu- tationally infeasible, we could decide to split the observations into nequally sized subsets, say X1;:::;Xnand use each of them as a validation set at a time while the remainder is used to generate a density estimate. Typicallynis chosen to be 10, in which case this procedure is

2.2 Parzen Windows 57 referred to as 10-fold cross-validation . It is a computationally at- tractive procedure insofar as it does not require us to change the basic estimation algorithm. Nonetheless, computation can be costly. Leave-one-out Estimator: At the extreme end of cross-validation we could choosen=m. That is, we only remove a single observation at a time and use the remainder of the data for the estimate. Using the average over the likelihood scores provides us with an even more ne-grained estimate. Denote by pi(x) the density estimate obtained by using X:=fx1;:::;xmgwithoutxi. For a Parzen windows estimate this is given by pi(xi) = (m 1) 1X j6=ir dhxi xj r =m m 1h p(xi) r dh(0)i : (2.34) Note that this is precisely the term r dh(0) that is removed from the estimate. It is this term which led to divergent estimates for r!0. This means that the leave-one-out log-likelihood estimate can be computed easily via L(X) =mlogm m 1+mX i=1logh p(xi) r dh(0)i : (2.35) We then choose rsuch thatL(X) is maximized. This strategy is very robust and whenever it can be implemented in a computationally ecient manner, it is very reliable in performing model selection. An alternative, probably more of theoretical interest, is to choose the scale r a priori based on the amount of data we have at our disposition. Intuitively, we need a scheme which ensures that r!0 as the number of observations increasesm!1 . However, we need to ensure that this happens slowly enough that the number of observations within range rkeeps on increasing in order to ensure good statistical performance. For details we refer the reader to [Sil86]. Chapter ??discusses issues of model selection for estimators in general in considerably more detail. 2.2.4 Silverman's Rule Assume you are an aspiring demographer who wishes to estimate the popu- lation density of a country, say Australia. You might have access to a limited census which, for a random portion of the population determines where they live. As a consequence you will obtain a relatively high number of samples

58 2 Density Estimation Fig. 2.8. Nonuniform density. Left: original density with samples drawn from the distribution. Middle: density estimate with a uniform kernel. Right: density estimate using Silverman's adjustment. of city dwellers, whereas the number of people living in the countryside is likely to be very small. If we attempt to perform density estimation using Parzen windows, we will encounter an interesting dilemma: in regions of high density (i.e. the cities) we will want to choose a narrow kernel width to allow us to model the variations in population density accurately. Conversely, in the outback, a very wide kernel is preferable, since the population there is very low. Unfortunately, this information is exactly what a density estimator itself could tell us. In other words we have a chicken and egg situation where having a good density estimate seems to be necessary to come up with a good density estimate. Fortunately this situation can be addressed by realizing that we do not actually need to know the density but rather a rough estimate of the latter. This can be obtained by using information about the average distance of the knearest neighbors of a point. One of Silverman's rules of thumb [Sil86] is to chooserias ri=c kX x2kNN (xi)kx xik: (2.36) Typicallycis chosen to be 0 :5 andkis small, e.g. k= 9 to ensure that the estimate is computationally ecient. The density estimate is then given by p(x) =1 mmX i=1r d ih x xi ri : (2.37) Figure 2.8 shows an example of such a density estimate. It is clear that a locality dependent kernel width is better than choosing a uniformly constant kernel density estimate. However, note that this increases the computational complexity of performing a density estimate, since rst the knearest neigh- bors need to be found before the density estimate can be carried out.

2.2 Parzen Windows 59 2.2.5 Watson-Nadaraya Estimator Now that we are able to perform density estimation we may use it to perform classication and regression. This leads us to an eective method for non- parametric data analysis, the Watson-Nadaraya estimator [Wat64, Nad65]. The basic idea is very simple: assume that we have a binary classication problem, i.e. we need to distinguish between two classes. Provided that we are able to compute density estimates p(x) given a set of observations Xwe could appeal to Bayes rule to obtain p(yjx) =p(xjy)p(y) p(x)=my m1 myP i:yi=yr dh xi x r 1 mPm i=1r dh xi x r: (2.38) Here we only take the sum over all xiwith labelyi=yin the numerator. The advantage of this approach is that it is very cheap to design such an estimator. After all, we only need to compute sums. The downside, similar to that of the k-nearest neighbor classier is that it may require sums (or search) over a large number of observations. That is, evaluation of (2.38) is potentially an O(m) operation. Fast tree based representations can be used to accelerate this [BKL06, KM00], however their behavior depends signi- cantly on the dimensionality of the data. We will encounter computationally more attractive methods at a later stage. For binary classication (2.38) can be simplied considerably. Assume thaty2f 1g. Forp(y= 1jx)>0:5 we will choose that we should estimate y= 1 and in the converse case we would estimate y= 1. Taking the dierence between twice the numerator and the denominator we can see that the function f(x) =P iyih xi x r P ih xi x r=X iyih xi x r P ih xi x r=:X iyiwi(x) (2.39) can be used to achieve the same goal since f(x)>0()p(y= 1jx)>0:5. Note thatf(x) is a weighted combination of the labels yiassociated with weightswi(x) which depend on the proximity of xto an observation xi. In other words, (2.39) is a smoothed-out version of the k-nearest neighbor classier of Section 1.3.2. Instead of drawing a hard boundary at the kclosest observation we use a soft weighting scheme with weights wi(x) depending on which observations are closest. Note furthermore that the numerator of (2.39) is very similar to the simple classier of Section 1.3.3. In fact, for kernels k(x;x0) such as the Gaussian RBF kernel, which are also kernels in the sense of a Parzen windows den- sity estimate, i.e. k(x;x0) =r dh x x0 r the two terms are identical. This

60 2 Density Estimation Fig. 2.9. Watson Nadaraya estimate. Left: a binary classier. The optimal solution would be a straight line since both classes were drawn from a normal distribution with the same variance. Right: a regression estimator. The data was generated from a sinusoid with additive noise. The regression tracks the sinusoid reasonably well. means that the Watson Nadaraya estimator provides us with an alternative explanation as to why (1.24) leads to a usable classier. In the same fashion as the Watson Nadaraya classier extends the k- nearest neighbor classier we also may construct a Watson Nadaraya re- gression estimator by replacing the binary labels yiby real-valued values yi2Rto obtain the regression estimatorP iyiwi(x). Figure 2.9 has an ex- ample of the workings of both a regression estimator and a classier. They are easy to use and they work well for moderately dimensional data. 2.3 Exponential Families Distributions from the exponential family are some of the most versatile tools for statistical inference. Gaussians, Poisson, Gamma and Wishart dis- tributions all form part of the exponential family. They play a key role in dealing with graphical models, classication, regression and conditional ran- dom elds which we will encounter in later parts of this book. Some of the reasons for their popularity are that they lead to convex optimization prob- lems and that they allow us to describe probability distributions by linear models. 2.3.1 Basics Densities from the exponential family are dened by p(x;) :=p0(x) exp (h(x);i g()): (2.40)

2.3 Exponential Families 61 Herep0(x) is a density on Xand is often called the base measure, (x) is a map from xto the sucient statistics (x).is commonly referred to as thenatural parameter. It lives in the space dual to (x). Moreover, g() is a normalization constant which ensures that p(x) is properly normalized. gis often referred to as the log-partition function. The name stems from physics whereZ=eg()denotes the number of states of a physical ensemble. gcan be computed as follows: g() = logZ Xexp (h(x);i)dx: (2.41) Example 2.5 (Binary Model) Assume that X=f0; 1gand that(x) = x. In this case we have g() = log e0+e = log 1 +e . It follows that p(x= 0;) =1 1+eandp(x= 1;) =e 1+e. In other words, by choosing dierent values of one can recover dierent Bernoulli distributions. One of the convenient properties of exponential families is that the log- partition function gcan be used to generate moments of the distribution itself simply by taking derivatives. Theorem 2.14 (Log partition function) The function g()is convex. Moreover, the distribution p(x;)satises rg() =Ex[(x)]andr2 g() = Varx[(x)]: (2.42) Proof Note thatr2 g() = Varx[(x)] implies that gis convex, since the covariance matrix is positive semidenite. To show (2.42) we expand rg() =R X(x) exph(x);idxR Xexph(x);i=Z (x)p(x;)dx=Ex[(x)]:(2.43) Next we take the second derivative to obtain r2 g() =Z X(x) [(x) rg()]>p(x;)dx (2.44) =Exh (x)(x)>i  Ex[(x)]Ex[(x)]>(2.45) which proves the claim. For the rst equality we used (2.43). For the second line we used the denition of the variance. One may show that higher derivatives rn g() generate higher order cu- mulants of (x) underp(x;). This is why gis often also referred as the cumulant-generating function. Note that in general, computation of g()

62 2 Density Estimation is nontrivial since it involves solving a highdimensional integral. For many cases, in fact, the computation is NP hard, for instance when Xis the do- main of permutations [FJ95]. Throughout the book we will discuss a number of approximation techniques which can be applied in such a case. Let us briey illustrate (2.43) using the binary model of Example 2.5. We have thatr=e 1+eandr2 =e (1+e)2:This is exactly what we would have obtained from direct computation of the mean p(x= 1;) and variance p(x= 1;) p(x= 1;)2subject to the distribution p(x;). 2.3.2 Examples A large number of densities are members of the exponential family. Note, however, that in statistics it is not common to express them in the dot product formulation for historic reasons and for reasons of notational com- pactness. We discuss a number of common densities below and show why they can be written in terms of an exponential family. A detailed description of the most commonly occurring types are given in a table. Gaussian Letx;2Rdand let 2Rddwhere 0, that is,  is a positive denite matrix. In this case the normal distribution can be expressed via p(x) = (2) d 2jj 1 2exp  1 2(x )> 1(x ) (2.46) = exp x>  1 + tr  1 2xx>  1  c(;) wherec(;) =1 2> 1+d 2log 2+1 2logjj. By combining the terms inxinto(x) := (x; 1 2xx>) we obtain the sucient statistics ofx. The corresponding linear coecients ( 1; 1) constitute the natural parameter . All that remains to be done to express p(x) in terms of (2.40) is to rewrite g() in terms of c(;). The summary table on the following page contains details. Multinomial Another popular distribution is one over kdiscrete events. In this case X=f1;:::;kgand we have in completely generic terms p(x) =xwherex0 andP xx= 1. Now denote by ex2Rkthe x-th unit vector of the canonical basis, that is hex;ex0i= 1 ifx=x0 and 0 otherwise. In this case we may rewrite p(x) via p(x) =x= exp (hex;logi) (2.47) where log= (log1;:::; logk). In other words, we have succeeded

2.3 Exponential Families 63 in rewriting the distribution as a member of the exponential family where(x) =exand where= log. Note that in this denition  is restricted to a k 1 dimensional manifold (the kdimensional prob- ability simplex). If we relax those constraints we need to ensure that p(x) remains normalized. Details are given in the summary table. Poisson This distribution is often used to model distributions over discrete events. For instance, the number of raindrops which fall on a given surface area in a given amount of time, the number of stars in a given volume of space, or the number of Prussian soldiers killed by horse-kicks in the Prussian cavalry all follow this distribution. It is given by p(x) =e x x!=1 x!exp (xlog ) wherex2N0: (2.48) By dening (x) =xwe obtain an exponential families model. Note that things are a bit less trivial here since1 x!is the nonuniform counting measure onN0. The case of the uniform measure which leads to the exponential distribution is discussed in Problem 2.16. The reason why many discrete processes follow the Poisson distri- bution is that it can be seen as the limit over the average of a large number of Bernoulli draws: denote by z2f0;1ga random variable withp(z= 1) =. Moreover, denote by znthe sum over ndraws from this random variable. In this case znfollows the multinomial distribution with p(zn=k) = n k k(1 )n k. Now assume that we letn!1 such that the expected value of znremains constant. That is, we rescale = n. In this case we have p(zn=k) =n! (n k)!k!k nk 1  nn k (2.49) =k k! 1  nn" n! nk(n k)! 1  nk# Forn!1 the second term converges to e . The third term con- verges to 1, since we have a product of only 2 kterms, each of which converge to 1. Using the exponential families notation we may check thatE[x] =and that moreover also Var[ x] =. Beta This is a distribution on the unit interval X= [0;1] which is very versatile when it comes to modelling unimodal and bimodal distri-

64 2 Density Estimation 0  5  10  15  20  25 300.00 0.05 0.10 0.15 0.20 0.25 0.30 0.350.40 0.0  0.2  0.4  0.6  0.8 1.00.0 0.5 1.0 1.5 2.0 2.5 3.03.5 Fig. 2.10. Left: Poisson distributions with =f1;3;10g. Right: Beta distributions witha= 2 andb2 f1;2;3;5;7g. Note how with increasing bthe distribution becomes more peaked close to the origin. butions. It is given by p(x) =xa 1(1 x)b 1 (a+b)  (a) (b): (2.50) Taking logarithms we see that this, too, is an exponential families distribution, since p(x) = exp((a 1) logx+ (b 1) log(1 x) + log  (a+b) log  (a) log  (b)). Figure 2.10 has a graphical description of the Poisson distribution and the Beta distribution. For a more comprehensive list of exponential family dis- tributions see the table below and [Fel71, FT94, MN83]. In principle any map(x), domain Xwith underlying measure are suitable, as long as the log-partition function g() can be computed eciently. Theorem 2.15 (Convex feasible domain) The domain of denition  ofg()is convex. Proof By construction gis convex and dierentiable everywhere. Hence the below-sets for all values cwithfxjg(x)cgexist. Consequently the domain of denition is convex. Having a convex function is very valuable when it comes to parameter infer- ence since convex minimization problems have unique minimum values and global minima. We will discuss this notion in more detail when designing maximum likelihood estimators.

2.3 Exponential Families 65Name Domain XMeasure (x) g() Domain  Bernoulli f0;1g Counting x log  1 +e R Multinomial f1::Ng Counting ex logPN i=1ei RN Exponential N+ 0 Counting x  log  1 e ( 1;0) Poisson N+ 01 x!x eR Laplace [0 ;1) Lebesgue x log ( 1;0) Gaussian R Lebesgue  x; 1 2x21 2log 2 1 2log2+1 22 1 2R(0;1) RnLebesgue  x; 1 2xx>n 2log 2 1 2logj2j+1 2> 1 1 21RnCn Inverse Normal [0 ;1)x 3 2   x; 1 x1 2log 2p12 1 2log2 (0;1)2 Beta [0 ;1]1 x(1 x)(logx;log (1 x)) log (1) (2)  (1+2)R2 Gamma [0 ;1)1 x(logx; x) log  ( 1) 1log2 (0;1)2 Wishart CnjXj n+1 2  logjxj; 1 2x  1logj2j+1nlog 2 RCn +Pn i=1log    1+1 i 2 Dirichlet Sn (Qn i=1xi) 1(logx1;:::; logxn)Pn i=1log  (i) log   (Pn i=1i) (R+)n Inverse2R+e 1 2x logx ( 1) log 2 + log(  1) (0 ;1) Logarithmic N1 xx log( log(1 e)) (  1;0) Conjugate  Lebesgue ( ; g()) generic Sndenotes the probability simplex in ndimensions. Cnis the cone of positive semidenite matrices in Rnn.

66 2 Density Estimation 2.4 Estimation In many statistical problems the challenge is to estimate parameters of in- terest. For instance, in the context of exponential families, we may want to estimate a parameter ^such that it is close to the \true" parameter  in the distribution. While the problem is fully general, we will describe the relevant steps in obtaining estimates for the special case of the exponential family. This is done for two reasons | rstly, exponential families are an important special case and we will encounter slightly more complex variants on the reasoning in later chapters of the book. Secondly, they are of a su- ciently simple form that we are able to show a range of dierent techniques. In more advanced applications only a small subset of those methods may be practically feasible. Hence exponential families provide us with a working example based on which we can compare the consequences of a number of dierent techniques. 2.4.1 Maximum Likelihood Estimation Whenever we have a distribution p(x;) parametrized by some parameter we may use data to nd a value of which maximizes the likelihood that the data would have been generated by a distribution with this choice of parameter. For instance, assume that we observe a set of temperature measurements X=fx1;:::;xmg. In this case, we could try nding a normal distribution such that the likelihood p(X;) of the data under the assumption of a normal distribution is maximized. Note that this does notimply in any way that the temperature measurements are actually drawn from a normal distribution. Instead, it means that we are attempting to nd the Gaussian which ts the data in the best fashion. While this distinction may appear subtle, it is critical: we do notassume that our model accurately reects reality. Instead, we simply try doing the best possible job at modeling the data given a specied model class. Later we will encounter alternative approaches at estimation, namely Bayesian methods, which make the assumption that our model ought to be able to describe the data accurately. Denition 2.16 (Maximum Likelihood Estimator) For a model p(;) parametrized by and observations Xthe maximum likelihood estimator (MLE) is ^ML[X] := argmax p(X;): (2.51)

2.4 Estimation 67 In the context of exponential families this leads to the following procedure: givenmobservations drawn iid from some distribution, we can express the joint likelihood as p(X;) =mY i=1p(xi;) =mY i=1exp (h(xi);i g()) (2.52) = exp (m(h[X];i g())) (2.53) where[X] :=1 mmX i=1(xi): (2.54) Here[X] is the empirical average of the map (x). Maximization of p(X;) is equivalent to minimizing the negative log-likelihood  logp(X;). The latter is a common practical choice since for independently drawn data, the product of probabilities decomposes into the sum of the logarithms of individual likelihoods. This leads to the following objective function to be minimized  logp(X;) =m[g() h;[X]i] (2.55) Sinceg() is convex andh;[X]iis linear in, it follows that minimization of (2.55) is a convex optimization problem. Using Theorem 2.14 and the rst order optimality condition rg() =[X] for (2.55) implies that = [rg] 1([X]) or equivalently Exp(x;)[(x)] =rg() =[X]: (2.56) Put another way, the above conditions state that we aim to nd the distribu- tionp(x;) which has the same expected value of (x) as what we observed empirically via [X]. Under very mild technical conditions a solution to (2.56) exists. In general, (2.56) cannot be solved analytically. In certain special cases, though, this is easily possible. We discuss two such choices in the following: Multinomial and Poisson distributions. Example 2.6 (Poisson Distribution) For the Poisson distribution1where p(x;) =1 x!exp(x e)it follows that g() =eand(x) =x. This allows 1Often the Poisson distribution is specied using := logas its rate parameter. In this case we havep(x;) =xe =x! as its parametrization. The advantage of the natural parametrization usingis that we can directly take advantage of the properties of the log-partition function as generating the cumulants of x.

68 2 Density Estimation us to solve (2.56) in closed form using rg() =e=1 mmX i=1xiand hence= logmX i=1xi logm: (2.57) Example 2.7 (Multinomial Distribution) For the multinomial distri- bution the log-partition function is given by g() = logPN i=1ei, hence we have that rig() =ei PN j=1ej=1 mmX j=1fxj=ig: (2.58) It is easy to check that (2.58) is satised for ei=Pm j=1fxj=ig. In other words, the MLE for a discrete distribution simply given by the empirical frequencies of occurrence. The multinomial setting also exhibits two rather important aspects of ex- ponential families: rstly, choosing i=c+ logPm i=1fxj=igfor anyc2R will lead to an equivalent distribution. This is the case since the sucient statistic(x) is not minimal. In our context this means that the coordinates of(x) are linearly dependent | for any xwe have thatP j[(x)]j= 1, hence we could eliminate one dimension. This is precisely the additional degree of freedom which is reected in the scaling freedom in . Secondly, for data where some events do not occur at all, the expression loghPm j=1fxj=igi = log 0 is ill dened. This is due to the fact that this particular set of counts occurs on the boundary of the convex set within which the natural parameters are well dened. We will see how dierent types of priors can alleviate the issue. Using the MLE is not without problems. As we saw in Figure 2.1, conver- gence can be slow, since we are not using any side information. The latter can provide us with problems which are both numerically better conditioned and which show better convergence, provided that our assumptions are ac- curate . Before discussing a Bayesian approach to estimation, let us discuss basic statistical properties of the estimator. 2.4.2 Bias, Variance and Consistency When designing any estimator ^(X) we would like to obtain a number of desirable properties: in general it should not be biased towards a particular solution unless we have good reason to believe that this solution should be preferred. Instead, we would like the estimator to recover, at least on

2.4 Estimation 69 average, the \correct" parameter, should it exist. This can be formalized in the notion of an unbiased estimator. Secondly, we would like that, even if no correct parameter can be found, e.g. when we are trying to t a Gaussian distribution to data which is not normally distributed, that we will converge to the best possible parameter choice as we obtain more data. This is what is understood by consistency . Finally, we would like the estimator to achieve low bias and near-optimal estimates as quickly as possible. The latter is measured by the eciency of an estimator. In this context we will encounter the Cram er-Rao bound which controls the best possible rate at which an estimator can achieve this goal. Figure 2.11 gives a pictorial description. Fig. 2.11. Left: unbiased estimator; the estimates, denoted by circles have as mean the true parameter, as denoted by a star. Middle: consistent estimator. While the true model is not within the class we consider (as denoted by the ellipsoid), the estimates converge to the white star which is the best model within the class that approximates the true model, denoted by the solid star. Right: dierent estimators have dierent regions of uncertainty, as made explicit by the ellipses around the true parameter (solid star). Denition 2.17 (Unbiased Estimator) An estimator ^[X]is unbiased if for allwhere Xp(X;)we have EX[^[X]] =. In other words, in expectation the parameter estimate matches the true pa- rameter. Note that this only makes sense if a true parameter actually exists . For instance, if the data is Poisson distributed and we attempt modeling it by a Gaussian we will obviously not obtain unbiased estimates. For nite sample sizes MLE is often biased . For instance, for the normal distribution the variance estimates carry bias O(m 1). See problem 2.19 for details. In general, under fairly mild conditions, MLE is asymptotically unbiased [DGL96]. We prove this for exponential families. For more general settings the proof depends on the dimensionality and smoothness of the family of densities that we have at our disposition.

70 2 Density Estimation Theorem 2.18 (MLE for Exponential Families) Assume that Xis an m-sample drawn iid from p(x;). The estimate ^[X] =g 1([X])is asymp- totically normal with m 1 2[^[X] ]!N(0; r2 g() 1): (2.59) In other words, the estimate ^[X] is asymptotically normal, it converges to the true parameter , and moreover, the variance at the correct parameter is given by the inverse of the covariance matrix of the data, as given by the second derivative of the log-partition function r2 g(). Proof Denote by=rg() the true mean. Moreover, note that r2 g() is the covariance of the data drawn from p(x;). By the central limit theorem (Theorem 2.3) we have that n 1 2[[X] ]!N(0;r2 g()). Now note that ^[X] = [rg] 1([X]). Therefore, by the delta method (Theorem 2.5) we know that ^[X] is also asymptotically normal. Moreover, by the inverse function theorem the Jacobian of g 1satisesr[rg] 1() = r2 g() 1. Applying Slutsky's theorem (Theorem 2.4) proves the claim. Now that we established the asymptotic properties of the MLE for exponen- tial families it is only natural to ask how much variation one may expect in ^[X] when performing estimation. The Cramer-Rao bound governs this. Theorem 2.19 (Cram er and Rao [Rao73]) Assume that Xis drawn from p(X;)and let ^[X]be an asymptotically unbiased estimator. Denote by I the Fisher information matrix and by Bthe variance of ^[X]where I:= Cov [rlogp(X;)]andB:= Varh ^[X]i : (2.60) In this case detIB1for all estimators ^[X]. Proof We prove the claim for the scalar case. The extension to matrices is straightforward. Using the Cauchy-Schwarz inequality we have Cov2h rlogp(X;);^[X]i Var [rlogp(X;)] Varh ^[X]i =IB: (2.61) Note that at the true parameter the expected log-likelihood score vanishes EX[rlogp(X;)] =rZ p(X;)dX=r1 = 0: (2.62)

2.4 Estimation 71 Hence we may simplify the covariance formula by dropping the means via Covh rlogp(X;);^[X]i =EXh rlogp(X;)^[X]i =Z p(X;)^(X)rlogp(X;)d =rZ p(X;)^(X)dX=r= 1: Here the last equality follows since we may interchange integration by X and the derivative with respect to . The Cram er-Rao theorem implies that there is a limit to how well we may estimate a parameter given nite amounts of data. It is also a yardstick by which we may measure how eciently an estimator uses data. Formally, we dene the eciency as the quotient between actual performance and the Cram er-Rao bound via e:= 1=detIB: (2.63) The closer eis to 1, the lower the variance of the corresponding estimator ^(X). Theorem 2.18 implies that for exponential families MLE is asymptot- ically ecient. It turns out to be generally true. Theorem 2.20 (Eciency of MLE [Cra46, GW92, Ber85]) The max- imum likelihood estimator is asymptotically ecient ( e= 1). So far we only discussed the behavior of ^[X] whenever there exists a true generatingp(;X). If this is not true, we need to settle for less: how well ^[X] approaches the best possible choice of within the given model class. Such behavior is referred to as consistency. Note that it is not possible to dene consistency per se . For instance, we may ask whether ^converges to the optimal parameter , or whether p(x;^) converges to the optimal density p(x;), and with respect to which norm. Under fairly general conditions this turns out to be true for nite-dimensional parameters and smoothly parametrized densities. See [DGL96, vdG00] for proofs and further details. 2.4.3 A Bayesian Approach The analysis of the Maximum Likelihood method might suggest that in- ference is a solved problem. After all, in the limit, MLE is unbiased and it exhibits as small variance as possible. Empirical results using a nite amount of data, as present in Figure 2.1 indicate otherwise. While not making any assumptions can lead to interesting and general

72 2 Density Estimation theorems, it ignores the fact that in practice we almost always have some idea about what to expect of our solution. It would be foolish to ignore such additional information. For instance, when trying to determine the voltage of a battery, it is reasonable to expect a measurement in the order of 1 :5V or less. Consequently such prior knowledge should be incorporated into the estimation process. In fact, the use of side information to guide estimation turns out to be thetool to building estimators which work well in high dimensions. Recall Bayes' rule (1.15) which states that p(jx) =p(xj)p() p(x). In our con- text this means that if we are interested in the posterior probability of  assuming a particular value, we may obtain this using the likelihood (often referred to as evidence) of xhaving been generated by viap(xj) and our prior belief p() thatmight be chosen in the distribution generating x. Observe the subtle but important dierence to MLE: instead of treating  as a parameter of a density model, we treat as an unobserved random variable which we may attempt to infer given the observations X. This can be done for a number of dierent purposes: we might want to infer the most likely value of the parameter given the posterior distribution p(jX). This is achieved by ^MAP(X) := argmax p(jX) = argmin  logp(Xj) logp(): (2.64) The second equality follows since p(X) does not depend on . This estimator is also referred to as the Maximum a Posteriori , or MAP estimator. It diers from the maximum likelihood estimator by adding the negative log-prior to the optimization problem. For this reason it is sometimes also referred to as Penalized MLE. Eectively we are penalizing unlikely choices via  logp(). Note that using ^MAP(X) as the parameter of choice is not quite accurate. After all, we can only infer a distribution over and in general there is no guarantee that the posterior is indeed concentrated around its mode. A more accurate treatment is to use the distribution p(jX) directly via p(xjX) =Z p(xj)p(jX)d: (2.65) In other words, we integrate out the unknown parameter and obtain the density estimate directly. As we will see, it is generally impossible to solve (2.65) exactly, an important exception being conjugate priors. In the other cases one may resort to sampling from the posterior distribution to approx- imate the integral. While it is possible to design a wide variety of prior distributions, this book

2.4 Estimation 73 focuses on two important families: norm-constrained prior and conjugate priors. We will encounter them throughout, the former sometimes in the guise of regularization and Gaussian Processes, the latter in the context of exchangeable models such as the Dirichlet Process. Norm-constrained priors take on the form p()/exp( k 0kd p) forp;d1 and>0: (2.66) That is, they restrict the deviation of the parameter value from some guess 0. The intuition is that extreme values of are much less likely than more moderate choices of which will lead to more smooth and even distributions p(xj). A popular choice is the Gaussian prior which we obtain for p=d= 1 and= 1=22. Typically one sets 0= 0 in this case. Note that in (2.66) we did not spell out the normalization of p() | in the context of MAP estimation this is not needed since it simply becomes a constant oset in the optimization problem (2.64). We have ^MAP[X] = argmin m[g() h;[X]i] +k 0kd p (2.67) Ford;p1 and0 the resulting optimization problem is convex and it has a unique solution. Moreover, very ecient algorithms exist to solve this problem. We will discuss this in detail in Chapter 3. Figure 2.12 shows the regions of equal prior probability for a range of dierent norm-constrained priors. As can be seen from the diagram, the choice of the norm can have profound consequences on the solution. That said, as we will show in Chapter ??, the estimate ^MAP is well concentrated and converges to the optimal solution under fairly general conditions. An alternative to norm-constrained priors are conjugate priors. They are designed such that the posterior p(jX) has the same functional form as the priorp(). In exponential families such priors are dened via p(jn;) = exp (hn;i ng() h(;n)) where (2.68) h(;n) = logZ exp (hn;i ng())d: (2.69) Note thatp(jn;) itself is a member of the exponential family with the feature map () = (; g()). Henceh(;n) isconvex in (n;n ). Moreover, the posterior distribution has the form p(jX)/p(Xj)p(jn;)/exp (hm[X] +n;i (m+n)g()):(2.70)

74 2 Density Estimation Fig. 2.12. From left to right: regions of equal prior probability in R2for priors using the`1,`2and`1norm. Note that only the `2norm is invariant with regard to the coordinate system. As we shall see later, the `1norm prior leads to solutions where only a small number of coordinates is nonzero. That is, the posterior distribution has the same form as a conjugate prior with parametersm[X]+n m+nandm+n. In other words, nacts like a phantom sample size and is the corresponding mean parameter. Such an interpreta- tion is reasonable given our desire to design a prior which, when combined with the likelihood remains in the same model class: we treat prior knowl- edge as having observed virtual data beforehand which is then added to the actual set of observations. In this sense data and prior become completely equivalent | we obtain our knowledge either from actual observations or from virtual observations which describe our belief into how the data gen- eration process is supposed to behave. Eq. (2.70) has the added benet of allowing us to provide an exact nor- malized version of the posterior. Using (2.68) we obtain that p(jX) = exp hm[X] +n;i (m+n)g() h m[X]+n m+n;m+n : The main remaining challenge is to compute the normalization hfor a range of important conjugate distributions. The table on the following page pro- vides details. Besides attractive algebraic properties, conjugate priors also have a second advantage | the integral (2.65) can be solved exactly: p(xjX) =Z exp (h(x);i g()) exp hm[X] +n;i (m+n)g() h m[X]+n m+n;m+n d Combining terms one may check that the integrand amounts to the normal-

2.4 Estimation 75 ization in the conjugate distribution, albeit (x) added. This yields p(xjX) = exp h m[X]+n+(x) m+n+1;m+n+ 1  h m[X]+n m+n;m+n Such an expansion is very useful whenever we would like to draw xfrom p(xjX) without the need to obtain an instantiation of the latent variable . We provide explicit expansions in appendix 2. [GS04] use the fact that  can be integrated out to obtain what is called a collapsed Gibbs sampler for topic models [BNJ03]. 2.4.4 An Example Assume we would like to build a language model based on available doc- uments. For instance, a linguist might be interested in estimating the fre- quency of words in Shakespeare's collected works, or one might want to compare the change with respect to a collection of webpages. While mod- els describing documents by treating them as bags of words which all have been obtained independently of each other are exceedingly simple, they are valuable for quick-and-dirty content ltering and categorization, e.g. a spam lter on a mail server or a content lter for webpages. Hence we model a document das a multinomial distribution: denote by wifori2 f1;:::;mdgthe words in d. Moreover, denote by p(wj) the probability of occurrence of word w, then under the assumption that the words are independently drawn, we have p(dj) =mdY i=1p(wij): (2.71) It is our goal to nd parameters such thatp(dj) is accurate. For a given collectionDof documents denote by mwthe number of counts for word w in the entire collection. Moreover, denote by mthe total number of words in the entire collection. In this case we have p(Dj) =Y ip(dij) =Y wp(wj)mw: (2.72) Finding suitable parameters givenDproceeds as follows: In a maximum likelihood model we set p(wj) =mw m: (2.73) In other words, we use the empirical frequency of occurrence as our best guess and the sucient statistic of Dis(w) =ew, whereewdenotes the unit vector which is nonzero only for the \coordinate" w. Hence[D]w=mw m.

76 2 Density Estimation We know that the conjugate prior of the multinomial model is a Dirichlet model. It follows from (2.70) that the posterior mode is obtained by replacing [D] bym[D]+n m+n. Denote by nw:=wnthe pseudo-counts arising from the conjugate prior with parameters ( ;n). In this case we will estimate the probability of the word was p(wj) =mw+nw m+n=mw+nw m+n: (2.74) In other words, we add the pseudo counts nwto the actual word counts mw. This is particularly useful when the document we are dealing with is brief, that is, whenever we have little data: it is quite unreasonable to infer from a webpage of approximately 1000 words that words not occurring in this page have zero probability. This is exactly what is mitigated by means of the conjugate prior ( ;n). Finally, let us consider norm-constrained priors of the form (2.66). In this case, the integral required for p(D) =Z p(Dj)p()d /Z exp  k 0kd p+mh[D];i mg() d isintractable and we need to resort to an approximation. A popular choice is to replace the integral by p(Dj) wheremaximizes the integrand. This is precisely the MAP approximation of (2.64). Hence, in order to perform estimation we need to solve minimize g() h[D];i+ mk 0kd p: (2.75) A very simple strategy for minimizing (2.75) is gradient descent. That is for a given value of we compute the gradient of the objective function and take a xed step towards its minimum. For simplicity assume that d=p= 2 and = 1=22, that is, we assume that is normally distributed with variance 2and mean0. The gradient is given by r[ logp(D;)] =Exp(xj)[(x)] [D] +1 m2[ 0] (2.76) In other words, it depends on the discrepancy between the mean of (x) with respect to our current model and the empirical average [X], and the dierence between and the prior mean 0. Unfortunately, convergence of the procedure   r[:::] is usually very slow, even if we adjust the steplength eciently. The reason is that the gradient need not point towards the minimum as the space is most likely

2.5 Sampling 77 distorted. A better strategy is to use Newton's method (see Chapter 3 for a detailed discussion and a convergence proof). It relies on a second order Taylor approximation  logp(D;+) logp(D;) +h;Gi+1 2>H (2.77) whereGandHare the rst and second derivatives of  logp(D;) with respect to. The quadratic expression can be minimized with respect to  by choosing = H 1Gand we can fashion an update algorithm from this by letting  H 1G. One may show (see Chapter 3) that Algorithm 2.1 is quadratically convergent. Note that the prior on ensures that His well conditioned even in the case where the variance of (x) is not. In practice this means that the prior ensures fast convergence of the optimization algorithm. Algorithm 2.1 Newton method for MAP estimation NewtonMAP( D) Initialize=0 while not converged do ComputeG=Exp(xj)[(x)] [D] +1 m2[ 0] ComputeH= Varxp(xj)[(x)] +1 m21 Update  H 1G end while return 2.5 Sampling So far we considered the problem of estimating the underlying probability density, given a set of samples drawn from that density. Now let us turn to the converse problem, that is, how to generate random variables given the underlying probability density. In other words, we want to design a random variable generator. This is useful for a number of reasons: We may encounter probability distributions where optimization over suit- able model parameters is essentially impossible and where it is equally im- possible to obtain a closed form expression of the distribution. In these cases it may still be possible to perform sampling to draw examples of the kind of data we expect to see from the model. Chapter ??discusses a number of graphical models where this problem arises. Secondly, assume that we are interested in testing the performance of a network router under dierent load conditions. Instead of introducing the under-development router in a live network and wreaking havoc, one could

78 2 Density Estimation estimate the probability density of the network trac under various load conditions and build a model. The behavior of the network can then be simulated by using a probabilistic model. This involves drawing random variables from an estimated probability distribution. Carrying on, suppose that we generate data packets by sampling and see an anomalous behavior in your router. In order to reproduce and debug this problem one needs access to the same set of random packets which caused the problem in the rst place. In other words, it is often convenient if our random variable generator is reproducible; At rst blush this seems like a contradiction. After all, our random number generator is supposed to generate random variables. This is less of a contradiction if we consider how random numbers are generated in a computer | given a particular initialization (which typically depends on the state of the system, e.g. time, disk size, bios checksum, etc.) the random number algorithm produces a sequence of numbers which, for all practical purposes, can be treated as iid. A simple method is the linear congruential generator [PTVF94] xi+1= (axi+b) modc: The performance of these iterations depends signicantly on the choice of the constantsa;b;c . For instance, the GNU C compiler uses a= 1103515245 ;b= 12345 andc= 232. In general bandcneed to be relatively prime and a 1 needs to be divisible by all prime factors of cand by 4. It is very much advisable notto attempt implementing such generators on one's own unless it is absolutely necessary. Useful desiderata for a pseudo random number generator (PRNG) are that for practical purposes it is statistically indistinguishable from a sequence of iid data. That is, when applying a number of statistical tests, we will accept the null-hypothesis that the random variables are iid. See Chapter ??for a detailed discussion of statistical testing procedures for random variables. In the following we assume that we have access to a uniform RNGU[0;1] which draws random numbers uniformly from the range [0 ;1]. 2.5.1 Inverse Transformation We now consider the scenario where we would like to draw from some dis- tinctively non-uniform distribution. Whenever the latter is relatively simple this can be achieved by applying an inverse transform: Theorem 2.21 Forzp(z)withz2Zand an injective transformation :Z!Xwith inverse transform  1on(Z)it follows that the random

2.5 Sampling 79 1 2 3 4 500:10:20:3Discrete Probability Distribution 1 2 3 4 5 600:20:40:60:81Cumulative Density Function Fig. 2.13. Left: discrete probability distribution over 5 possible outcomes. Right: associated cumulative distribution function. When sampling, we draw xuniformly at random from U[0;1] and compute the inverse of F. variablex:=(z)is drawn fromrx 1(x)p( 1(x)). Hererx 1(x) denotes the determinant of the Jacobian of  1. This follows immediately by applying a variable transformation for a mea- sure, i.e. we change dp(z) todp( 1(x))rx 1(x). Such a conversion strat- egy is particularly useful for univariate distributions. Corollary 2.22 Denote byp(x)a distribution on Rwith cumulative distri- bution function F(x0) =Rx0  1dp(x). Then the transformation x=(z) = F 1(z)converts samples zU[0;1]to samples drawn from p(x). We now apply this strategy to a number of univariate distributions. One of the most common cases is sampling from a discrete distribution. Example 2.8 (Discrete Distribution) In the case of a discrete distribu- tion overf1;:::;kgthe cumulative distribution function is a step-function with steps atf1;:::;kgwhere the height of each step is given by the corre- sponding probability of the event. The implementation works as follows: denote by p2[0;1]kthe vector of probabilities and denote by f2[0;1]kwithfi=fi 1+piandf1=p1the steps of the cumulative distribution function. Then for a random variable z drawn from U[0;1]we obtainx=(z) := argminiffizg. See Figure 2.13 for an example of a distribution over 5events.

80 2 Density Estimation 0 2 4 6 8 1000:20:40:60:81Exponential Distribution 0 2 4 6 8 1000:20:40:60:81Cumulative Distribution Function Fig. 2.14. Left: Exponential distribution with = 1. Right: associated cumulative distribution function. When sampling, we draw xuniformly at random from U[0;1] and compute the inverse. Example 2.9 (Exponential Distribution) The density of a Exponential- distributed random variable is given by p(xj) =exp( x)if>0andx0: (2.78) This allows us to compute its cdf as F(xj) = 1 exp( x)if>0forx0: (2.79) Therefore to generate a Exponential random variable we draw zU[0;1] and solvex=(z) =F 1(zj) =  1log(1 z). Sincezand1 zare drawn from U[0;1]we can simplify this to x=  1logz. We could apply the same reasoning to the normal distribution in order to draw Gaussian random variables. Unfortunately, the cumulative distribution function of the Gaussian is not available in closed form and we would need resort to rather nontrivial numerical techniques. It turns out that there exists a much more elegant algorithm which has its roots in Gauss' proof of the normalization constant of the Normal distribution. This technique is known as the Box-M uller transform. Example 2.10 (Box-M uller Transform) Denote byX;Y independent Gaus- sian random variables with zero mean and unit variance. We have p(x;y) =1p 2e 1 2x21p 2e 1 2y2=1 2e 1 2(x2+y2)(2.80)

2.5 Sampling 81 4  3  2  1  0  1  2  3  4  5 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 0.45 Fig. 2.15. Red: true density of the standard normal distribution (red line) is con- trasted with the histogram of 20,000 random variables generated by the Box-M uller transform. The key observation is that the joint distribution p(x;y)is radially symmet- ric, i.e. it only depends on the radius r2=x2+y2. Hence we may perform a variable substitution in polar coordinates via the map where x=rcosandy=rsinhence (x;y) = 1(r;): (2.81) This allows us to express the density in terms of (r;)via p(r;) =p( 1(r;))rr; 1(r;)=1 2e 1 2r2cos sin  rsin rcos=r 2e 1 2r2: The fact that p(r;)isconstant inmeans that we can easily sample 2 [0;2]by drawing a random variable, say zfromU[0;1]and rescaling it with 2. To obtain a sampler for rwe need to compute the cumulative distribution function for p(r) =re 1 2r2: F(r0) =Zr0 0re 1 2r2dr= 1 e 1 2r02and hencer=F 1(z) =p  2 log(1 z): (2.82) Observing that zU[0;1]implies that 1 zU[0;1]yields the following sampler: draw z;zrU[0;1]and compute xandyby x=p  2 logzrcos 2zandy=p  2 logzrsin 2z: Note that the Box-M uller transform yields two independent Gaussian ran- dom variables. See Figure 2.15 for an example of the sampler.

82 2 Density Estimation Example 2.11 (Uniform distribution on the disc) A similar strategy can be employed when sampling from the unit disc. In this case the closed- form expression of the distribution is simply given by p(x;y) =( 1 ifx2+y21 0 otherwise(2.83) Using the variable transform (2.81) yields p(r;) =p( 1(r;))rr; 1(r;)=( r ifr1 0 otherwise(2.84) Integrating out yieldsp(r) = 2rforr2[0;1]with corresponding CDF F(r) =r2forr2[0;1]. Hence our sampler draws zr;zU[0;1]and then computesx=pzrcos 2zandy=pzrsin 2z. 2.5.2 Rejection Sampler All the methods for random variable generation that we looked at so far re- quire intimate knowledge about the pdf of the distribution. We now describe a general purpose method, which can be used to generate samples from an arbitrary distribution. Let us begin with sampling from a set: Example 2.12 (Rejection Sampler) Denote byXXa set and let pbe a density on X. Then a sampler for drawing from pX(x)/p(x)forx2X andpX(x) = 0 forx62X, that is,pX(x) =p(xjx2X)is obtained by the procedure: repeat drawxp(x) untilx2X returnx That is, the algorithm keeps on drawing from puntil the random variable is contained in X. The probability that this occurs is clearly p(X). Hence the largerp(X)the higher the eciency of the sampler. See Figure 2.16. Example 2.13 (Uniform distribution on a disc) The procedure works trivially as follows: draw x;yU[0;1]. Accept if (2x 1)2+ (2y 1)21 and return sample (2x 1;2y 1). This sampler has eciency4 since this is the surface ratio between the unit square and the unit ball. Note that this time we did not need to carry out any sophisticated measure

2.5 Sampling 83 Fig. 2.16. Rejection sampler. Left: samples drawn from the uniform distribution on [0;1]2. Middle: the samples drawn from the uniform distribution on the unit disc are all the points in the grey shaded area. Right: the same procedure allows us to sample uniformly from arbitrary sets. 0.0  0.2  0.4  0.6  0.8  1.0 0.0 0.5 1.0 1.5 2.0 2.5 0.0  0.2  0.4  0.6  0.8  1.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 Fig. 2.17. Accept reject sampling for the Beta(2 ;5) distribution. Left: Samples are generated uniformly from the blue rectangle (shaded area). Only those samples which fall under the red curve of the Beta(2 ;5) distribution (darkly shaded area) are accepted. Right: The true density of the Beta(2 ;5) distribution (red line) is contrasted with the histogram of 10,000 samples drawn by the rejection sampler. transform. This mathematical convenience came at the expense of a slightly less ecient sampler | about 21% of all samples are rejected. The same reasoning that we used to obtain a hard accept/reject procedure can be used for a considerably more sophisticated rejection sampler. The basic idea is that if, for a given distribution pwe can nd another distribution qwhich, after rescaling, becomes an upper envelope on p, we can use qto sample from and reject depending on the ratio between qandp. Theorem 2.23 (Rejection Sampler) Denote bypandqdistributions on Xand letcbe a constant such that such that cq(x)p(x)for allx2X.

84 2 Density Estimation Then the algorithm below draws from pwith acceptance probability c 1. repeat drawxq(x)andtU[0;1] untilctp(x) q(x) returnx Proof Denote by Zthe event that the sample drawn from qis accepted. Then by Bayes rule the probability Pr( xjZ) can be written as follows Pr(xjZ) =Pr(Zjx) Pr(x) Pr(Z)=p(x) cq(x)q(x) c 1=p(x) (2.85) Here we used that Pr( Z) =R Pr(Zjx)q(x)dx=R c 1p(x)dx=c 1. Note that the algorithm of Example 2.12 is a special case of such a rejection sampler | we majorize pXby the uniform distribution rescaled by1 p(X). Example 2.14 (Beta distribution) Recall that the Beta(a;b)distribution, as a member of the Exponential Family with sucient statistics (logx;log(1  x)), is given by p(xja;b) = (a+b)  (a) (b)xa 1(1 x)b 1; (2.86) For given (a;b)one can verify (problem 2.25) that M:= argmax xp(xja;b) =a 1 a+b 2: (2.87) provideda>1. Hence, if we use as proposal distribution the uniform distri- butionU[0;1]with scaling factor c=p(Mja;b)we may apply Theorem 2.23. As illustrated in Figure 2.17, to generate a sample from Beta(a;b)we rst generate a pair (x;t), uniformly at random from the shaded rectangle. A sample is retained if ctp(xja;b), and rejected otherwise. The acceptance rate of this sampler is1 c. Example 2.15 (Normal distribution) We may use the Laplace distri- bution to generate samples from the Normal distribution. That is, we use q(xj) = 2e jxj(2.88) as the proposal distribution. For a normal distribution p=N(0;1)with zero

2.5 Sampling 85 mean and unit variance it turns out that choosing = 1 yields the most ecient sampling scheme (see Problem 2.27) with p(x)r 2e q(xj= 1) As illustrated in Figure 2.18, we rst generate xq(xj= 1) using the inverse transform method (see Example 2.9 and Problem 2.21) and t U[0;1]. Iftp 2e=p(x)we acceptx, otherwise we reject it. The eciency of this scheme isp 2e.  4 2 0 2 400:20:40:6q 2e g(xj0;1) p(x) Fig. 2.18. Rejection sampling for the Normal distribution (red curve). Samples are generated uniformly from the Laplace distribution rescaled byp 2e=. Only those samples which fall under the red curve of the standard normal distribution (darkly shaded area) are accepted. While rejection sampling is fairly ecient in low dimensions its eciency is unsatisfactory in high dimensions. This leads us to an instance of the curse of dimensionality [Bel61]: the pdf of a d-dimensional Gaussian random variable centered at 0 with variance 21is given by p(xj2) = (2) d 2 de 1 22kxk2 Now suppose that we want to draw from p(xj2) by sampling from another Gaussianqwith slightly larger variance 2> 2. In this case the ratio between both distributions is maximized at 0 and it yields c=q(0j2) p(0j2)=h id

86 2 Density Estimation If suppose = 1:01, andd= 1000, we nd that c20960. In other words, we need to generate approximately 21,000 samples on the average from qto draw a single sample from p. We will discuss a more sophisticated sampling algorithms, namely Gibbs Sampling, in Section ??. It allows us to draw from rather nontrivial distributions as long as the distributions in small subsets of random variables are simple enough to be tackled directly. Problems Problem 2.1 (Bias Variance Decomposition f1g)Prove that the vari- ance VarX[x]of a random variable can be written as EX[x2] EX[x]2. Problem 2.2 (Moment Generating Function f2g)Prove that the char- acteristic function can be used to generate moments as given in (2.12). Hint: use the Taylor expansion of the exponential and apply the dierential oper- ator before the expectation. Problem 2.3 (Cumulative Error Function f2g) erf(x) =p 2=Zx 0e x2dx: (2.89) Problem 2.4 (Weak Law of Large Numbers f2g)In analogy to the proof of the central limit theorem prove the weak law of large numbers. Hint: use a rst order Taylor expansion of ei!t= 1 +i!t+o(t)to compute an approx- imation of the characteristic function. Next compute the limit m!1 for Xm. Finally, apply the inverse Fourier transform to associate the constant distribution at the mean with it. Problem 2.5 (Rates and condence bounds f3g)Show that the rate of hoeding is tight | get bound from central limit theorem and compare to the hoeding rate. Problem 2.6 Why can't we just use each chip on the wafer as a random variable? Give a counterexample. Give bounds if we actually were allowed to do this. Problem 2.7 (Union Bound) Work on many bounds at the same time. We only have logarithmic penalty. Problem 2.8 (Randomized Rounding f4g)Solve the linear system of equationsAx=bfor integral x.

2.5 Sampling 87 Problem 2.9 (Randomized Projections f3g)Prove that the random- ized projections converge. Problem 2.10 (The Count-Min Sketch f5g)Prove the projection trick Problem 2.11 (Parzen windows with triangle kernels f1g)Suppose you are given the following data: X=f2;3;3;5;5g. Plot the estimated den- sity using a kernel density estimator with the following kernel: k(u) =( 0:5 0:25jujifjuj2 0otherwise: Problem 2.12 Gaussian process link with Gaussian prior on natural pa- rameters Problem 2.13 Optimization for Gaussian regularization Problem 2.14 Conjugate prior (student-t and wishart). Problem 2.15 (Multivariate Gaussian f1g)Prove that 0is a nec- essary and sucient condition for the normal distribution to be well dened. Problem 2.16 (Discrete Exponential Distribution f2g)(x) =xand uniform measure. Problem 2.17 Exponential random graphs. Problem 2.18 (Maximum Entropy Distribution) Show that exponen- tial families arise as the solution of the maximum entropy estimation prob- lem. Problem 2.19 (Maximum Likelihood Estimates for Normal Distributions) Derive the maximum likelihood estimates for a normal distribution, that is, show that they result in ^=1 mmX i=1xiand^2=1 mmX i=1(xi ^)2(2.90) using the exponential families parametrization. Next show that while the mean estimate ^is unbiased, the variance estimate has a slight bias of O(1 m). To see this, take the expectation with respect to ^2.

88 2 Density Estimation Problem 2.20 (cdf of Logistic random variable f1g)Show that the cdf of the Logistic random variable (??)is given by (??). Problem 2.21 (Double-exponential (Laplace) distribution f1g)Use the inverse-transform method to generate a sample from the double-exponential (Laplace) distribution (2.88) . Problem 2.22 (Normal random variables in polar coordinates f1g) IfX1andX2are standard normal random variables and let (R;)de- note the polar coordinates of the pair (X1;X2). Show that R22 2and Unif[0;2]. Problem 2.23 (Monotonically increasing mappings f1g)A mapping T:R!Ris one-to-one if, and only if, Tis monotonically increasing, that is,x>y implies that T(x)>T(y). Problem 2.24 (Monotonically increasing multi-maps f2g)LetT:Rn! Rnbe one-to-one. If XpX(x), then show that the distribution pY(y)of Y=T(X)can be obtained via (??). Problem 2.25 (Argmax of the Beta(a;b)distribution f1g)Show that the mode of the Beta(a;b)distribution is given by (2.87) . Problem 2.26 (Accept reject sampling for the unit disk f2g)Give at least TWO dierent accept-reject based sampling schemes to generate sam- ples uniformly at random from the unit disk. Compute their eciency. Problem 2.27 (Optimizing Laplace for Standard Normal f1g)Optimize the ratiop(x)=g(xj;), with respect to and, wherep(x)is the standard normal distribution (??), andg(xj;)is the Laplace distribution (2.88) . Problem 2.28 (Normal Random Variable Generation f2g)The aim of this problem is to write code to generate standard normal random vari- ables (??)by using dierent methods. To do this generate UUnif[0;1] and apply (i)the Box-Muller transformation outlined in Section ??. (ii)use the following approximation to the inverse CDF  1()t a0+a1t 1 +b1t+b2t2; (2.91)

2.5 Sampling 89 wheret2= log( 2)and a0= 2:30753;a1= 0:27061;b1= 0:99229;b2= 0:04481 (iii) use the method outlined in example 2.15. Plot a histogram of the samples you generated to conrm that they are nor- mally distributed. Compare these dierent methods in terms of the time needed to generate 1000 random variables. Problem 2.29 (Non-standard Normal random variables f2g)Describe a scheme based on the Box-Muller transform to generate ddimensional nor- mal random variables p(xj0;I). How can this be used to generate arbitrary normal random variables p(xj;). Problem 2.30 (Uniform samples from a disk f2g)Show how the ideas described in Section ??can be generalized to draw samples uniformly at ran- dom from an axis parallel ellipse: f(x;y) :x2 1 a2+x2 2 b21g.

3 Optimization Optimization plays an increasingly important role in machine learning. For instance, many machine learning algorithms minimize a regularized risk functional: min fJ(f) := (f) +Remp(f); (3.1) with the empirical risk Remp(f) :=1 mmX i=1l(f(xi);yi): (3.2) Herexiare the training instances and yiare the corresponding labels. lthe loss function measures the discrepancy between yand the predictions f(xi). Finding the optimal finvolves solving an optimization problem. This chapter provides a self-contained overview of some basic concepts and tools from optimization, especially geared towards solving machine learning problems. In terms of concepts, we will cover topics related to convexity, duality, and Lagrange multipliers. In terms of tools, we will cover a variety of optimization algorithms including gradient descent, stochastic gradient descent, Newton's method, and Quasi-Newton methods. We will also look at some specialized algorithms tailored towards solving Linear Programming and Quadratic Programming problems which often arise in machine learning problems. 3.1 Preliminaries Minimizing an arbitrary function is, in general, very dicult, but if the ob- jective function to be minimized is convex then things become considerably simpler. As we will see shortly, the key advantage of dealing with convex functions is that a local optima is also the global optima. Therefore, well developed tools exist to nd the global minima of a convex function. Conse- quently, many machine learning algorithms are now formulated in terms of convex optimization problems. We briey review the concept of convex sets and functions in this section. 91

92 3 Optimization 3.1.1 Convex Sets Denition 3.1 (Convex Set) A subsetCofRnis said to be convex if (1 )x+y2Cwheneverx2C;y2Cand0<< 1. Intuitively, what this means is that the line joining any two points xandy from the set Clies insideC(see Figure 3.1). It is easy to see (Exercise 3.1) that intersections of convex sets are also convex. Fig. 3.1. The convex set (left) contains the line joining any two points that belong to the set. A non-convex set (right) does not satisfy this property. A vector sumP iixiis called a convex combination if i0 andP ii= 1. Convex combinations are helpful in dening a convex hull: Denition 3.2 (Convex Hull) The convex hull, conv(X), of a nite sub- setX=fx1;:::;xngofRnconsists of all convex combinations of x1;:::;xn. 3.1.2 Convex Functions Letfbe a real valued function dened on a set XRn. The set f(x;) :x2X;2R;f(x)g (3.3) is called the epigraph off. The function fis dened to be a convex function if its epigraph is a convex set in Rn+1. An equivalent, and more commonly used, denition (Exercise 3.5) is as follows (see Figure 3.2 for geometric intuition): Denition 3.3 (Convex Function) A function fdened on a set Xis called convex if, for any x;x02Xand any 0<  < 1such thatx+ (1  )x02X, we have f(x+ (1 )x0)f(x) + (1 )f(x0): (3.4)

3.1 Preliminaries 93 A function fis called strictly convex if f(x+ (1 )x0)<f (x) + (1 )f(x0) (3.5) wheneverx6=x0. In fact, the above denition can be extended to show that if fis a convex function and i0 withP ii= 1 then f X iixi! X iif(xi): (3.6) The above inequality is called the Jensen's inequality (problem ). 6  4  2  0 2 4 6x02004006008001000 f(x) 3  2  1  0 1 2 3x1.5 1.0 0.5 0.00.51.01.5f(x) Fig. 3.2. A convex function (left) satises (3.4); the shaded region denotes its epi- graph. A nonconvex function (right) does not satisfy (3.4). Iff:X!Ris dierentiable, then fis convex if, and only if, f(x0)f(x) + x0 x;rf(x) for allx;x02X: (3.7) In other words, the rst order Taylor approximation lower bounds the convex function universally (see Figure 3.4). Here and in the rest of the chapter hx;yidenotes the Euclidean dot product between vectors xandy, that is, hx;yi:=X ixiyi: (3.8) Iffis twice dierentiable, then fis convex if, and only if, its Hessian is positive semi-denite, that is, r2f(x)0: (3.9) For twice dierentiable strictly convex functions, the Hessian matrix is pos- itive denite, that is, r2f(x)0. We briey summarize some operations which preserve convexity:

94 3 Optimization Addition Iff1andf2are convex, then f1+f2is also convex. Scaling Iffis convex, then fis convex for >0. Ane Transform Iffis convex, then g(x) =f(Ax+b) for some matrix Aand vector bis also convex. Adding a Linear Function Iffis convex, then g(x) =f(x)+ha;xifor some vector ais also convex. Subtracting a Linear Function Iffis convex, then g(x) =f(x) ha;xifor some vector ais also convex. Pointwise Maximum Iffiare convex, then g(x) = maxifi(x) is also convex. Scalar Composition Iff(x) =h(g(x)), thenfis convex if a) gis convex, andhis convex, non-decreasing or b) gis concave, and his convex, non-increasing. -3-2-1 0 1 2 3-3-2-1 0 1 2 3 0 2 4 6 8 10 12 14 16 18 -3 -2 -1  0  1  2  3-3-2-1 0 1 2 3 Fig. 3.3. Left: Convex Function in two variables. Right: the corresponding convex below-setsfxjf(x)cg, for dierent values of c. This is also called a contour plot. There is an intimate relation between convex functions and convex sets. For instance, the following lemma show that the below sets (level sets) of convex functions, sets for which f(x)c, are convex. Lemma 3.4 (Below-Sets of Convex Functions) Denote byf:X!R a convex function. Then the set Xc:=fxjx2Xandf(x)cg;for allc2R; (3.10) is convex. Proof For anyx;x02Xc, we havef(x);f(x0)c. Moreover, since fis convex, we also have f(x+ (1 )x0)f(x) + (1 )f(x0)cfor all 0<< 1:(3.11) Hence, for all 0 << 1, we have ( x+ (1 )x0)2Xc, which proves the claim. Figure 3.3 depicts this situation graphically.

3.1 Preliminaries 95 As we hinted in the introduction of this chapter, minimizing an arbitrary function on a (possibly not even compact) set of arguments can be a dicult task, and will most likely exhibit many local minima. In contrast, minimiza- tion of a convex objective function on a convex set exhibits exactly one global minimum. We now prove this property. Theorem 3.5 (Minima on Convex Sets) If the convex function f:X! Rattains its minimum, then the set of x2X, for which the minimum value is attained, is a convex set. Moreover, if fis strictly convex, then this set contains a single element. Proof Denote by cthe minimum of fonX. Then the set Xc:=fxjx2 Xandf(x)cgis clearly convex. Iffis strictly convex, then for any two distinct x;x02Xcand any 0< <1 we have f(x+ (1 )x0)<f (x) + (1 )f(x0) =c+ (1 )c=c; which contradicts the assumption that fattains its minimum on Xc. There- foreXcmust contain only a single element. As the following lemma shows, the minimum point can be characterized precisely. Lemma 3.6 Letf:X!Rbe a dierentiable convex function. Then xis a minimizer of f, if, and only if,

x0 x;rf(x) 0for allx0: (3.12) Proof To show the forward implication, suppose that xis the optimum but (3.12) does not hold, that is, there exists an x0for which

x0 x;rf(x) <0: Consider the line segment z() = (1 )x+x0, with 0<< 1. SinceX is convex,z() lies inX. On the other hand, d df(z()) =0= x0 x;rf(x) <0; which shows that for small values of we havef(z())<f(x), thus showing thatxis not optimal. The reverse implication follows from (3.7) by noting that f(x0)f(x), whenever (3.12) holds.

96 3 Optimization One way to ensure that (3.12) holds is to set rf(x) = 0. In other words, minimizing a convex function is equivalent to nding a xsuch thatrf(x) = 0. Therefore, the rst order conditions are both necessary and sucient when minimizing a convex function. 3.1.3 Subgradients So far, we worked with dierentiable convex functions. The subgradient is a generalization of gradients appropriate for convex functions, including those which are not necessarily smooth. Denition 3.7 (Subgradient) Supposexis a point where a convex func- tionfis nite. Then a subgradient is the normal vector of any tangential supporting hyperplane of fatx. Formally is called a subgradient of fat xif, and only if, f(x0)f(x) + x0 x; for allx0: (3.13) The set of all subgradients at a point is called the subdierential, and is de- noted by@xf(x). If this set is not empty then fis said to be subdierentiable atx. On the other hand, if this set is a singleton then, the function is said to be dierentiable atx. In this case we use rf(x) to denote the gradient off. Convex functions are subdierentiable everywhere in their domain. We now state some simple rules of subgradient calculus: Addition @x(f1(x) +f2(x)) =@xf1(x) +@xf2(x) Scaling@xf(x) =@xf(x), for>0 Ane Transform Ifg(x) =f(Ax+b) for some matrix Aand vector b, then@xg(x) =A>@yf(y). Pointwise Maximum Ifg(x) = maxifi(x) then@g(x) = conv(@xfi0) where i02argmaxifi(x). The denition of a subgradient can also be understood geometrically. As illustrated by Figure 3.4, a dierentiable convex function is always lower bounded by its rst order Taylor approximation. This concept can be ex- tended to non-smooth functions via subgradients, as Figure 3.5 shows. By using more involved concepts, the proof of Lemma 3.6 can be extended to subgradients. In this case, minimizing a convex nonsmooth function en- tails nding a xsuch that 02@f(x).

3.1 Preliminaries 97 3.1.4 Strongly Convex Functions When analyzing optimization algorithms, it is sometimes easier to work with strongly convex functions, which generalize the denition of convexity. Denition 3.8 (Strongly Convex Function) A convex function fis- strongly convex if, and only if, there exists a constant  > 0such that the functionf(x)  2kxk2is convex. The constant is called the modulus of strong convexity of f. Iffis twice dierentiable, then there is an equivalent, and perhaps easier, denition of strong convexity: fis strongly convex if there exists a such that r2f(x)I: (3.14) In other words, the smallest eigenvalue of the Hessian of fisuniformly lower bounded byeverywhere. Some important examples of strongly con- vex functions include: Example 3.1 (Squared Euclidean Norm) The function f(x) = 2kxk2 is-strongly convex. Example 3.2 (Negative Entropy) Letn=fxs.t.P ixi= 1andxi0g be thendimensional simplex, and f: n!Rbe the negative entropy: f(x) =X ixilogxi: (3.15) Thenfis 1-strongly convex with respect to the kk1norm on the simplex (see Problem 3.7). Iffis a-strongly convex function then one can show the following prop- erties (Exercise 3.8). Here x;x0are arbitrary and 2@f(x) and02@f(x0). f(x0)f(x) + x0 x; + 2x0 x2(3.16) f(x0)f(x) + x0 x; +1 20 2(3.17)

x x0; 0 x x02(3.18)

x x0; 0 1  02: (3.19)

98 3 Optimization 3.1.5 Convex Functions with Lipschitz Continous Gradient A somewhat symmetric concept to strong convexity is the Lipschitz conti- nuity of the gradient. As we will see later they are connected by Fenchel duality. Denition 3.9 (Lipschitz Continuous Gradient) A dierentiable con- vex function fis said to have a Lipschitz continuous gradient, if there exists a constantL>0, such that rf(x) rf(x0)Lx x08x;x0: (3.20) As before, if fis twice dierentiable, then there is an equivalent, and perhaps easier, denition of Lipschitz continuity of the gradient: fhas a Lipschitz continuous gradient strongly convex if there exists a Lsuch that LIr2f(x): (3.21) In other words, the largest eigenvalue of the Hessian of fisuniformly upper bounded byLeverywhere. If fhas a Lipschitz continuous gradient with modulusL, then one can show the following properties (Exercise 3.9). f(x0)f(x) + x0 x;rf(x) +L 2x x02(3.22) f(x0)f(x) + x0 x;rf(x) +1 2Lrf(x) rf(x0)2(3.23)

x x0;rf(x) rf(x0) Lx x02(3.24)

x x0;rf(x) rf(x0) 1 Lrf(x) rf(x0)2: (3.25) 3.1.6 Fenchel Duality The Fenchel conjugate of a function fis given by f(x) = sup xfhx;xi f(x)g: (3.26) Even iffis not convex, the Fechel conjugate which is written as a supremum over linear functions is always convex. Some rules for computing Fenchel duals are summarized in Table 3.1.6. If fis convex and its epigraph (3.3) is a closed convex set, then f=f. Iffandfare convex, then they satisfy the so-called Fenchel-Young inequality f(x) +f(x)hx;xifor allx;x: (3.27)

3.1 Preliminaries 99 Fig. 3.4. A convex function is always lower bounded by its rst order Taylor ap- proximation. This is true even if the function is not dierentiable (see Figure 3.5) 4  3  2  1  0  1  2  3  4 1 0 1 2 3 4 5 Fig. 3.5. Geometric intuition of a subgradient. The nonsmooth convex function (solid blue) is only subdierentiable at the \kink" points. We illustrate two of its subgradients (dashed green and red lines) at a \kink" point which are tangential to the function. The normal vectors to these lines are subgradients. Observe that the rst order Taylor approximations obtained by using the subgradients lower bounds the convex function. This inequality becomes an equality whenever x2@f(x), that is, f(x) +f(x) =hx;xifor allxandx2@f(x): (3.28) Strong convexity (Section 3.1.4) and Lipschitz continuity of the gradient

100 3 Optimization Table 3.1. Rules for computing Fenchel Duals Scalar Addition Ifg(x) =f(x) +theng(x) =f(x) . Function Scaling If>0 andg(x) =f(x) theng(x) =f(x=). Parameter Scaling If6= 0 andg(x) =f(x) theng(x) =f(x=) Linear Transformation IfAis an invertible matrix then ( fA)=f(A 1). Shift Ifg(x) =f(x x0) theng(x) =f(x) +hx;x0i. Sum Ifg(x) =f1(x) +f2(x) theng(x) = infff 1(x 1) +f 2(x 2) s.t.x 1+x 2=xg. Pointwise Inmum Ifg(x) = inffi(x) theng(x) = supif i(x). (Section 3.1.5) are related by Fenchel duality according to the following lemma, which we state without proof. Lemma 3.10 (Theorem 4.2.1 and 4.2.2 [HUL93]) (i)Iffis-strongly convex, then fhas a Lipschitz continuous gradient with modulus1 . (ii)Iffis convex and has a Lipschitz continuous gradient with modulus L, thenfis1 L-strongly convex. Next we describe some convex functions and their Fenchel conjugates. Example 3.3 (Squared Euclidean Norm) Wheneverf(x) =1 2kxk2we havef(x) =1 2kxk2, that is, the squared Euclidean norm is its own con- jugate. Example 3.4 (Negative Entropy) The Fenchel conjugate of the negative entropy (3.15) is f(x) = logX iexp(x i): 3.1.7 Bregman Divergence Letfbe a dierentiable convex function. The Bregman divergence dened byfis given by f(x;x0) =f(x) f(x0)  x x0;rf(x0) : (3.29) Also see Figure 3.6. Here are some well known examples. Example 3.5 (Square Euclidean Norm) Setf(x) =1 2kxk2. Clearly, rf(x) =xand therefore f(x;x0) =1 2kxk2 1 2x02  x x0;x0 =1 2x x02:

3.1 Preliminaries 101 f(x/prime)f(x) f(x/prime)+/angbracketleftbig xâˆ’x/prime,âˆ‡f(x/prime)/angbracketrightbigâˆ†f(x,x/prime) Fig. 3.6.f(x) is the value of the function at x, whilef(x0)+hx x0;rf(x0)idenotes the rst order Taylor expansion of faroundx0, evaluated at x. The dierence between these two quantities is the Bregman divergence, as illustrated. Example 3.6 (Relative Entropy) Letfbe the un-normalized entropy f(x) =X i(xilogxi xi): (3.30) One can calculate rf(x) = logx, where logxis the component wise loga- rithm of the entries of x, and write the Bregman divergence f(x;x0) =X ixilogxi X ixi X ix0 ilogx0 i+X ix0 i  x x0;logx0 =X i xilogxi x0 i +x0 i xi : Example 3.7 ( p-norm) Letfbe the square p-norm f(x) =1 2kxk2 p=1 2 X ixp i!2=p : (3.31)

102 3 Optimization We say that the q-norm is dual to the p-norm whenever1 p+1 q= 1. One can verify (Problem 3.12) that the i-th component of the gradient rf(x)is rxif(x) =sign(xi)jxijp 1 kxkp 2 p: (3.32) The corresponding Bregman divergence is f(x;x0) =1 2kxk2 p 1 2x02 p X i(xi x0 i)sign(x0 i)jx0 ijp 1 kx0kp 2 p: The following properties of the Bregman divergence immediately follow: f(x;x0) is convex in x. f(x;x0)0. fmay not be symmetric, that is, in general  f(x;x0)6= f(x0;x).  rxf(x;x0) =rf(x) rf(x0). The next lemma establishes another important property. Lemma 3.11 The Bregman divergence (3.29) dened by a dierentiable convex function fsatises f(x;y) + f(y;z) f(x;z) =hrf(z) rf(y);x yi: (3.33) Proof f(x;y) + f(y;z) =f(x) f(y) hx y;rf(y)i+f(y) f(z) hy z;rf(z)i =f(x) f(z) hx y;rf(y)i hy z;rf(z)i = f(x;z) +hrf(z) rf(y);x yi: 3.2 Unconstrained Smooth Convex Minimization In this section we will describe various methods to minimize a smooth convex objective function. 3.2.1 Minimizing a One-Dimensional Convex Function As a warm up let us consider the problem of minimizing a smooth one di- mensional convex function J:R!Rin the interval [ L;U]. This seemingly

3.2 Unconstrained Smooth Convex Minimization 103 Algorithm 3.1 Interval Bisection 1:Input:L,U, precision 2:Sett= 0,a0=Landb0=U 3:while (bt at)J0(U)>do 4:ifJ0(at+bt 2)>0then 5:at+1=atandbt+1=at+bt 2 6:else 7:at+1=at+bt 2andbt+1=bt 8:end if 9:t=t+ 1 10:end while 11:Return:at+bt 2 simple problem has many applications. As we will see later, many optimiza- tion methods nd a direction of descent and minimize the objective function along this direction1; this subroutine is called a line search. Algorithm 3.1 depicts a simple line search routine based on interval bisection. Before we show that Algorithm 3.1 converges, let us rst derive an im- portant property of convex functions of one variable. For a dierentiable one-dimensional convex function J(3.7) reduces to J(w)J(w0) + (w w0)J0(w0); (3.34) whereJ0(w) denotes the gradient of J. Exchanging the role of wandw0in (3.34), we can write J(w0)J(w) + (w0 w)J0(w): (3.35) Adding the above two equations yields (w w0)(J0(w) J0(w0))0: (3.36) Ifww0, then this implies that J0(w)J0(w0). In other words, the gradient of a one dimensional convex function is monotonically non-decreasing. Recall that minimizing a convex function is equivalent to nding wsuch thatJ0(w) = 0. Furthermore, it is easy to see that the interval bisection maintains the invariant J0(at)<0 andJ0(bt)>0. This along with the monotonicity of the gradient suces to ensure that w2(at;bt). Setting w=win (3.34), and using the monotonicity of the gradient allows us to 1If the objective function is convex, then the one dimensional function obtained by restricting it along the search direction is also convex (Exercise 3.10).

104 3 Optimization write for any w02(at;bt) J(w0) J(w)(w0 w)J0(w0)(bt at)J0(U): (3.37) Since we halve the interval ( at;bt) at every iteration, it follows that ( bt at) = (U L)=2t. Therefore J(w0) J(w)(U L)J0(U) 2t; (3.38) for allw02(at;bt). In other words, to nd an -accurate solution, that is, J(w0) J(w)we only need log( U L) + logJ0(U) + log(1=)<titera- tions. An algorithm which converges to an accurate solution in O(log(1=)) iterations is said to be linearly convergent. For multi-dimensional objective functions, one cannot rely on the mono- tonicity property of the gradient. Therefore, one needs more sophisticated optimization algorithms, some of which we now describe. 3.2.2 Coordinate Descent Coordinate descent is conceptually the simplest algorithm for minimizing a multidimensional smooth convex function J:Rn!R. At every iteration select a coordinate, say i, and update wt+1=wt tei: (3.39) Hereeidenotes the i-th basis vector, that is, a vector with one at the i-th co- ordinate and zeros everywhere else, while t2Ris a non-negative scalar step size. One could, for instance, minimize the one dimensional convex function J(wt ei) to obtain the stepsize t. The coordinates can either be selected cyclically, that is, 1 ;2;:::;n; 1;2;:::or greedily, that is, the coordinate which yields the maximum reduction in function value. Even though coordinate descent can be shown to converge if Jhas a Lip- schitz continuous gradient [LT92], in practice it can be quite slow. However, if a high precision solution is not required, as is the case in some machine learning applications, coordinate descent is often used because a) the cost per iteration is very low and b) the speed of convergence may be acceptable especially if the variables are loosely coupled. 3.2.3 Gradient Descent Gradient descent (also widely known as steepest descent) is an optimization technique for minimizing multidimensional smooth convex objective func- tions of the form J:Rn!R. The basic idea is as follows: Given a location

3.2 Unconstrained Smooth Convex Minimization 105 wtat iteration t, compute the gradient rJ(wt), and update wt+1=wt trJ(wt); (3.40) wheretis a scalar stepsize. See Algorithm 3.2 for details. Dierent variants of gradient descent depend on how tis chosen: Exact Line Search: SinceJ(wt rJ(wt)) is a one dimensional convex function in , one can use the Algorithm 3.1 to compute: t= argmin J(wt rJ(wt)): (3.41) Instead of the simple bisecting line search more sophisticated line searches such as the More-Thuente line search or the golden bisection rule can also be used to speed up convergence (see [NW99] Chapter 3 for an extensive discussion). Inexact Line Search: Instead of minimizing J(wt rJ(wt)) we could simply look for a stepsize which results in sucient decrease in the objective function value. One popular set of sucient decrease conditions is the Wolfe conditions J(wt+1)J(wt) +c1thrJ(wt);wt+1 wti(sucient decrease) (3.42) hrJ(wt+1);wt+1 wti c2hrJ(wt);wt+1 wti(curvature) (3.43) with 0<c1<c2<1 (see Figure 3.7). The Wolfe conditions are also called the Armijio-Goldstein conditions. If only sucient decrease (3.42) alone is enforced, then it is called the Armijio rule. acceptable stepsize acceptable stepsize Fig. 3.7. The sucient decrease condition (left) places an upper bound on the acceptable stepsizes while the curvature condition (right) places a lower bound on the acceptable stepsizes.

106 3 Optimization Algorithm 3.2 Gradient Descent 1:Input: Initial point w0, gradient norm tolerance  2:Sett= 0 3:whilekrJ(wt)kdo 4:wt+1=wt trJ(wt) 5:t=t+ 1 6:end while 7:Return:wt Decaying Stepsize: Instead of performing a line search at every itera- tion, one can use a stepsize which decays according to a xed schedule, for example,t= 1=p t. In Section 3.2.4 we will discuss the decay schedule and convergence rates of a generalized version of gradient descent. Fixed Stepsize: SupposeJhas a Lipschitz continuous gradient with mod- ulusL. Using (3.22) and the gradient descent update wt+1=wt trJ(wt) one can write J(wt+1)J(wt) +hrJ(wt);wt+1 wti+L 2kwt+1 wtk (3.44) =J(wt) tkrJ(wt)k2+L2 t 2krJ(wt)k2: (3.45) Minimizing (3.45) as a function of tclearly shows that the upper bound on J(wt+1) is minimized when we set t=1 L, which is the xed stepsize rule. Theorem 3.12 SupposeJhas a Lipschitz continuous gradient with modu- lusL. Then Algorithm 3.2 with a xed stepsize t=1 Lwill return a solution wtwithkrJ(wt)kin at mostO(1=2)iterations. Proof Plugging in t=1 Land rearranging (3.45) obtains 1 2LkrJ(wt)k2J(wt) J(wt+1) (3.46) Summing this inequality 1 2LTX t=0krJ(wt)k2J(w0) J(wT)J(w0) J(w); which clearly shows that krJ(wt)k! 0 ast!1 . Furthermore, we can write the following simple inequality: krJ(wT)kr 2L(J(w0) J(w)) T+ 1:

3.2 Unconstrained Smooth Convex Minimization 107 Solving for r 2L(J(w0) J(w)) T+ 1= shows that TisO(1=2) as claimed. If in addition to having a Lipschitz continuous gradient, if Jis-strongly convex, then more can be said. First, one can translate convergence in krJ(wt)kto convergence in function values. Towards this end, use (3.17) to write J(wt)J(w) +1 2krJ(wt)k2: Therefore, it follows that whenever krJ(wt)k<we haveJ(wt) J(w)< 2=2. Furthermore, we can strengthen the rates of convergence. Theorem 3.13 Assume everything as in Theorem 3.12. Moreover assume thatJis-strongly convex, and let c:= 1  L. ThenJ(wt) J(w) after at most log((J(w0) J(w))=) log(1=c)(3.47) iterations. Proof Combining (3.46) with krJ(wt)k22(J(wt) J(w)), and using the denition of cone can write c(J(wt) J(w))J(wt+1) J(w): Applying the above equation recursively cT(J(w0) J(w))J(wT) J(w): Solving for =cT(J(w0) J(w)) and rearranging yields (3.47). When applied to practical problems which are not strongly convex gra- dient descent yields a low accuracy solution within a few iterations. How- ever, as the iterations progress the method \stalls" and no further increase in accuracy is obtained because of the O(1=2) rates of convergence. On the other hand, if the function is strongly convex, then gradient descent converges linearly, that is, in O(log(1=)) iterations. However, the number

108 3 Optimization of iterations depends inversely on log(1 =c). If we approximate log(1 =c) =  log(1 =L)=L, then it shows that convergence depends on the ratio L=. This ratio is called the condition number of a problem. If the problem is well conditioned, i.e.,Lthen gradient descent converges extremely fast. In contrast, if Lthen gradient descent requires many iterations. This is best illustrated with an example: Consider the quadratic objective function J(w) =1 2w>Aw bw; (3.48) whereA2Rnnis a symmetric positive denite matrix, and b2Rnis any arbitrary vector. Recall that a twice dierentiable function is -strongly convex and has a Lipschitz continuous gradient with modulus Lif and only if its Hessian sat- isesLIr2J(w)I(see (3.14) and (3.21)). In the case of the quadratic function (3.48)r2J(w) =Aand hence=minandL=max, wheremin (respectively max) denotes the minimum (respectively maximum) eigen- value ofA. One can thus change the condition number of the problem by varying the eigen-spectrum of the matrix A. For instance, if we set Ato thennidentity matrix, then max=min= 1 and hence the problem is well conditioned. In this case, gradient descent converges very quickly to the optimal solution. We illustrate this behavior on a two dimensional quadratic function in Figure 3.8 (right). On the other hand, if we choose Asuch thatmaxminthen the problem (3.48) becomes ill-conditioned. In this case gradient descent exhibits zigzagging and slow convergence as can be seen in Figure 3.8 (left). Because of these shortcomings, gradient descent is not widely used in practice. A number of dierent algorithms we described below can be understood as explicitly or implicitly changing the condition number of the problem to accelerate convergence. 3.2.4 Mirror Descent One way to motivate gradient descent is to use the following quadratic ap- proximation of the objective function Qt(w) :=J(wt) +hrJ(wt);w wti+1 2(w wt)>(w wt); (3.49) where, as in the previous section, rJ() denotes the gradient of J. Mini- mizing this quadratic model at every iteration entails taking gradients with

3.2 Unconstrained Smooth Convex Minimization 109 Fig. 3.8. Convergence of gradient descent with exact line search on two quadratic problems (3.48). The problem on the left is ill-conditioned, whereas the problem on the right is well-conditioned. We plot the contours of the objective function, and the steps taken by gradient descent. As can be seen gradient descent converges fast on the well conditioned problem, while it zigzags and takes many iterations to converge on the ill-conditioned problem. respect towand setting it to zero, which gives w wt:= rJ(wt): (3.50) Performing a line search along the direction  rJ(wt) recovers the familiar gradient descent update wt+1=wt trJ(wt): (3.51) The closely related mirror descent method replaces the quadratic penalty in (3.49) by a Bregman divergence dened by some convex function fto yield Qt(w) :=J(wt) +hrJ(wt);w wti+ f(w;wt): (3.52) Computing the gradient, setting it to zero, and using rwf(w;wt) =rf(w)  rf(wt), the minimizer of the above model can be written as rf(w) rf(wt) = rJ(wt): (3.53) As before, by using a stepsize tthe resulting updates can be written as wt+1=rf 1(rf(wt) trJ(wt)): (3.54) It is easy to verify that choosing f() =1 2kk2recovers the usual gradient descent updates. On the other hand if we choose fto be the un-normalized entropy (3.30) then rf() = log and therefore (3.54) specializes to wt+1= exp(log(wt) trJ(wt)) =wtexp( trJ(wt)); (3.55) which is sometimes called the Exponentiated Gradient (EG) update.

110 3 Optimization Theorem 3.14 LetJbe a convex function and J(w)denote its minimum value. The mirror descent updates (3.54) with a-strongly convex function fsatisfy f(w;w1) +1 2P t2 tkrJ(wt)k2 P ttmin tJ(wt) J(w): Proof Using the convexity of J(see (3.7)) and (3.54) we can write J(w)J(wt) +hw wt;rJ(wt)i J(wt) 1 thw wt;f(wt+1) f(wt)i: Now applying Lemma 3.11 and rearranging f(w;wt) f(w;wt+1) + f(wt;wt+1)t(J(wt) J(w)): Summing over t= 1;:::;T f(w;w1) f(w;wT+1) +X tf(wt;wt+1)X tt(J(wt) J(w)): Noting that  f(w;wT+1)0,J(wt) J(w)mintJ(wt) J(w), and rearranging it follows that f(w;w1) +P tf(wt;wt+1)P ttmin tJ(wt) J(w): (3.56) Using (3.17) and (3.54) f(wt;wt+1)1 2krf(wt) rf(wt+1)k2=1 22 tkrJ(wt)k2:(3.57) The proof is completed by plugging in (3.57) into (3.56). Corollary 3.15 IfJhas a Lipschitz continuous gradient with modulus L, and the stepsizes tare chosen as t=p 2f(w;w1) L1p tthen (3.58) min 1tTJ(wt) J(w)Lr 2f(w;w1) 1p T: Proof SincerJis Lipschitz continuous min 1tTJ(wt) J(w)f(w;w1) +1 2P t2 tL2 P tt:

3.2 Unconstrained Smooth Convex Minimization 111 Plugging in (3.58) and using Problem 3.15 min 1tTJ(wt) J(w)Lr f(w;w1) 2(1 +P t1 t)P t1p tLr f(w;w1) 21p T: 3.2.5 Conjugate Gradient Let us revisit the problem of minimizing the quadratic objective function (3.48). SincerJ(w) =Aw b, at the optimum rJ(w) = 0 (see Lemma 3.6) and hence Aw=b: (3.59) In fact, the Conjugate Gradient (CG) algorithm was rst developed as a method to solve the above linear system. As we already saw, updating walong the negative gradient direction may lead to zigzagging. Therefore CG uses the so-called conjugate directions . Denition 3.16 (Conjugate Directions) Non zero vectors ptandpt0are said to be conjugate with respect to a symmetric positive denite matrix A ifp> t0Apt= 0ift6=t0. Conjugate directions fp0;:::;pn 1gare linearly independent and form a basis. To see this, suppose the pt's are not linearly independent. Then there exists non-zero coecients tsuch thatP ttpt= 0. Thept's are conjugate directions, therefore p> t0A(P ttpt) =P ttp> t0Apt=t0p> t0Apt0= 0 for allt0. SinceAis positive denite this implies that t0= 0 for allt0, a contradiction. As it turns out, the conjugate directions can be generated iteratively as follows: Starting with any w02Rndenep0= g0=b Aw0, and set t= g> tpt p> tApt(3.60a) wt+1=wt+tpt (3.60b) gt+1=Awt+1 b (3.60c) t+1=g> t+1Apt p> tApt(3.60d) pt+1= gt+1+t+1pt (3.60e)

112 3 Optimization The following theorem asserts that the ptgenerated by the above procedure are indeed conjugate directions. Theorem 3.17 Suppose the t-th iterate generated by the conjugate gradient method (3.60) is not the solution of (3.59) , then the following properties hold: spanfg0;g1;:::;gtg= spanfg0;Ag 0;:::;Atg0g: (3.61) spanfp0;p1;:::;ptg= spanfg0;Ag 0;:::;Atg0g: (3.62) p> jgt= 0for allj <t (3.63) p> jApt= 0for allj <t: (3.64) Proof The proof is by induction. The induction hypothesis holds trivially att= 0. Assuming that (3.61) to (3.64) hold for some t, we prove that they continue to hold for t+ 1. Step 1: We rst prove that (3.63) holds. Using (3.60c), (3.60b) and (3.60a) p> jgt+1=p> j(Awt+1 b) =p> j(Awt+tpt b) =p> j Awt g> tpt p> tAptApt b =p> jgt p> jApt p> tAptg> tpt: Forj=t, both terms cancel out, while for j <t both terms vanish due to the induction hypothesis. Step 2: Next we prove that (3.61) holds. Using (3.60c) and (3.60b) gt+1=Awt+1 b=Awt+tApt b=gt+tApt: By our induction hypothesis, gt2spanfg0;Ag 0;:::;Atg0g, whileApt2 spanfAg0;A2g0;:::;At+1g0g. Combining the two we conclude that gt+12 spanfg0;Ag 0;:::;At+1g0g. On the other hand, we already showed that gt+1 is orthogonal tofp0;p1;:::;ptg. Therefore, gt+1=2spanfp0;p1;:::;ptg. Thus our induction assumption implies that gt+1=2spanfg0;Ag 0;:::;Atg0g. This allows us to conclude that span fg0;g1;:::;gt+1g= spanfg0;Ag 0;:::;At+1g0g.

3.2 Unconstrained Smooth Convex Minimization 113 Step 3 We now prove (3.64) holds. Using (3.60e) p> t+1Apj= g> t+1Apj+t+1p> tApj: By the denition of t+1(3.60d) the above expression vanishes for j=t. For j <t , the rst term is zero because Apj2spanfp0;p1;:::;pj+1g, a subspace orthogonal to gt+1as already shown in Step 1. The induction hypothesis guarantees that the second term is zero. Step 4 Clearly, (3.61) and (3.60e) imply (3.62). This concludes the proof. A practical implementation of (3.60) requires two more observations: First, using (3.60e) and (3.63)  g> tpt=g> tgt tg> tpt 1=g> tgt: Therefore (3.60a) simplies to t=g> tgt p> tApt: (3.65) Second, using (3.60c) and (3.60b) gt+1 gt=A(wt+1 wt) =tApt: Butgt2spanfp0;:::;ptg, a subspace orthogonal to gt+1by (3.63). Therefore g> t+1Apt=1 t(g> t+1gt+1). Substituting this back into (3.60d) and using (3.65) yields t+1=g> t+1gt+1 g> tgt: (3.66) We summarize the CG algorithm in Algorithm 3.3. Unlike gradient descent whose convergence rates for minimizing the quadratic objective function (3.48) depend upon the condition number of A, as the following theorem shows, the CG iterates converge in at most nsteps. Theorem 3.18 The CG iterates (3.60) converge to the minimizer of (3.48) after at most nsteps. Proof Letwdenote the minimizer of (3.48). Since the pt's form a basis w w0=0p0+:::+n 1pn 1; for some scalars t. Our proof strategy will be to show that the coecients

114 3 Optimization Algorithm 3.3 Conjugate Gradient 1:Input: Initial point w0, residual norm tolerance  2:Sett= 0,g0=Aw0 b, andp0= g0 3:whilekAwt bkdo 4:t=g> tgt p> tApt 5:wt+1=wt+tpt 6:gt+1=gt+tApt 7:t+1=g> t+1gt+1 g> tgt 8:pt+1= gt+1+t+1pt 9:t=t+ 1 10:end while 11:Return:wt tcoincide with tdened in (3.60a). Towards this end premultiply with p> tAand use conjugacy to obtain t=p> tA(w w0) p> tApt: (3.67) On the other hand, following the iterative process (3.60b) from w0untilwt yields wt w0=0p0+:::+t 1pt 1: Again premultiplying with p> tAand using conjugacy p> tA(wt w0) = 0: (3.68) Substituting (3.68) into (3.67) produces t=p> tA(w wt) p> tApt= g> tpt p> tApt; (3.69) thus showing that t=t. Observe that the gt+1computed via (3.60c) is nothing but the gradient of J(wt+1). Furthermore, consider the following one dimensional optimization problem: min 2Rt() :=J(wt+pt): Dierentiating twith respect to  0 t() =p> t(Awt+Apt b) =p> t(gt+Apt):

3.2 Unconstrained Smooth Convex Minimization 115 The gradient vanishes if we set = g> tpt p> tApt, which recovers (3.60a). In other words, every iteration of CG minimizes J(w) along a conjugate direction pt. Contrast this with gradient descent which minimizes J(w) along the negative gradient direction gtat every iteration. It is natural to ask if this idea of generating conjugate directions and minimizing the objective function along these directions can be applied to general convex functions. The main diculty here is that Theorems 3.17 and 3.18 do not hold. In spite of this, extensions of CG are eective even in this setting. Basically the update rules for gtandptremain the same, but the parameters tandtare computed dierently. Table 3.2 gives an overview of dierent extensions. See [NW99, Lue84] for details. Table 3.2. Non-Quadratic modications of Conjugate Gradient Descent Generic Method Compute Hessian Kt:=r2J(wt) and update t andtwith t= g> tpt p> tKtptandt= g> t+1Ktpt p> tKtpt Fletcher-Reeves Set t= argminJ(wt+pt) andt=g> t+1gt+1 g> tgt. Polak-Ribi ere Set t= argminJ(wt+pt),yt=gt+1 gt, and t=y> tgt+1 g> tgt. In practice, Polak-Ribi ere tends to be better than Fletcher-Reeves. Hestenes-Stiefel Set t= argminJ(wt+pt),yt=gt+1 gt, and t=y> tgt+1 y> tpt. 3.2.6 Higher Order Methods Recall the motivation for gradient descent as the minimizer of the quadratic model Qt(w) :=J(wt) +hrJ(wt);w wti+1 2(w wt)>(w wt); The quadratic penalty in the above equation uniformly penalizes deviation fromwtin dierent dimensions. When the function is ill-conditioned one would intuitively want to penalize deviations in dierent directions dier- ently. One way to achieve this is by using the Hessian, which results in the

116 3 Optimization Algorithm 3.4 Newton's Method 1:Input: Initial point w0, gradient norm tolerance  2:Sett= 0 3:whilekrJ(wt)k>do 4: Computept:= r2J(wt) 1rJ(wt) 5: Computet= argminJ(wt+pt)e.g., via Algorithm 3.1. 6:wt+1=wt+tpt 7:t=t+ 1 8:end while 9:Return:wt following second order Taylor approximation: Qt(w) :=J(wt) +hrJ(wt);w wti+1 2(w wt)>r2J(wt)(w wt): (3.70) Of course, this requires that Jbe twice dierentiable. We will also assume thatJis strictly convex and hence its Hessian is positive denite and in- vertible. Minimizing Qtby taking gradients with respect to wand setting it zero obtains w wt:= r2J(wt) 1rJ(wt); (3.71) Since we are only minimizing a model of the objective function, we perform a line search along the descent direction (3.71) to compute the stepsize t, which yields the next iterate: wt+1=wt tr2J(wt) 1rJ(wt): (3.72) Details can be found in Algorithm 3.4. Supposewdenotes the minimum of J(w). We say that an algorithm exhibits quadratic convergence if the sequences of iterates fwkggenerated by the algorithm satises: kwk+1 wkCkwk wk2(3.73) for some constant C > 0. We now show that Newton's method exhibits quadratic convergence close to the optimum. Theorem 3.19 (Quadratic convergence of Newton's Method) Suppose Jis twice dierentiable, strongly convex, and the Hessian of Jis bounded and Lipschitz continuous with modulus Min a neighborhood of the so- lutionw. Furthermore, assume thatr2J(w) 1N. The iterations

3.2 Unconstrained Smooth Convex Minimization 117 wt+1=wt r2J(wt) 1rJ(wt)converge quadratically to w, the minimizer ofJ. Proof First notice that rJ(wt) rJ(w) =Z1 0r2J(wt+t(w wt))(wt w)dt: (3.74) Next using the fact that r2J(wt) is invertible and the gradient vanishes at the optimum (rJ(w) = 0), write wt+1 w=wt w r2J(wt) 1rJ(wt) =r2J(wt) 1[r2J(wt)(wt w) (rJ(wt) rJ(w))]:(3.75) Using (3.75), (3.74), and the Lipschitz continuity of r2J rJ(wt) rJ(w) r2J(wt)(wt w) =Z1 0[r2J(wt+t(wt w)) r2J(wt)](wt w)dt Z1 0[r2J(wt+t(wt w)) r2J(wt)]k(wt w)kdt kwt wk2Z1 0Mtdt =M 2kwt wk2: (3.76) Finally use (3.75) and (3.76) to conclude that kwt+1 wkM 2r2J(wt) 1kwt wk2NM 2kwt wk2: Newton's method as we described it suers from two major problems. First, it applies only to twice dierentiable, strictly convex functions. Sec- ond, it involves computing and inverting of the nnHessian matrix at every iteration, thus making it computationally very expensive. Although Newton's method can be extended to deal with positive semi-denite Hes- sian matrices, the computational burden often makes it unsuitable for large scale applications. In such cases one resorts to Quasi-Newton methods. 3.2.6.1 Quasi-Newton Methods Unlike Newton's method, which computes the Hessian of the objective func- tion at every iteration, quasi-Newton methods never compute the Hessian; they approximate it from past gradients. Since they do not require the ob- jective function to be twice dierentiable, quasi-Newton methods are much

118 3 Optimization 6  4  2  0  2  4  6 400 200 0 200 400 600 800 1000 1200 Fig. 3.9. The blue solid line depicts the one dimensional convex function J(w) = w4+ 20w2+w. The green dotted-dashed line represents the rst order Taylor approximation to J(w), while the red dashed line represents the second order Taylor approximation, both evaluated at w= 2. more widely applicable. They are widely regarded as the workhorses of smooth nonlinear optimization due to their combination of computational ef- ciency and good asymptotic convergence. The most popular quasi-Newton algorithm is BFGS, named after its discoverers Broyde, Fletcher, Goldfarb, and Shanno. In this section we will describe BFGS and its limited memory counterpart LBFGS. Suppose we are given a smooth (not necessarily strictly) convex objective functionJ:Rn!Rand a current iterate wt2Rn. Just like Newton's method, BFGS forms a local quadratic model of the objective function, J: Qt(w) :=J(wt) +hrJ(wt);w wti+1 2(w wt)>Ht(w wt):(3.77) Unlike Newton's method which uses the Hessian to build its quadratic model (3.70), BFGS uses the matrix Ht0, which is a positive-denite estimate of the Hessian. A quasi-Newton direction of descent is found by minimizing Qt(w): w wt= H 1 trJ(wt): (3.78) The stepsize t>0 is found by a line search obeying the Wolfe conditions

3.2 Unconstrained Smooth Convex Minimization 119 (3.42) and (3.43). The nal update is given by wt+1=wt tH 1 trJ(wt): (3.79) Givenwt+1we need to update our quadratic model (3.77) to Qt+1(w) :=J(wt+1) +hrJ(wt+1);w wt+1i+1 2(w wt+1)>Ht+1(w wt+1): (3.80) When updating our model it is reasonable to expect that the gradient of Qt+1should match the gradient of Jatwtandwt+1. Clearly, rQt+1(w) =rJ(wt+1) +Ht+1(w wt+1); (3.81) which implies that rQt+1(wt+1) =rJ(wt+1), and hence our second con- dition is automatically satised. In order to satisfy our rst condition, we require rQt+1(wt) =rJ(wt+1) +Ht+1(wt wt+1) =rJ(wt): (3.82) By rearranging, we obtain the so-called secant equation : Ht+1st=yt; (3.83) wherest:=wt+1 wtandyt:=rJ(wt+1) rJ(wt) denote the most recent step along the optimization trajectory in parameter and gradient space, respectively. Since Ht+1is a positive denite matrix, pre-multiplying the secant equation by styields the curvature condition s> tyt>0: (3.84) If the curvature condition is satised, then there are an innite number of matrices Ht+1which satisfy the secant equation (the secant equation representsnlinear equations, but the symmetric matrix Ht+1hasn(n+1)=2 degrees of freedom). To resolve this issue we choose the closest matrix to Htwhich satises the secant equation. The key insight of the BFGS comes from the observation that the descent direction computation (3.78) involves the inverse matrix Bt:=H 1 t. Therefore, we choose a matrix Bt+1:=H 1 t+1 such that it is close to Btand also satises the secant equation: min BkB Btk (3.85) s. t.B=B>andByt=st: (3.86) If the matrix norm kkis appropriately chosen [NW99], then it can be shown that Bt+1= (1 tsty> t)Bt(1 tyts> t) +tsts> t; (3.87)

120 3 Optimization Algorithm 3.5 LBFGS 1:Input: Initial point w0, gradient norm tolerance >0 2:Sett= 0 andB0=I 3:whilekrJ(wt)k>do 4:pt= BtrJ(wt) 5: Findtthat obeys (3.42) and (3.43) 6:st=tpt 7:wt+1=wt+st 8:yt:=rJ(wt+1) rJ(wt) 9: ift= 0 :Bt:=s> tyt y> tytI 10:t= (s> tyt) 1 11:Bt+1= (I tsty> t)Bt(I tyts> t) +tsts> t 12:t=t+ 1 13:end while 14:Return:wt wheret:= (y> tst) 1. In other words, the matrix Btis modied via an incremental rank-two update, which is very ecient to compute, to obtain Bt+1. There exists an interesting connection between the BFGS update (3.87) and the Hestenes-Stiefel variant of Conjugate gradient. To see this assume that an exact line search was used to compute wt+1, and therefore s> trJ(wt+1) = 0. Furthermore, assume that Bt=1, and use (3.87) to write pt+1= Bt+1rJ(wt+1) = rJ(wt+1) +y> trJ(wt+1) y> tstst; (3.88) which recovers the Hestenes-Stiefel update (see (3.60e) and Table 3.2). Limited-memory BFGS (LBFGS) is a variant of BFGS designed for solv- ing large-scale optimization problems where the O(d2) cost of storing and updatingBtwould be prohibitively expensive. LBFGS approximates the quasi-Newton direction (3.78) directly from the last mpairs ofstandytvia a matrix-free approach. This reduces the cost to O(md) space and time per iteration, with mfreely chosen. Details can be found in Algorithm 3.5. 3.2.6.2 Spectral Gradient Methods Although spectral gradient methods do not use the Hessian explicitly, they are motivated by arguments very reminiscent of the Quasi-Newton methods. Recall the update rule (3.79) and secant equation (3.83). Suppose we want

3.2 Unconstrained Smooth Convex Minimization 121 a very simple matrix which approximates the Hessian. Specically, we want Ht+1=t+1I (3.89) wheret+1is a scalar and Idenotes the identity matrix. Then the secant equation (3.83) becomes t+1st=yt: (3.90) In general, the above equation cannot be solved. Therefore we use the t+1 which minimizeskt+1st ytk2which yields the Barzilai-Borwein (BB) step- size t+1=s> tyt s> tst: (3.91) As it turns out, t+1lies between the minimum and maximum eigenvalue of the average Hessian in the direction st, hence the name Spectral Gradient method. The parameter update (3.79) is now given by wt+1=wt 1 trJ(wt): (3.92) A practical implementation uses safeguards to ensure that the stepsize t+1 is neither too small nor too large. Given 0 < min< max<1we compute t+1= min max;max min;s> tyt s> tst : (3.93) One of the peculiar features of spectral gradient methods is their use of a non-monotone line search. In all the algorithms we have seen so far, the stepsize is chosen such that the objective function Jdecreases at every iteration. In contrast, non-monotone line searches employ a parameter M 1 and ensure that the objective function decreases in every Miterations. Of course, setting M= 1 results in the usual monotone line search. Details can be found in Algorithm 3.6. 3.2.7 Bundle Methods The methods we discussed above are applicable for minimizing smooth, con- vex objective functions. Some regularized risk minimization problems involve a non-smooth objective function. In such cases, one needs to use bundle methods. In order to lay the ground for bundle methods we rst describe their precursor the cutting plane method [Kel60]. Cutting plane method is based on a simple observation: A convex function is bounded from below by

122 3 Optimization Algorithm 3.6 Spectral Gradient Method 1:Input:w0,M1,max>  min>0,2(0;1), 1>  2>  1>0, 02[min;max], and>0 2:Initialize: t= 0 3:whilekrJ(wt)k>do 4:= 1 5:while TRUE do 6:dt= 1 trJ(wt) 7:w+=wt+dt 8:=hdt;rJ(wt)i 9: ifJ(w+)min 0jmin(t;M 1)J(xt j) +then 10:wt+1=w+ 11:st=wt+1 wt 12:yt=rJ(wt+1) rJ(wt) 13: break 14: else 15:tmp= 1 22=(J(w+) J(wt) ) 16: iftmp> 1andtmp< 2then 17: =tmp 18: else 19: ==2 20: end if 21: end if 22: end while 23:t+1= min(max;max(min;s> tyt s> tst)) 24:t=t+ 1 25:end while 26:Return:wt its linearization ( i.e.,rst order Taylor approximation). See Figures 3.4 and 3.5 for geometric intuition, and recall (3.7) and (3.13): J(w)J(w0) + w w0;s0 8wands02@J(w0): (3.94) Given subgradients s1;s2;:::;stevaluated at locations w0;w1;:::;wt 1, we can construct a tighter (piecewise linear) lower bound for Jas follows (also see Figure 3.10): J(w)JCP t(w) := max 1itfJ(wi 1) +hw wi 1;siig: (3.95)

3.2 Unconstrained Smooth Convex Minimization 123 Given iteratesfwigt 1 i=0, the cutting plane method minimizes JCP tto obtain the next iterate wt: wt:= argmin wJCP t(w): (3.96) This iteratively renes the piecewise linear lower bound JCPand allows us to get close to the minimum of J(see Figure 3.10 for an illustration). Ifwdenotes the minimizer of J, then clearly each J(wi)J(w) and hence min 0itJ(wi)J(w). On the other hand, since JJCP tit fol- lows thatJ(w)JCP t(wt). In other words, J(w) is sandwiched between min 0itJ(wi) andJCP t(wt) (see Figure 3.11 for an illustration). The cutting plane method monitors the monotonically decreasing quantity t:= min 0itJ(wi) JCP t(wt); (3.97) and terminates whenever tfalls below a predened threshold . This ensures that the solution J(wt) isoptimum, that is, J(wt)J(w) +. Fig. 3.10. A convex function (blue solid curve) is bounded from below by its lin- earizations (dashed lines). The gray area indicates the piecewise linear lower bound obtained by using the linearizations. We depict a few iterations of the cutting plane method. At each iteration the piecewise linear lower bound is minimized and a new linearization is added at the minimizer (red rectangle). As can be seen, adding more linearizations improves the lower bound. Although cutting plane method was shown to be convergent [Kel60], it is

124 3 Optimization Fig. 3.11. A convex function (blue solid curve) with four linearizations evaluated at four dierent locations (magenta circles). The approximation gap 3at the end of fourth iteration is indicated by the height of the cyan horizontal band i.e.,dierence between lowest value of J(w) evaluated so far and the minimum of JCP 4(w) (red diamond). well known (see e.g., [LNN95, Bel05]) that it can be very slow when new iterates move too far away from the previous ones ( i.e., causing unstable \zig-zag" behavior in the iterates). In fact, in the worst case the cutting plane method might require exponentially many steps to converge to an  optimum solution. Bundle methods stabilize CPM by augmenting the piecewise linear lower (e.g.,JCP t(w) in (3.95)) with a prox-function ( i.e.,proximity control func- tion) which prevents overly large steps in the iterates [Kiw90]. Roughly speaking, there are 3 popular types of bundle methods, namely, proximal [Kiw90], trust region [SZ92], and level set [LNN95]. All three versions use 1 2kk2as their prox-function, but dier in the way they compute the new iterate: proximal: wt:= argmin wft 2kw ^wt 1k2+JCP t(w)g; (3.98) trust region: wt:= argmin wfJCP t(w)j1 2kw ^wt 1k2tg; (3.99) level set:wt:= argmin wf1 2kw ^wt 1k2jJCP t(w)tg;(3.100) where ^wt 1is the current prox-center, and t;t;andtare positive trade- o parameters of the stabilization. Although (3.98) can be shown to be equivalent to (3.99) for appropriately chosen tandt, tuningtis rather dicult while a trust region approach can be used for automatically tuning

3.3 Constrained Optimization 125 t. Consequently the trust region algorithm BT of [SZ92] is widely used in practice. 3.3 Constrained Optimization So far our focus was on unconstrained optimization problems. Many ma- chine learning problems involve constraints, and can often be written in the following canonical form: min wJ(w) (3.101a) s. t.ci(w)0 fori2I (3.101b) ei(w) = 0 fori2E (3.101c) where both ciandeiare convex functions. We say that wis feasible if and only if it satises the constraints, that is, ci(w)0 fori2Iandei(w) = 0 fori2E. Recall that wis the minimizer of an unconstrained problem if and only if krJ(w)k= 0 (see Lemma 3.6). Unfortunately, when constraints are present one cannot use this simple characterization of the solution. For instance, the wat whichkrJ(w)k= 0 may not be a feasible point. To illustrate, consider the following simple minimization problem (see Figure 3.12): min w1 2w2(3.102a) s. t. 1w2: (3.102b) Clearly,1 2w2is minimized at w= 0, but because of the presence of the con- straints, the minimum of (3.102) is attained at w= 1 whererJ(w) =wis equal to 1. Therefore, we need other ways to detect convergence. In Section 3.3.1 we discuss some general purpose algorithms based on the concept of or- thogonal projection. In Section 3.3.2 we will discuss Lagrange duality, which can be used to further characterize the solutions of constrained optimization problems. 3.3.1 Projection Based Methods Suppose we are interested in minimizing a smooth convex function of the following form: min w2 J(w); (3.103)

126 3 Optimization 6  4  2  0 2 4 6 w02468101214 J(w) Fig. 3.12. The unconstrained minimum of the quadratic function1 2w2is attained atw= 0 (red circle). But, if we enforce the constraints 1 w2 (illustrated by the shaded area) then the minimizer is attained at w= 1 (green diamond). where   is a convex feasible region. For instance,   may be described by convex functions ciandeias in (3.101). The algorithms we describe in this section are applicable when   is a relatively simple set onto which we can compute an orthogonal projection. Given a point w0and a feasible region

, the orthogonal projection P (w0) ofw0on   is dened as P (w0) := argmin w2 w0 w2: (3.104) Geometrically speaking, P (w0) is the closest point to w0in  . Of course, if w02  thenP (w0) =w0. We are interested in nding an approximate solution of (3.103), that is, aw2  such that J(w) min w2 J(w) =J(w) J; (3.105) for some pre-dened tolerance >0. Of course, Jis unknown and hence the gapJ(w) Jcannot be computed in practice. Furthermore, as we showed in Section 3.3, for constrained optimization problems krJ(w)kdoes not vanish at the optimal solution. Therefore, we will use the following stopping

3.3 Constrained Optimization 127 Algorithm 3.7 Basic Projection Based Method 1:Input: Initial point w02 , and projected gradient norm tolerance >0 2:Initialize: t= 0 3:whilekP (wt rJ(wt)) wtk>do 4: Find direction of descent dt 5:wt+1=P (wt+tdt) 6:t=t+ 1 7:end while 8:Return:wt criterion in our algorithms kP (wt rJ(wt)) wtk: (3.106) The intuition here is as follows: If wt rJ(wt)2  thenP (wt  rJ(wt)) =wtif, and only if,rJ(wt) = 0, that is, wtis the global minimizer ofJ(w). On the other hand, if wt rJ(wt)=2  butP (wt rJ(wt)) =wt, then the constraints are preventing us from making any further progress along the descent direction  rJ(wt) and hence we should stop. The basic projection based method is described in Algorithm 3.7. Any unconstrained optimization algorithm can be used to generate the direction of descent dt. A line search is used to nd the stepsize t. The updated parameterwt tdtis projected onto   to obtain wt+1. Ifdtis chosen to be the negative gradient direction  rJ(wt), then the resulting algorithm is called the projected gradient method. One can show that the rates of convergence of gradient descent with various line search schemes is also preserved by projected gradient descent. 3.3.2 Lagrange Duality Lagrange duality plays a central role in constrained convex optimization. The basic idea here is to augment the objective function (3.101) with a weighted sum of the constraint functions by dening the Lagrangian: L(w;; ) =J(w) +X i2Iici(w) +X i2Eiei(w) (3.107) fori0 andi2R. In the sequel, we will refer to (respectively ) as the Lagrange multipliers associated with the inequality (respectively equality) constraints. Furthermore, we will call anddual feasible if and only if

128 3 Optimization i0 andi2R. The Lagrangian satises the following fundamental property, which makes it extremely useful for constrained optimization. Theorem 3.20 The Lagrangian (3.107) of(3.101) satises max 0;L(w;; ) =( J(w)ifwis feasible 1otherwise: In particular, if Jdenotes the optimal value of (3.101) , then J= min wmax 0;L(w;; ): Proof First assume that wis feasible, that is, ci(w)0 fori2Iand ei(w) = 0 fori2E. Sincei0 we have X i2Iici(w) +X i2Eiei(w)0; (3.108) with equality being attained by setting i= 0 whenever ci(w)<0. Conse- quently, max 0;L(w;; ) = max 0;J(w) +X i2Iici(w) +X i2Eiei(w) =J(w) wheneverwis feasible. On the other hand, if wis not feasible then either ci0(w)>0 orei0(w)6= 0 for some i0. In the rst case simply let i0!1 to see that max 0;L(w;; )!1 . Similarly, when ei0(w)6= 0 leti0!1 ifei0(w)>0 ori0! 1 ifei0(w)<0 to arrive at the same conclusion. If dene the Lagrange dual function D(;) = min wL(w;; ); (3.109) for0 and, then one can prove the following property, which is often called as weak duality . Theorem 3.21 (Weak Duality) The Lagrange dual function (3.109) sat- ises D(;)J(w) for all feasible wand0and. In particular D:= max 0;min wL(w;; )min wmax 0;L(w;; ) =J: (3.110)

3.3 Constrained Optimization 129 Proof As before, observe that whenever wis feasible X i2Iici(w) +X i2Eiei(w)0: Therefore D(;) = min wL(w;; ) = min wJ(w) +X i2Iici(w) +X i2Eiei(w)J(w) for all feasible wand0 and. In particular, one can choose wto be the minimizer of (3.101) and 0 andto be maximizers of D(;) to obtain (3.110). Weak duality holds for any arbitrary function, not-necessarily convex. When the objective function and constraints are convex, and certain technical con- ditions, also known as Slater's conditions hold, then we can say more. Theorem 3.22 (Strong Duality) Supposed the objective function fand constraints cifori2Iandeifori2Ein(3.101) are convex and the following constraint qualication holds: There exists a wsuch thatci(w)<0for alli2I. Then the Lagrange dual function (3.109) satises D:= max 0;min wL(w;; ) = min wmax 0;L(w;; ) =J: (3.111) The proof of the above theorem is quite technical and can be found in any standard reference ( e.g., [BV04]). Therefore we will omit the proof and proceed to discuss various implications of strong duality. First note that min wmax 0;L(w;; ) = max 0;min wL(w;; ): (3.112) In other words, one can switch the order of minimization over wwith max- imization over and. This is called the saddle point property of convex functions. Suppose strong duality holds. Given any 0 andsuch thatD(;)>  1 and a feasible wwe can immediately write the duality gap J(w) J=J(w) DJ(w) D(;); whereJandDwere dened in (3.111). Below we show that if wis primal optimal and ( ;) are dual optimal then J(w) D(;) = 0. This provides a non-heuristic stopping criterion for constrained optimization: stop whenJ(w) D(;), whereis a pre-specied tolerance.

130 3 Optimization Suppose the primal and dual optimal values are attained at wand (;) respectively, and consider the following line of argument: J(w) =D(;) (3.113a) = min wJ(w) +X i2I ici(w) +X i2E iej(w) (3.113b) J(w) +X i2I ici(w) +X i2E iei(w) (3.113c) J(w): (3.113d) To write (3.113a) we used strong duality, while (3.113c) obtains by setting w=win (3.113c). Finally, to obtain (3.113d) we used the fact that wis feasible and hence (3.108) holds. Since (3.113) holds with equality, one can conclude that the following complementary slackness condition : X i2I ici(w) +X i2E iei(w) = 0: In other words,  ici(w) = 0 or equivalently  i= 0 whenever ci(w)<0. Furthermore, since wminimizesL(w;;) overw, it follows that its gradient must vanish at w, that is, rJ(w) +X i2I irci(w) +X i2E irei(w) = 0: Putting everything together, we obtain ci(w)08i2I (3.114a) ej(w) = 08i2E (3.114b)  i0 (3.114c)  ici(w) = 0 (3.114d) rJ(w) +X i2I irci(w) +X i2E irei(w) = 0: (3.114e) The above conditions are called the KKT conditions. If the primal problem is convex, then the KKT conditions are both necessary and sucient. In other words, if ^wand (^;^) satisfy (3.114) then ^ wand (^;^) are primal and dual optimal with zero duality gap. To see this note that the rst two conditions show that ^wis feasible. Since i0,L(w;; ) is convex in w. Finally the last condition states that ^ wminimizesL(w;^;^). Since ^ici( ^w) = 0 and

3.3 Constrained Optimization 131 ej( ^w) = 0, we have D(^;^) = min wL(w;^;^) =J( ^w) +nX i=1^ici( ^w) +mX j=1^jej( ^w) =J( ^w): 3.3.3 Linear and Quadratic Programs So far we discussed general constrained optimization problems. Many ma- chine learning problems have special structure which can be exploited fur- ther. We discuss the implication of duality for two such problems. 3.3.3.1 Linear Programming An optimization problem with a linear objective function and (both equality and inequality) linear constraints is said to be a linear program (LP). A canonical linear program is of the following form: min wc>w (3.115a) s. t.Aw=b;w0: (3.115b) Herewandcarendimensional vectors, while bis amdimensional vector, andAis amnmatrix with m<n . Suppose we are given a LP of the form: min wc>w (3.116a) s. t.Awb; (3.116b) we can transform it into a canonical LP by introducing non-negative slack variables min w;c>w (3.117a) s. t.Aw =b;0: (3.117b) Next, we split winto its positive and negative parts w+andw respec- tively by setting w+ i= max(0;wi) andw  i= max(0; wi). Using these new

132 3 Optimization variables we rewrite (3.117) as min w+;w ;2 4c  c 03 5>2 4w+ w  3 5 (3.118a) s. t. A A I2 4w+ w  3 5=b;2 4w+ w  3 50; (3.118b) thus yielding a canonical LP (3.115) in the variables w+,w and. By introducing non-negative Lagrange multipliers andone can write the Lagrangian of (3.115) as L(w;;s ) =c>w+>(Aw b) >w: (3.119) Taking gradients with respect to the primal and dual variables and setting them to zero obtains A> =c (3.120a) Aw=b (3.120b) >w= 0 (3.120c) w0 (3.120d) 0: (3.120e) Condition (3.120c) can be simplied by noting that both wandare con- strained to be non-negative, therefore >w= 0 if, and only if, iwi= 0 for i= 1;:::;n . Using (3.120a), (3.120c), and (3.120b) we can write c>w= (A> )>w=>Aw=>b: Substituting this into (3.115) and eliminating the primal variable wyields the following dual LP max ;b> (3.121a) s.t.A> =c;0: (3.121b) As before, we let += max(;0) and = max(0; ) and convert the

3.3 Constrained Optimization 133 above LP into the following canonical LP max ;+; 2 4b  b 03 5>2 4+   3 5 (3.122a) s.t. A> A> I2 4+   3 5=c;2 4+   3 50: (3.122b) It can be easily veried that the primal-dual problem is symmetric; by taking the dual of the dual we recover the primal (Problem 3.17). One important thing to note however is that the primal (3.115) involves nvariables and n+mconstraints, while the dual (3.122) involves 2 m+nvariables and 4m+ 2nconstraints. 3.3.3.2 Quadratic Programming An optimization problem with a convex quadratic objective function and lin- ear constraints is said to be a convex quadratic program (QP). The canonical convex QP can be written as follows: min w1 2w>Gx+w>d (3.123a) s.t.a> iw=bifori2E (3.123b) a> iwbifori2I (3.123c) HereG0 is annpositive semi-denite matrix, EandIare nite set of indices, while dandaiarendimensional vectors, and biare scalars. As a warm up let us consider the arguably simpler equality constrained quadratic programs. In this case, we can stack the aiinto a matrix Aand thebiinto a vector bto write min w1 2w>Gw+w>d (3.124a) s.t.Aw=b (3.124b) By introducing non-negative Lagrange multipliers the Lagrangian of the above optimization problem can be written as L(w;) =1 2w>Gw+w>d+(Aw b): (3.125) To nd the saddle point of the Lagrangian we take gradients with respect

134 3 Optimization towandand set them to zero. This obtains Gw+d+A>= 0 Aw=b: Putting these two conditions together yields the following linear system of equations G A> A 0w  = d b : (3.126) The matrix in the above equation is called the KKT matrix, and we can use it to characterize the conditions under which (3.124) has a unique solution. Theorem 3.23 LetZbe an(n m)matrix whose columns form a basis for the null space of A, that is,AZ= 0. IfAhas full row rank, and the reduced-Hessian matrix Z>GZis positive denite, then there exists a unique pair(w;)which solves (3.126) . Furthermore, walso minimizes (3.124) . Proof Note that a unique ( w;) exists whenever the KKT matrix is non-singular. Suppose this is not the case, then there exist non-zero vectors aandbsuch that G A> A 0a b = 0: SinceAa= 0 this implies that alies in the null space of Aand hence there exists ausuch thata=Zu. Therefore  Zu 0G A> A 0Zu 0 =u>Z>GZu = 0: Positive deniteness of Z>GZimplies that u= 0 and hence a= 0. On the other hand, the full row rank of AandA>b= 0 implies that b= 0. In summary, both aandbare zero, a contradiction. Letw6=wbe any other feasible point and  w=w w. SinceAw= Aw=bwe have that Aw= 0. Hence, there exists a non-zero usuch that w=Zu. The objective function J(w) can be written as J(w) =1 2(w w)>G(w w) + (w w)>d =J(w) +1 2w>Gw (Gw+d)>w: First note that1 2w>Gw=1 2u>Z>GZu > 0 by positive deniteness of the reduced Hessian. Second, since wsolves (3.126) it follows that ( Gw+

3.4 Stochastic Optimization 135 d)>w=>Aw= 0. Together these two observations imply that J(w)> J(w). If the technical conditions of the above theorem are met, then solving the equality constrained QP (3.124) is equivalent to solving the linear system (3.126). See [NW99] for a extensive discussion of algorithms that can be used for this task. Next we turn our attention to the general QP (3.123) which also contains inequality constraints. The Lagrangian in this case can be written as L(w;) =1 2w>Gw+w>d+X i2Ii(a> iw bi) +X i2Ei(a> iw bi):(3.127) Letwdenote the minimizer of (3.123). If we dene the active set A(w) as A(w) =n is.t.i2Ianda> iw=bio ; then the KKT conditions (3.114) for this problem can be written as a> iw bi<08i2InA(w) (3.128a) a> iw bi= 08i2E[A(w) (3.128b)  i08i2A(w) (3.128c) Gw+d+X i2A(w) iai+X i2Eiai= 0: (3.128d) Conceptually the main diculty in solving (3.123) is in identifying the active setA(w). This is because  i= 0 for all i2InA(w). Most algorithms for solving (3.123) can be viewed as dierent ways to identify the active set. See [NW99] for a detailed discussion. 3.4 Stochastic Optimization Recall that regularized risk minimization involves a data-driven optimization problem in which the objective function involves the summation of loss terms over a set of data to be modeled: min fJ(f) := (f) +1 mmX i=1l(f(xi);yi): Classical optimization techniques must compute this sum in its entirety for each evaluation of the objective, respectively its gradient. As available data sets grow ever larger, such \batch" optimizers therefore become increasingly inecient. They are also ill-suited for the incremental setting, where partial data must be modeled as it arrives.

136 3 Optimization Stochastic gradient-based methods, by contrast, work with gradient esti- mates obtained from small subsamples (mini-batches) of training data. This can greatly reduce computational requirements: on large, redundant data sets, simple stochastic gradient descent routinely outperforms sophisticated second-order batch methods by orders of magnitude. The key idea here is that J(w) is replaced by an instantaneous estimate Jtwhich is computed from a mini-batch of size kcomprising of a subset of points (xt i;yt i) withi= 1;:::;k drawn from the dataset: Jt(w) = (w) +1 kkX i=1l(w;xt i;yt i): (3.129) Settingk= 1 obtains an algorithm which processes data points as they arrive. 3.4.1 Stochastic Gradient Descent Perhaps the simplest stochastic optimization algorithm is Stochastic Gradi- ent Descent (SGD). The parameter update of SGD takes the form: wt+1=wt trJt(wt): (3.130) IfJtis not dierentiable, then one can choose an arbitrary subgradient from @Jt(wt) to compute the update. It has been shown that SGD asymptotically converges to the true minimizer of J(w) if the stepsize tdecays asO(1=p t). For instance, one could set t=r +t; (3.131) where >0 is a tuning parameter. See Algorithm 3.8 for details. 3.4.1.1 Practical Considerations One simple yet eective rule of thumb to tune is to select a small subset of data, try various values of on this subset, and choose the that most reduces the objective function. In some cases letting tto decay as O(1=t) has been found to be more eective: t= +t: (3.132) The free parameter  > 0 can be tuned as described above. If  ( w) is- strongly convex, then dividing the stepsize tbyyields good practical performance.

3.5 Nonconvex Optimization 137 Algorithm 3.8 Stochastic Gradient Descent 1:Input: Maximum iterations T, batch size k, and 2:Sett= 0 andw0= 0 3:whilet<T do 4: Choose a subset of kdata points ( xt i;yt i) and computerJt(wt) 5: Compute stepsize t=q  +t 6:wt+1=wt trJt(wt) 7:t=t+ 1 8:end while 9:Return:wT 3.5 Nonconvex Optimization Our focus in the previous sections was on convex objective functions. Some- times non-convex objective functions also arise in machine learning applica- tions. These problems are signicantly harder and tools for minimizing such objective functions are not as well developed. We briey describe one algo- rithm which can be applied whenever we can write the objective function as a dierence of two convex functions. 3.5.1 Concave-Convex Procedure Any function with a bounded Hessian can be decomposed into the dierence of two (non-unique) convex functions, that is, one can write J(w) =f(w) g(w); (3.133) wherefandgare convex functions. Clearly, Jis not convex, but there exists a reasonably simple algorithm namely the Concave-Convex Procedure (CCP) for nding a local minima of J. The basic idea is simple: In the tthiteration replace gby its rst order Taylor expansion at wt, that is, g(wt) +hw wt;rg(wt)iand minimize Jt(w) =f(w) g(wt) hw wt;rg(wt)i: (3.134) Taking gradients and setting it to zero shows that Jtis minimized by setting rf(wt+1) =rg(wt): (3.135) The iterations of CCP on a toy minimization problem is illustrated in Figure 3.13, while the complete algorithm listing can be found in Algorithm 3.9.

138 3 Optimization 1.0  1.5  2.0  2.5  3.0  3.5  4.0 80 70 60 50 40 30 20 10 1.0  1.5  2.0  2.5  3.0  3.5  4.0 50 0 50 100 150 200 Fig. 3.13. Given the function on the left we decompose it into the dierence of two convex functions depicted on the right panel. The CCP algorithm generates iterates by matching points on the two convex curves which have the same tangent vectors. As can be seen, the iterates approach the solution x= 2:0. Algorithm 3.9 Concave-Convex Procedure 1:Input: Initial point w0, maximum iterations T, convex functions f,g 2:Sett= 0 3:whilet<T do 4: Setwt+1= argminwf(w) g(wt) hw wt;rg(wt)i 5:t=t+ 1 6:end while 7:Return:wT Theorem 3.24 LetJbe a function which can be decomposed into a dier- ence of two convex functions e.g., (3.133) . The iterates generated by (3.135) monotically decrease J. Furthermore, the stationary point of the iterates is a local minima of J. Proof Sincefandgare convex f(wt)f(wt+1) +hwt wt+1;rf(wt+1)i g(wt+1)g(wt) +hwt+1 wt;rg(wt)i: Adding the two inequalities, rearranging, and using (3.135) shows that J(wt) = f(wt) g(wt)f(wt+1) g(wt+1) =J(wt+1), as claimed. Letwbe a stationary point of the iterates. Then rf(w) =rg(w), which in turn implies that wis a local minima of JbecauserJ(w) = 0. There are a number of extensions to CCP. We mention only a few in the passing. First, it can be shown that all instances of the EM algorithm (Sec- tion??) can be shown to be special cases of CCP. Second, the rate of con-

3.6 Some Practical Advice 139 vergence of CCP is related to the eigenvalues of the positive semi-denite matrixr2(f+g). Third, CCP can also be extended to solve constrained problems of the form: min wf0(w) g0(w) s.t.fi(w) gi(w)cifori= 1;:::;n: where, as before, fiandgifori= 0;1;:::;n are assumed convex. At every iteration, we replace giby its rst order Taylor approximation and solve the following constrained convex problem: min wf0(w) g0(wt) +hw wt;rg0(wt)i s.t.fi(w) gi(wt) +hw wt;rgi(wt)icifori= 1;:::;n: 3.6 Some Practical Advice The range of optimization algorithms we presented in this chapter might be somewhat intimidating for the beginner. Some simple rules of thumb can alleviate this anxiety Code Reuse: Implementing an ecient optimization algorithm correctly is both time consuming and error prone. Therefore, as far as possible use existing libraries. A number of high class optimization libraries both com- mercial and open source exist. Unconstrained Problems: For unconstrained minimization of a smooth convex function LBFGS (Section 3.2.6.1 is the algorithm of choice. In many practical situations the spectral gradient method (Section 3.2.6.2) is also very competitive. It also has the added advantage of being easy to imple- ment. If the function to be minimized is non-smooth then Bundle methods (Section 3.2.7) are to be preferred. Amongst the dierent formulations, the Bundle Trust algorithm tends to be quite robust. Constrained Problems: For constrained problems it is very important to understand the nature of the constraints. Simple equality ( Ax=b) and box (lxu) constraints are easier to handle than general non-linear constraints. If the objective function is smooth, the constraint set   is simple, and orthogonal projections P are easy to compute, then spectral projected gradient (Section 3.3.1) is the method of choice. If the optimization problem is a QP or an LP then specialized solvers tend to be much faster than general purpose solvers.

140 3 Optimization Large Scale Problems: If your parameter vector is high dimensional then consider coordinate descent (Section 3.2.2) especially if the one dimensional line search along a coordinate can be carried out eciently. If the objective function is made up of a summation of large number of terms, consider stochastic gradient descent (Section 3.4.1). Although both these algorithms do not guarantee a very accurate solution, practical experience shows that for large scale machine learning problems this is rarely necessary. Duality: Sometimes problems which are hard to optimize in the primal may become simpler in the dual. For instance, if the objective function is strongly convex but non-smooth, its Fenchel conjugate is smooth with a Lipschitz continuous gradient. Problems Problem 3.1 (Intersection of Convex Sets f1g)IfC1andC2are con- vex sets, then show that C1\C2is also convex. Extend your result to show thatTn i=1Ciare convex if Ciare convex. Problem 3.2 (Linear Transform of Convex Sets f1g)Given a set C Rnand a linear transform A2Rmn, deneAC:=fy=Ax:x2Cg. If Cis convex then show that ACis also convex. Problem 3.3 (Convex Combinations f1g)Show that a subset of Rnis convex if and only if it contains all the convex combination of its elements. Problem 3.4 (Convex Hull f2g)Show that the convex hull, conv(X)is the smallest convex set which contains X. Problem 3.5 (Epigraph of a Convex Function f2g)Show that a func- tion satises Denition 3.3 if, and only if, its epigraph is convex. Problem 3.6 Prove the Jensen's inequality (3.6) . Problem 3.7 (Strong convexity of the negative entropy f3g)Show that the negative entropy (3.15) is 1-strongly convex with respect to the kk1norm on the simplex. Hint: First show that (t) := (t 1) logt 2(t 1)2 t+10for allt0. Next substitute t=xi=yito show that X i(xi yi) logxi yikx yk2 1:

3.6 Some Practical Advice 141 Problem 3.8 (Strongly Convex Functions f2g)Prove 3.16, 3.17, 3.18 and 3.19. Problem 3.9 (Convex Functions with Lipschitz Continuous Gradient f2g) Prove 3.22, 3.23, 3.24 and 3.25. Problem 3.10 (One Dimensional Projection f1g)Iff:Rd!Ris convex, then show that for an arbitrary xandpinRdthe one dimensional function () :=f(x+p)is also convex. Problem 3.11 (Quasi-Convex Functions f2g)In Section 3.1 we showed that the below-sets of a convex function Xc:=fxjf(x)cgare convex. Give a counter-example to show that the converse is not true, that is, there exist non-convex functions whose below-sets are convex. This class of functions is called Quasi-Convex. Problem 3.12 (Gradient of the p-norm f1g)Show that the gradient of thep-norm (3.31) is given by (3.32) . Problem 3.13 Derive the Fenchel conjugate of the following functions f(x) =( 0 ifx2C 1 otherwise:whereCis a convex set f(x) =ax+b f(x) =1 2x>AxwhereAis a positive denite matrix f(x) = log(x) f(x) = exp(x) f(x) =xlog(x) Problem 3.14 (Convergence of gradient descent f2g)SupposeJhas a Lipschitz continuous gradient with modulus L. Then show that Algorithm 3.2 with an inexact line search satisfying the Wolfe conditions (3.42) and (3.43) will return a solution wtwithkrJ(wt)kin at mostO(1=2)iter- ations. Problem 3.15 Show that 1 +PT t=11 tPT t=11p t1p T

142 3 Optimization Problem 3.16 (Coordinate Descent for Quadratic Programming f2g) Derive a projection based method which uses coordinate descent to generate directions of descent for solving the following box constrained QP: min w2Rn1 2w>Qw+c>w s.t.lwu: You may assume that Qis positive denite and landuare scalars. Problem 3.17 (Dual of a LP f1g)Show that the dual of the LP (3.122) is(3.115) . In other words, we recover the primal by computing the dual of the dual.

4 Online Learning and Boosting So far the learning algorithms we considered assumed that all the training data is available before building a model for predicting labels on unseen data points. In many modern applications data is available only in a streaming fashion, and one needs to predict labels on the y. To describe a concrete example, consider the task of spam ltering. As emails arrive the learning algorithm needs to classify them as spam or ham. Tasks such as these are tackled via online learning. Online learning proceeds in rounds. At each round a training example is revealed to the learning algorithm, which uses its current model to predict the label. The true label is then revealed to the learner which incurs a loss and updates its model based on the feedback provided. This protocol is summarized in Algorithm 4.1. The goal of online learning is to minimize the total loss incurred. By an appropriate choice of labels and loss functions, this setting encompasses a large number of tasks such as classication, regression, and density estimation. In our spam detection example, if an email is misclassied the user can provide feedback which is used to update the spam lter, and the goal is to minimize the number of misclassied emails. 4.1 Halving Algorithm The halving algorithm is conceptually simple, yet it illustrates many of the concepts in online learning. Suppose we have access to a set of nexperts, that is, functions fiwhich map from the input space Xto the output space Y=f1g. Furthermore, assume that one of the experts is consistent, that is, there exists a j2f1;:::;ngsuch thatfj(xt) =ytfort= 1;:::;T . The halving algorithm maintains a set Ctof consistent experts at time t. Initially C0=f1;:::;ng, and it is updated recursively as Ct+1=fi2Cts.t.fi(xt+1) =yt+1g: (4.1) The prediction on a new data point is computed via a majority vote amongst the consistent experts: ^ yt= majority( Ct). Lemma 4.1 The Halving algorithm makes at most log2(n)mistakes. 143

144 4 Online Learning and Boosting Algorithm 4.1 Protocol of Online Learning 1:fort= 1;:::;T dodo 2: Get training instance xt 3: Predict label ^ yt 4: Get true label yt 5: Incur lossl(^yt;xt;yt) 6: Update model 7:end for Proof LetMdenote the total number of mistakes. The halving algorithm makes a mistake at iteration tif at least half the consistent experts Ctpredict the wrong label. This in turn implies that jCt+1jjCtj 2jC0j 2M=n 2M: On the other hand, since one of the experts is consistent it follows that 1jCt+1j. Therefore, 2Mn. Solving for Mcompletes the proof. 4.2 Weighted Majority We now turn to the scenario where none of the experts is consistent. There- fore, the aim here is not to minimize the number mistakes but to minimize regret. In this chapter we will consider online methods for solving the following optimization problem: min w2 J(w) whereJ(w) =TX t=1ft(w): (4.2) Suppose we have access to a function  which is continuously dierentiable and strongly convex with modulus of strong convexity  > 0 (see Section 3.1.4 for denition of strong convexity), then we can dene the Bregman divergence (3.29) corresponding to  as  (w;w0) = (w)  (w0)  w w0;r (w0) : We can also generalize the orthogonal projection (3.104) by replacing the square Euclidean norm with the above Bregman divergence: P ; (w0) = argmin w2  (w;w0): (4.3)

4.2 Weighted Majority 145 Algorithm 4.2 Stochastic (sub)gradient Descent 1:Input: Initial point x1, maximum iterations T 2:fort= 1;:::;T do 3: Compute ^wt+1=r (r (wt) tgt) withgt=@wft(wt) 4: Setwt+1=P ; ( ^wt+1) 5:end for 6:Return:wT+1 Denotew=P ; (w0). Just like the Euclidean distance is non-expansive, the Bregman projection can also be shown to be non-expansive in the following sense:  (w;w0) (w;w) +  (w;w0) (4.4) for allw2 . The diameter of   as measured by   is given by diam ( ) = max w;w02  (w;w0): (4.5) For the rest of this chapter we will make the following standard assumptions: Eachftis convex and revealed at time instance t.   is a closed convex subset of Rnwith non-empty interior. The diameter diam  ( ) of   is bounded by F <1. The set of optimal solutions of (4.2) denoted by  is non-empty. The subgradient @wft(w) can be computed for every tandw2 . The Bregman projection (4.3) can be computed for every w02Rn. The gradientr , and its inverse ( r ) 1=r can be computed. The method we employ to solve (4.2) is given in Algorithm 4.2. Before analyzing the performance of the algorithm we would like to discuss three special cases. First, Euclidean distance squared which recovers projected stochastic gradient descent, second Entropy which recovers Exponentiated gradient descent, and third the p-norms forp>2 which recovers the p-norm Perceptron. BUGBUG TODO. Our key result is Lemma 4.3 given below. It can be found in various guises in dierent places most notably Lemma 2.1 and 2.2 in [ ?], Theorem 4.1 and Eq. (4.21) and (4.15) in [ ?], in the proof of Theorem 1 of [ ?], as well as Lemma 3 of [?]. We prove a slightly general variant; we allow for projections with an arbitrary Bregman divergence and also take into account a generalized version of strong convexity of ft. Both these modications will allow us to deal with general settings within a unied framework.

146 4 Online Learning and Boosting Denition 4.2 We say that a convex function fis strongly convex with respect to another convex function  with modulus if f(w) f(w0)  w w0;  (w;w0)for all2@f(w0): (4.6) The usual notion of strong convexity is recovered by setting  () =1 2kk2. Lemma 4.3 Letftbe strongly convex with respect to  with modulus 0 for allt. For anyw2 the sequences generated by Algorithm 4.2 satisfy  (w;wt+1) (w;wt) thgt;wt wi+2 t 2kgtk2(4.7) (1 t) (w;wt) t(ft(wt) ft(w)) +2 t 2kgtk2:(4.8) Proof We prove the result in three steps. First we upper bound   (w;wt+1) by  (w;^wt+1). This is a consequence of (4.4) and the non-negativity of the Bregman divergence which allows us to write  (w;wt+1) (w;^wt+1): (4.9) In the next step we use Lemma 3.11 to write  (w;wt) +  (wt;^wt+1)  (w;^wt+1) =hr ( ^wt+1) r (wt);w wti: Sincer = (r ) 1, the update in step 3 of Algorithm 4.2 can equivalently be written asr ( ^wt+1) r (wt) = tgt. Plugging this in the above equation and rearranging  (w;^wt+1) =  (w;wt) thgt;wt wi+  (wt;^wt+1): (4.10) Finally we upper bound   (wt;^wt+1). For this we need two observations: First,hx;yi1 2kxk2+ 2kyk2for allx;y2Rnand >0. Second, the  strong convexity of  allows us to bound   ( ^wt+1;wt) 2kwt ^wt+1k2. Using these two observations  (wt;^wt+1) = (wt)  ( ^wt+1) hr ( ^wt+1);wt ^wt+1i = ( ( ^wt+1)  (wt) hr (wt);^wt+1 wti) +htgt;wt ^wt+1i =  ( ^wt+1;wt) +htgt;wt ^wt+1i   2kwt ^wt+1k2+2 t 2kgtk2+ 2kwt ^wt+1k2 =2 t 2kgtk2: (4.11) Inequality (4.7) follows by putting together (4.9), (4.10), and (4.11), while (4.8) follows by using (4.6) with f=ftandw0=wtand substituting into

4.2 Weighted Majority 147 (4.7). Now we are ready to prove regret bounds. Lemma 4.4 Letw2 denote the best parameter chosen in hindsight, and letkgtkLfor allt. Then the regret of Algorithm 4.2 can be bounded via TX t=1ft(wt) ft(w)F1 T T +L2 2TX t=1t: (4.12) Proof Setw=wand rearrange (4.8) to obtain ft(wt) ft(w)1 t((1 t) (w;wt)  (w;wt+1)) +t 2kgtk2: Summing over t TX t=1ft(wt) ft(w)TX t=11 t((1 t) (w;wt)  (w;wt+1)) | {z } T1+TX t=1t 2kgtk2 |{z} T2: Since the diameter of   is bounded by Fand  is non-negative T1=1 1   (w;w1) 1 T (w;wT+1) +TX t=2 (w;wt)1 t 1 t 1  1 1   (w;w1) +TX t=2 (w;wt)1 t 1 t 1  1 1  F+TX t=2F1 t 1 t 1  =F1 T T : On the other hand, since the subgradients are Lipschitz continuous with constantLit follows that T2L2 2TX t=1t: Putting together the bounds for T1andT2yields (4.12). Corollary 4.5 If>0and we set t=1 tthen TX t=1ft(xt) ft(x)L2 2(1 + log(T));

148 4 Online Learning and Boosting On the other hand, when = 0, if we sett=1p tthen TX t=1ft(xt) ft(x) F+L2 p T: Proof First consider  > 0 witht=1 t. In this case1 T=T, and consequently (4.12) specializes to TX t=1ft(wt) ft(w)L2 2TX t=11 tL2 2(1 + log(T)): When= 0, and we set t=1p tand use problem 4.2 to rewrite (4.12) as TX t=1ft(wt) ft(w)Fp T+L2 TX t=11 2p tFp T+L2 p T: Problems Problem 4.1 (Generalized Cauchy-Schwartz f1g)Show thathx;yi 1 2kxk2+ 2kyk2for allx;y2Rnand>0. Problem 4.2 (Bounding sum of a series f1g)Show thatPb t=a1 2p tp b a+ 1.Hint: Upper bound the sum by an integral.

5 Conditional Densities A number of machine learning algorithms can be derived by using condi- tional exponential families of distribution (Section 2.3). Assume that the training setf(x1;y1);:::; (xm;ym)gwas drawn iid from some underlying distribution. Using Bayes rule (1.15) one can write the likelihood p(jX;Y )/p()p(YjX;) =p()mY i=1p(yijxi;); (5.1) and hence the negative log-likelihood  logp(jX;Y ) = mX i=1logp(yijxi;) logp() + const. (5.2) Because we do not have any prior knowledge about the data, we choose a zero mean unit variance isotropic normal distribution for p(). This yields  logp(jX;Y ) =1 2kk2 mX i=1logp(yijxi;) + const. (5.3) Finally, if we assume a conditional exponential family model for p(yjx;), that is, p(yjx;) = exp (h(x;y);i g(jx)); (5.4) then  logp(jX;Y ) =1 2kk2+mX i=1g(jxi) h(xi;yi);i+ const. (5.5) where g(jx) = logX y2Yexp (h(x;y);i); (5.6) is the log-partition function. Clearly, (5.5) is a smooth convex objective function, and algorithms for unconstrained minimization from Chapter 3 149

150 5 Conditional Densities can be used to obtain the maximum aposteriori (MAP) estimate for . Given the optimal , the class label at any given xcan be predicted using y= argmax yp(yjx;): (5.7) In this chapter we will discuss a number of these algorithms that can be derived by specializing the above setup. Our discussion unies seemingly disparate algorithms, which are often discussed separately in literature. 5.1 Logistic Regression We begin with the simplest case namely binary classication1. The key ob- servation here is that the labels y2f 1gand hence g(jx) = log (exp (h(x;+1);i) + exp (h(x; 1);i)): (5.8) Dene ^(x) :=(x;+1) (x; 1). Plugging (5.8) into (5.4), using the denition of ^and rearranging p(y= +1jx;) =1 1 + expD  ^(x);Eand p(y= 1jx;) =1 1 + expD ^(x);E; or more compactly p(yjx;) =1 1 + expD  y^(x);E: (5.9) Sincep(yjx;) is a logistic function, hence the name logistic regression. The classication rule (5.7) in this case specializes as follows: predict +1 when- everp(y= +1jx;)p(y= 1jx;) otherwise predict  1. However logp(y= +1jx;) p(y= 1jx;)=D ^(x);E ; therefore one can equivalently use signD ^(x);E as our prediction func- tion. Using (5.9) we can write the objective function of logistic regression as 1 2kk2+mX i=1log 1 + expD  yi^(xi);E 1The name logistic regression is a misnomer!

5.2 Regression 151 To minimize the above objective function we rst compute the gradient. rJ() =+mX i=1expD  yi^(xi);E 1 + expD  yi^(xi);E( yi^(xi)) =+mX i=1(p(yijxi;) 1)yi^(xi): Notice that the second term of the gradient vanishes whenever p(yijxi;) = 1. Therefore, one way to interpret logistic regression is to view it as a method to maximize p(yijxi;) for each point ( xi;yi) in the training set. Since the objective function of logistic regression is twice dierentiable one can also compute its Hessian r2J() =I mX i=1p(yijxi;)(1 p(yijxi;))^(xi)^(xi)>; where we used y2 i= 1. The Hessian can be used in the Newton method (Section 3.2.6) to obtain the optimal parameter . 5.2 Regression 5.2.1 Conditionally Normal Models xed variance 5.2.2 Posterior Distribution integrating out vs. Laplace approximation, ecient estimation (sparse greedy) 5.2.3 Heteroscedastic Estimation explain that we have two parameters. not too many details (do that as an assignment). 5.3 Multiclass Classication 5.3.1 Conditionally Multinomial Models joint feature map

152 5 Conditional Densities 5.4 What is a CRF? Motivation with learning a digit example general denition Gaussian process + structure = CRF 5.4.1 Linear Chain CRFs Graphical model Applications Optimization problem 5.4.2 Higher Order CRFs 2-d CRFs and their applications in vision Skip chain CRFs Hierarchical CRFs (graph transducers, sutton et. al. JMLR etc) 5.4.3 Kernelized CRFs From feature maps to kernels The clique decomposition theorem The representer theorem Optimization strategies for kernelized CRFs 5.5 Optimization Strategies 5.5.1 Getting Started three things needed to optimize {MAP estimate {log-partition function {gradient of log-partition function Worked out example (linear chain?) 5.5.2 Optimization Algorithms - Optimization algorithms (LBFGS, SGD, EG (Globerson et. al)) 5.5.3 Handling Higher order CRFs - How things can be done for higher order CRFs (briey)

5.6 Hidden Markov Models 153 5.6 Hidden Markov Models Denition Discuss that they are modeling joint distribution p(x;y) The way they predict is by marginalizing out x Why they are wasteful and why CRFs generally outperform them 5.7 Further Reading What we did not talk about: Details of HMM optimization CRFs applied to predicting parse trees via matrix tree theorem (collins, koo et al) CRFs for graph matching problems CRFs with Gaussian distributions (yes they exist) 5.7.1 Optimization issues in optimization (blows up with number of classes). structure is not there. can we do better? Problems Problem 5.1 Poisson models Problem 5.2 Bayes Committee Machine Problem 5.3 Newton / CG approach

6 Kernels and Function Spaces Kernels are measures of similarity. Broadly speaking, machine learning al- gorithms which rely only on the dot product between instances can be \ker- nelized" by replacing all instances of hx;x0iby a kernel function k(x;x0). We saw examples of such algorithms in Sections 1.3.3 and 1.3.4 and we will see many more examples in Chapter 7. Arguably, the design of a good ker- nel underlies the success of machine learning in many applications. In this chapter we will lay the ground for the theoretical properties of kernels and present a number of examples. Algorithms which use these kernels can be found in later chapters. 6.1 The Basics LetXdenote the space of inputs and k:XX!Rbe a function which satises k(x;x0) =h(x);(x)i (6.1) where  is a feature map which maps Xinto some dot product space H. In other words, kernels correspond to dot products in some dot product space. The main advantage of using a kernel as a similarity measure are threefold: First, if the feature space is rich enough, then simple estimators such as hyperplanes and half-spaces may be sucient. For instance, to classify the points in Figure BUGBUG, we need a nonlinear decision boundary, but once we map the points to a 3 dimensional space a hyperplane suces. Second, kernels allow us to construct machine learning algorithms in the dot product space Hwithout explicitly computing ( x). Third, we need not make any assumptions about the input space Xother than for it to be a set. As we will see later in this chapter, this allows us to compute similarity between discrete objects such as strings, trees, and graphs. In the rst half of this chapter we will present some examples of kernels, and discuss some theoretical properties of kernels in the second half. 155

156 6 Kernels and Function Spaces 6.1.1 Examples 6.1.1.1 Linear Kernel Linear kernels are perhaps the simplest of all kernels. We assume that x2Rn and dene k(x;x0) = x;x0 =X ixix0 i: Ifxandx0are dense then computing the kernel takes O(n) time. On the other hand, for sparse vectors this can be reduced to O(jnnz(x)\nnz(x0)j), wherennz() denotes the set of non-zero indices of a vector and jjde- notes the size of a set. Linear kernels are a natural representation to use for vectorial data. They are also widely used in text mining where documents are represented by a vector containing the frequency of occurrence of words (Recall that we encountered this so-called bag of words representation in Chapter 1). Instead of a simple bag of words, one can also map a text to the set of pairs of words that co-occur in a sentence for a richer representation. 6.1.1.2 Polynomial Kernel Givenx2Rn, we can compute a feature map  by taking all the d-th order products (also called the monomials) of the entries of x. To illustrate with a concrete example, let us consider x= (x1;x2) andd= 2, in which case (x) =  x2 1;x2 2;x1x2;x2x1 . Although it is tedious to compute ( x) and (x0) explicitly in order to compute k(x;x), there is a shortcut as the following proposition shows. Proposition 6.1 Let(x)(resp. (x0)) denote the vector whose entries are all possible d-th degree ordered products of the entries of x(resp.x0). Then k(x;x0) = (x);(x0) =  x;x0d: (6.2) Proof By direct computation

(x);(x0) =X j1:::X jdxj1:::xjdx0 j1:::x0 jd =X j1xj1x0 j1:::X jdxjdx0 jd=0 @X jxjx0 j1 Ad =  x;x0d

6.1 The Basics 157 The kernel (6.2) is called the polynomial kernel. An useful extension is the inhomogeneous polynomial kernel k(x;x0) =  x;x0 +cd; (6.3) which computes all monomials up to degree d(problem 6.2). 6.1.1.3 Radial Basis Function Kernels 6.1.1.4 Convolution Kernels The framework of convolution kernels is a general way to extend the notion of kernels to structured objects such as strings, trees, and graphs. Let x2X be a discrete object which can be decomposed into Ppartsxp2Xpin many dierent ways. As a concrete example consider the string x=abcwhich can be split into two sets of substrings of size two namely fa;bcgandfab;cg. We denote the set of all such decompositions as R(x), and use it to build a kernel on Xas follows: [k1?:::?kP] (x;x0) =X x2R(x);x02R(x0)PY p=1kp(xp;x0 p): (6.4) Here, the sum is over all possible ways in which we can decompose xand x0into x1;:::; xPand x0 1;:::; x0 Prespectively. If the cardinality of R(x) is nite, then it can be shown that (6.4) results in a valid kernel. Although convolution kernels provide the abstract framework, specic instantiations of this idea lead to a rich set of kernels on discrete objects. We will now discuss some of them in detail. 6.1.1.5 String Kernels The basic idea behind string kernels is simple: Compare the strings by means of the subsequences they contain. More the number of common sub- sequences, the more similar two strings are. The subsequences need not have equal weights. For instance, the weight of a subsequence may be given by the inverse frequency of its occurrence. Similarly, if the rst and last characters of a subsequence are rather far apart, then its contribution to the kernel must be down-weighted. Formally, a string xis composed of characters from a nite alphabet  andjxjdenotes its length. We say that sis a subsequence of x=x1x2:::xjxj ifs=xi1xi2:::xijsjfor some 1i1<i2<:::<ijsjjxj. In particular, if ii+1=ii+1 thensis a substring of x. For example, acbis not a subsequence ofadbc whileabcis a subsequence and adcis a substring. Assume that there exists a function #( x;s) which returns the number of times a subsequence

158 6 Kernels and Function Spaces soccurs inxand a non-negative weighting function w(s)0 which returns the weight associated with s. Then the basic string kernel can be written as k(x;x0) =X s#(x;s) #(x0;s)w(s): (6.5) Dierent string kernels are derived by specializing the above equation: All substrings kernel: If we restrict the summation in (6.5) to sub- strings then [VS04] provide a sux tree based algorithm which allows one to compute for arbitrary w(s) the kernel k(x;x0) inO(jxj+jx0j) time and memory. k-Spectrum kernel: Thek-spectrum kernel is obtained by restricting the summation in (6.5) to substrings of length k. A slightly general variant considers all substrings of length up to k. Herekis a tuning parameter which is typically set to be a small number ( e.g., 5). A simple trie based algorithm can be used to compute the k-spectrum kernel in O((jxj+jx0j)k) time (problem 6.3). Inexact substring kernel: Sometimes the input strings might have measurement errors and therefore it is desirable to take into account inexact matches. This is done by replacing #( x;s) in (6.5) by another function #(x;s; ) which reports the number of approximate matches of sinx. Here denotes the number of mismatches allowed, typically a small number ( e.g., 3). By trading o computational complexity with storage the kernel can be computed eciently. See [LK03] for details. Mismatch kernel: Instead of simply counting the number of occurrences of a substring if we use a weighting scheme which down-weights the contribu- tions of longer subsequences then this yields the so-called mismatch kernel. Given an index sequence I= (i1;:::;ik) with 1i1< i2< ::: < i kjxj we can associate the subsequence x(I) =xi1xi2:::xikwithI. Furthermore, denejIj=ik i1+ 1. Clearly,jIj>kifIis not contiguous. Let 1 be a decay factor. Redene #(x;s) =X s=x(I)jIj; (6.6) that is, we count all occurrences of sinxbut now the weight associated with a subsequence depends on its length. To illustrate, consider the subsequence abcwhich occurs in the string abcebc twice, namely, abcebcandabcebc . The rst occurrence is counted with weight 3while the second occurrence is counted with the weight 6. As it turns out, this kernel can be computed by a dynamic programming algorithm (problem BUGBUG) in O(jxjjx0j) time.

6.1 The Basics 159 6.1.1.6 Graph Kernels There are two dierent notions of graph kernels. First, kernels ongraphs are used to compare nodes of a single graph. In contrast, kernels between graphs focus on comparing two graphs. A random walk (or its continuous time limit, diusion) underlie both types of kernels. The basic intuition is that two nodes are similar if there are a number of paths which connect them while two graphs are similar if they share many common paths. To describe these kernels formally we need to introduce some notation. A graphGconsists of an ordered set of nverticesV=fv1;v2;:::;vng, and a set of directed edges EVV. A vertexviis said to be a neighbor of another vertex vjif they are connected by an edge, i.e., if (vi;vj)2E; this is also denoted vivj. The adjacency matrix of a graph is the nn matrixAwithAij= 1 ifvivj, and 0 otherwise. A walk of length konG is a sequence of indices i0;i1;:::iksuch thatvir 1virfor all 1rk. The adjacency matrix has a normalized cousin, dened ~A:=D 1A, which has the property that each of its rows sums to one, and it can therefore serve as the transition matrix for a stochastic process. Here, Dis a diag- onal matrix of node degrees, i.e.,Dii=di=P jAij. A random walk on Gis a process generating sequences of vertices vi1;vi2;vi3;:::according to P(ik+1ji1;:::ik) =~Aik;ik+1. Thetthpower of ~Athus describes t-length walks, i.e.,(~At)ijis the probability of a transition from vertex vjto vertexvivia a walk of length t(problem BUGBUG). If p0is an initial probability dis- tribution over vertices, then the probability distribution ptdescribing the location of our random walker at time tispt=~Atp0. Thejthcomponent of ptdenotes the probability of nishing a t-length walk at vertex vj. A random walk need not continue indenitely; to model this, we associate every node vikin the graph with a stopping probability qik. The overall probability of stopping after tsteps is given by q>pt. Given two graphs G(V;E) andG0(V0;E0), their direct product Gis a graph with vertex set V=f(vi;v0 r) :vi2V; v0 r2V0g; (6.7) and edge set E=f((vi;v0 r);(vj;v0 s)) : (vi;vj)2E^(v0 r;v0 s)2E0g: (6.8) In other words, Gis a graph over pairs of vertices from GandG0, and two vertices in Gare neighbors if and only if the corresponding vertices inGandG0are both neighbors; see Figure 6.1 for an illustration. If Aand A0are the respective adjacency matrices of GandG0, then the adjacency

160 6 Kernels and Function Spaces G112 3 G21' 2' 3' 4' G11' 21' 31' 34' 12' 24' 22' 14' 32' 33' 23' 13' Fig. 6.1. Two graphs ( G1&G2) and their direct product ( G). Each node of the direct product graph is labeled with a pair of nodes (6.7); an edge exists in the direct product if and only if the corresponding nodes are adjacent in both original graphs (6.8). For instance, nodes 110and 320are adjacent because there is an edge between nodes 1 and 3 in the rst, and 10and 20in the second graph. matrix ofGisA=A A0. Similarly, ~A=~A ~A0. Performing a random walk on the direct product graph is equivalent to performing a simultaneous random walk on GandG0. Ifpandp0denote initial probability distributions over the vertices of GandG0, then the corresponding initial probability distribution on the direct product graph is p:=p p0. Likewise, if qand q0are stopping probabilities (that is, the probability that a random walk ends at a given vertex), then the stopping probability on the direct product graph isq:=q q0. To dene a kernel which computes the similarity between GandG0, one natural idea is to simply sum up q> ~At pfor all values of t. However, this sum might not converge, leaving the kernel value undened. To overcome this problem, we introduce appropriately chosen non-negative coecients (t), and dene the kernel between GandG0as k(G;G0) :=1X t=0(t)q> ~At p: (6.9) This idea can be extended to graphs whose nodes are associated with labels by replacing the matrix ~Awith a matrix of label similarities. For appro- priate choices of (t) the above sum converges and ecient algorithms for computing the kernel can be devised. See [ ?] for details. As it turns out, the simple idea of performing a random walk on the prod-

6.2 Kernels 161 uct graph can be extended to compute kernels on Auto Regressive Moving Average (ARMA) models [VSV07]. Similarly, it can also be used to dene kernels between transducers. Connections between the so-called rational ker- nels on transducers and the graph kernels dened via (6.9) are made explicit in [?]. 6.2 Kernels 6.2.1 Feature Maps give examples, linear classier, nonlinear ones with r2-r3 map 6.2.2 The Kernel Trick 6.2.3 Examples of Kernels gaussian, polynomial, linear, texts, graphs - stress the fact that there is a dierence between structure in the input space and structure in the output space 6.3 Algorithms 6.3.1 Kernel Perceptron 6.3.2 Trivial Classier 6.3.3 Kernel Principal Component Analysis 6.4 Reproducing Kernel Hilbert Spaces As it turns out, this class of functions coincides with the class of positive semi-denite functions. Intuitively, the notion of a positive semi-denite function is an extension of the familiar notion of a positive semi-denite matrix (also see Appendix BUGBUG): Denition 6.2 A realnnsymmetric matrix Ksatisfying X i;jijKi;j0 (6.10) for alli;j2Ris called positive semi-denite. If equality in (6.10) occurs only when1;:::;n= 0, thenKis said to be positive denite. Denition 6.3 Given a set of points x1;:::;xn2Xand a function k, the matrix Ki;j=k(xi;xj) (6.11)

162 6 Kernels and Function Spaces is called the Gram matrix or the kernel matrix of kwith respect to x1;:::;xn. Denition 6.4 LetXbe a nonempty set, k:XX!Rbe a function. If kgives rise to a positive (semi-)denite Gram matrix for all x1;:::;xn2X andn2Nthenkis said to be positive (semi-)denite. Clearly, every kernel function kof the form (6.1) is positive semi-denite. To see this simply write X i;jijk(xi;xj) =X i;jijhxi;xji=*X iixi;X jjxj+ 0: We now establish the converse, that is, we show that every positive semi- denite kernel function can be written as (6.1). Towards this end, dene a map  from Xinto the space of functions mapping XtoR(denoted RX) via (x) =k(;x). In other words, ( x) :X!Ris a function which assigns the valuek(x0;x) tox02X. Next construct a vector space by taking all possible linear combinations of ( x) f() =nX i=1i(xi) =nX i=1ik(;xi); (6.12) wherei2N,i2R, andxi2Xare arbitrary. This space can be endowed with a natural dot product hf;gi=nX i=1n0X j=1ijk(xi;x0 j): (6.13) To see that the above dot product is well dened even though it contains the expansion coecients (which need not be unique), note that hf;gi=Pn0 j=1jf(x0 j), independent of i. Similarly, for g, note thathf;gi=Pn i=1if(xi), this time independent of j. This also shows that hf;giis bilinear. Symme- try follows because hf;gi=hg;fi, while the positive semi-deniteness of k implies that hf;fi=X i;jijk(xi;xj)0: (6.14) Applying (6.13) shows that for all functions (6.12) we have hf;k(;x)i=f(x): (6.15) In particular

k(;x);k(;x0) =k(x;x0): (6.16)

6.4 Reproducing Kernel Hilbert Spaces 163 In view of these properties, kis called a reproducing kernel. By using (6.15) and the following property of positive semi-denite functions (problem 6.1) k(x;x0)2k(x;x)k(x0;x0) (6.17) we can now write jf(x)j2=jhf;k(;x)ijk(x;x)hf;fi: (6.18) From the above inequality, f= 0 wheneverhf;fi= 0, thus establishing h;ias a valid dot product. In fact, one can complete the space of functions (6.12) in the norm corresponding to the dot product (6.13), and thus get a Hilbert space H, called the reproducing kernel Hilbert Space (RKHS) . An alternate way to dene a RKHS is as a Hilbert space Hon functions from some input space XtoRwith the property that for any f2Hand x2X, the point evaluations f!f(x) are continuous (in particular, all points values f(x) are well dened, which already distinguishes an RKHS from many L2Hilbert spaces). Given the point evaluation functional, one can then construct the reproducing kernel using the Riesz representation theorem. The Moore-Aronszajn theorem states that, for every positive semi- denite kernel on XX, there exists a unique RKHS and vice versa. We nish this section by noting that h;iis a positive semi-denite func- tion in the vector space of functions (6.12). This follows directly from the bilinearity of the dot product and (6.14) by which we can write for functions f1;:::;fpand coecients 1;:::;p X iX jijhfi;fji=*X iifi;X jjfj+ 0: (6.19) 6.4.1 Hilbert Spaces evaluation functionals, inner products 6.4.2 Theoretical Properties Mercer's theorem, positive semideniteness 6.4.3 Regularization Representer theorem, regularization

164 6 Kernels and Function Spaces 6.5 Banach Spaces 6.5.1 Properties 6.5.2 Norms and Convex Sets - smoothest function (L2) - smallest coecients (L1) - structured priors (CAP formalism) Problems Problem 6.1 Show that (6.17) holds for an arbitrary positive semi-denite functionk. Problem 6.2 Show that the inhomogeneous polynomial kernel (6.3) is a valid kernel and that it computes all monomials of degree up to d. Problem 6.3 ( k-spectrum kernel f2g)Given two strings xandx0show how one can compute the k-spectrum kernel (section 6.1.1.5) in O((jxj+ jx0j)k)time. Hint: You need to use a trie.

7 Linear Models A hyperplane in a space Hendowed with a dot product h;iis described by the set fx2Hjhw;xi+b= 0g (7.1) wherew2Handb2R. Such a hyperplane naturally divides Hinto two half-spaces:fx2Hjhw;xi+b0gandfx2Hjhw;xi+b < 0g, and hence can be used as the decision boundary of a binary classier. In this chapter we will study a number of algorithms which employ such linear decision boundaries. Although such models look restrictive at rst glance, when combined with kernels (Chapter 6) they yield a large class of useful algorithms. All the algorithms we will study in this chapter maximize the margin. Given a set X=fx1;:::;xmg, the margin is the distance of the closest point inXto the hyperplane (7.1). Elementary geometric arguments (Problem 7.1) show that the distance of a point xito a hyperplane is given by jhw;xii+ bj=kwk, and hence the margin is simply min i=1;:::;mjhw;xii+bj kwk: (7.2) Note that the parameterization of the hyperplane (7.1) is not unique; if we multiply both wandbby the same non-zero constant, then we obtain the same hyperplane. One way to resolve this ambiguity is to set min i=1;:::mjhw;xii+bj= 1: In this case, the margin simply becomes 1 =kwk. We postpone justication of margin maximization for later and jump straight ahead to the description of various algorithms. 7.1 Support Vector Classication Consider a binary classication task, where we are given a training set f(x1;y1);:::; (xm;ym)gwithxi2Handyi2f 1g. Our aim is to nd a linear decision boundary parameterized by ( w;b) such thathw;xii+b0 165

166 7 Linear Models x1 wx2 yi= 1yi= +1fxjhw;xi+b= 1gfxjhw;xi+b= 1g fxjhw;xi+b= 0ghw;x 1i+b= +1 hw;x 2i+b= 1 hw;x 1 x2i= 2D w kwk;x1 x2E =2 kwk Fig. 7.1. A linearly separable toy binary classication problem of separating the diamonds from the circles. We normalize ( w;b) to ensure that min i=1;:::mjhw;xii+ bj= 1. In this case, the margin is given by1 kwkas the calculation in the inset shows. wheneveryi= +1 andhw;xii+b<0 whenever yi= 1. Furthermore, as dis- cussed above, we x the scaling of wby requiring min i=1;:::mjhw;xii+bj= 1. A compact way to write our desiderata is to require yi(hw;xii+b)1 for alli(also see Figure 7.1). The problem of maximizing the margin therefore reduces to max w;b1 kwk(7.3a) s.t.yi(hw;xii+b)1 for alli; (7.3b) or equivalently min w;b1 2kwk2(7.4a) s.t.yi(hw;xii+b)1 for alli: (7.4b) This is a constrained convex optimization problem with a quadratic objec- tive function and linear constraints (see Section 3.3). In deriving (7.4) we implicitly assumed that the data is linearly separable, that is, there is a hyperplane which correctly classies the training data. Such a classier is called a hard margin classier . If the data is not linearly separable, then (7.4) does not have a solution. To deal with this situation we introduce

7.1 Support Vector Classication 167 non-negative slack variables ito relax the constraints: yi(hw;xii+b)1 i: Given anywandbthe constraints can now be satised by making ilarge enough. This renders the whole optimization problem useless. Therefore, one has to penalize large i. This is done via the following modied optimization problem: min w;b;1 2kwk2+C mmX i=1i (7.5a) s.t.yi(hw;xii+b)1 ifor alli (7.5b) i0; (7.5c) whereC > 0 is a penalty parameter. The resultant classier is said to be a soft margin classier . By introducing non-negative Lagrange multipliers i andione can write the Lagrangian (see Section 3.3) L(w;b;;; ) =1 2kwk2+C mmX i=1i+mX i=1i(1 i yi(hw;xii+b)) mX i=1ii: Next take gradients with respect to w,bandand set them to zero. rwL=w mX i=1iyixi= 0 (7.6a) rbL= mX i=1iyi= 0 (7.6b) riL=C m i i= 0: (7.6c) Substituting (7.6) into the Lagrangian and simplifying yields the dual ob- jective function:  1 2X i;jyiyjijhxi;xji+mX i=1i; (7.7) which needs to be maximized with respect to . For notational convenience we will minimize the negative of (7.7) below. Next we turn our attention to the dual constraints. Recall that i0 andi0, which in conjunc- tion with (7.6c) immediately yields 0 iC m. Furthermore, by (7.6b)Pm i=1iyi= 0. Putting everything together, the dual optimization problem

168 7 Linear Models boils down to min 1 2X i;jyiyjijhxi;xji mX i=1i (7.8a) s.t.mX i=1iyi= 0 (7.8b) 0iC m: (7.8c) If we letHbe ammmatrix with entries Hij=yiyjhxi;xji, whilee,, andybem-dimensional vectors whose i-th components are one, i, andyi respectively, then the above dual can be compactly written as the following Quadratic Program (QP) (Section 3.3.3): min 1 2>H >e (7.9a) s.t.>y= 0 (7.9b) 0iC m: (7.9c) Before turning our attention to algorithms for solving (7.9), a number of observations are in order. First, note that computing Honly requires com- puting dot products between training examples. If we map the input data to a Reproducing Kernel Hilbert Space (RKHS) via a feature map , then we can still compute the entries of Hand solve for the optimal . In this case, Hij=yiyjh(xi);(xj)i=yiyjk(xi;xj), wherekis the kernel associated with the RKHS. Given the optimal , one can easily recover the decision boundary. This is a direct consequence of (7.6a), which allows us to write w as a linear combination of the training data: w=mX i=1iyi(xi); and hence the decision boundary as hw;xi+b=mX i=1iyik(xi;x) +b: (7.10) By the KKT conditions (Section 3.3) we have i(1 i yi(hw;xii+b)) = 0 and ii= 0: We now consider three cases for yi(hw;xii+b) and the implications of the KKT conditions (see Figure 7.2).

7.1 Support Vector Classication 169 fxjhw;xi+b= 1gfxjhw;xi+b= 1g Fig. 7.2. The picture depicts the well classied points ( yi(hw;xii+b)>1 in black, the support vectors yi(hw;xii+b) = 1 in blue, and margin errors yi(hw;xii+b)<1 in red. yi(hw;xii+b)<1:In this case, i>0, and hence the KKT conditions imply that i= 0. Consequently, i=C m(see (7.6c)). Such points are said to be margin errors. yi(hw;xii+b)>1:In this case, i= 0, (1 i yi(hw;xii+b))<0, and by the KKT conditions i= 0. Such points are said to be well classied. It is easy to see that the decision boundary (7.10) does not change even if these points are removed from the training set. yi(hw;xii+b) =1:In this case i= 0 andi0. Sinceiis non-negative and satises (7.6c) it follows that 0 iC m. Such points are said to be on the margin. They are also sometimes called support vectors . Since the support vectors satisfy yi(hw;xii+b) = 1 andyi2f 1git follows thatb=yi hw;xiifor any support vector xi. However, in practice to recoverbwe average b=yi X ihw;xii: (7.11) over all support vectors, that is, points xifor which 0 < i<C m. Because it uses support vectors, the overall algorithm is called C-Support Vector classier or C-SV classier for short.

170 7 Linear Models 7.1.1 A Regularized Risk Minimization Viewpoint A closer examination of (7.5) reveals that i= 0 whenever yi(hw;xii+b)>1. On the other hand, i= 1 yi(hw;xii+b) whenever yi(hw;xii+b)< 1. In short, i= max(0;1 yi(hw;xii+b)). Using this observation one can eliminate ifrom (7.5), and write it as the following unconstrained optimization problem: min w;b1 2kwk2+C mmX i=1max(0;1 yi(hw;xii+b)): (7.12) Writing (7.5) as (7.12) is particularly revealing because it shows that a support vector classier is nothing but a regularized risk minimizer. Here the regularizer is the square norm of the decision hyperplane1 2kwk2, and the loss function is the so-called binary hinge loss (Figure 7.3): l(w;x;y ) = max(0;1 y(hw;xi+b)): (7.13) It is easy to verify that the binary hinge loss (7.13) is convex but non- dierentiable (see Figure 7.3) which renders the overall objective function (7.12) to be convex but non-smooth. There are two dierent strategies to minimize such an objective function. If minimizing (7.12) in the primal, one can employ non-smooth convex optimizers such as bundle methods (Section 3.2.7). This yields a ddimensional problem where dis the dimension of x. On the other hand, since (7.12) is strongly convex because of the presence of the1 2kwk2term, its Fenchel dual has a Lipschitz continuous gradient (see Lemma 3.10). The dual problem is mdimensional and contains linear constraints. This strategy is particularly attractive when the kernel trick is used or whenever dm. In fact, the dual problem obtained via Fenchel duality is very related to the Quadratic programming problem (7.9) obtained via Lagrange duality (problem 7.4). 7.1.2 An Exponential Family Interpretation Our motivating arguments for deriving the SVM algorithm have largely been geometric. We now show that an equally elegant probabilistic interpre- tation also exists. Assuming that the training set f(x1;y1);:::; (xm;ym)g was drawn iid from some underlying distribution, and using the Bayes rule (1.15) one can write the likelihood p(jX;Y )/p()p(YjX;) =p()mY i=1p(yijxi;); (7.14)

7.1 Support Vector Classication 171 y(hw;xi+b)loss Fig. 7.3. The binary hinge loss. Note that the loss is convex but non-dierentiable at the kink point. Furthermore, it increases linearly as the distance from the decision hyperplane y(hw;xi+b) decreases. and hence the negative log-likelihood  logp(jX;Y ) = mX i=1logp(yijxi;) logp() + const. (7.15) In the absence of any prior knowledge about the data, we choose a zero mean unit variance isotropic normal distribution for p(). This yields  logp(jX;Y ) =1 2kk2 mX i=1logp(yijxi;) + const. (7.16) The maximum aposteriori (MAP) estimate for is obtained by minimizing (7.16) with respect to . Given the optimal , we can predict the class label at any given xvia y= argmax yp(yjx;): (7.17) Of course, our aim is not just to maximize p(yijxi;) but also to ensure thatp(yjxi;) is small for all y6=yi. This, for instance, can be achieved by requiring p(yijxi;) p(yjxi;);for ally6=yiand some1: (7.18) As we saw in Section 2.3 exponential families of distributions are rather ex- ible modeling tools. We could, for instance, model p(yijxi;) as a conditional exponential family distribution. Recall the denition: p(yjx;) = exp (h(x;y);i g(jx)): (7.19)

172 7 Linear Models Here(x;y) is a joint feature map which depends on both the input data x and the label y, whileg(jx) is the log-partition function. Now (7.18) boils down to p(yijxi;) maxy6=yip(yjxi;)= exp (xi;yi) max y6=yi(xi;y); : (7.20) If we choose such that log = 1, set(x;y) =y 2(x), and observe that y2f 1gwe can rewrite (7.20) as Dyi 2(xi)   yi 2 (xi);E =yih(xi);i1: (7.21) By replacing logp(yijxi;) in (7.16) with the condition (7.21) we obtain the following objective function: min 1 2kk2(7.22a) s.t.yih(xi);i1 for alli; (7.22b) which recovers (7.4), but without the bias b. The prediction function is recovered by noting that (7.17) specializes to y= argmax y2f1gh(x;y);i= argmax y2f1gy 2h(x);i= sign(h(x);i):(7.23) As before, we can replace (7.21) by a linear penalty for constraint viola- tion in order to recover (7.5). The quantity logp(yijxi;) maxy6=yip(yjxi;)is sometimes called the log-odds ratio , and the above discussion shows that SVMs can be interpreted as maximizing the log-odds ratio in the exponential family. This interpretation will be developed further when we consider extensions of SVMs to tackle multiclass, multilabel, and structured prediction problems. 7.1.3 Specialized Algorithms for Training SVMs The main task in training SVMs boils down to solving (7.9). The mm matrixHis usually dense and cannot be stored in memory. Decomposition methods are designed to overcome these diculties. The basic idea here is to identify and update a small working set Bby solving a small sub- problem at every iteration. Formally, let Bf1;:::;mgbe the working set andBbe the corresponding sub-vector of . Dene B=f1;:::;mgnB andBanalogously. In order to update Bwe need to solve the following

7.1 Support Vector Classication 173 sub-problem of (7.9) obtained by freezing B: min B1 2 > B>BHBBHBB HBBHBBB B   > B>B e(7.24a) s.t. > B>B y= 0 (7.24b) 0iC mfor alli2B: (7.24c) Here,HBBHBB HBBHBB is a permutation of the matrix H. By eliminating constant terms and rearranging, one can simplify the above problem to min B1 2> BHBBB+> B(HBBB e) (7.25a) s.t.> ByB= >ByB (7.25b) 0iC mfor alli2B: (7.25c) An extreme case of a decomposition method is the Sequential Minimal Op- timization (SMO) algorithm of Platt [Pla99], which updates only two coef- cients per iteration. The advantage of this strategy as we will see below is that the resultant sub-problem can be solved analytically. Without loss of generality let B=fi;jg, and dene s=yi=yj, cicj = (HBBB e)> andd= ( >ByB=yj). Then (7.25) specializes to min i;j1 2(Hii2 i+Hjj2 j+ 2Hijji) +cii+cjj (7.26a) s.t.si+j=d (7.26b) 0i;jC m: (7.26c) This QP in two variables has an analytic solution. Lemma 7.1 (Analytic solution of 2 variable QP) Dene bounds L=( max(0;d C m s)ifs>0 max(0;d s) otherwise(7.27) H=( min(C m;d s) ifs>0 min(C m;d C m s)otherwise;(7.28)

174 7 Linear Models and auxiliary variables = (Hii+Hjjs2 2sHij)and (7.29) = (cjs ci Hijd+Hjjds): (7.30) The optimal value of (7.26) can be computed analytically as follows: If = 0 then i=( L if<0 H otherwise: If>0, theni= max(L;min(H;= )). In both cases, j= (d si). Proof Eliminate the equality constraint by setting j= (d si). Due to the constraint 0 jC mit follows that si=d jcan be bounded viad C msid. Combining this with 0 iC mone can write LiHwhereLandHare given by (7.27) and (7.28) respectively. Substituting j= (d si) into the objective function, dropping the terms which do not depend on i, and simplifying by substituting andyields the following optimization problem in i: min i1 22 i i s.t.LiH: First consider the case when = 0. In this case, i=Lif<0 otherwise i=H. On other hand, if  > 0 then the unconstrained optimum of the above optimization problem is given by =. The constrained optimum is obtained by clipping appropriately: max( L;min(H;= )). This concludes the proof. To complete the description of SMO we need a valid stopping criterion as well as a scheme for selecting the working set at every iteration. In order to derive a stopping criterion we will use the KKT gap, that is, the extent to which the KKT conditions are violated. Towards this end introduce non- negative Lagrange multipliers b2R,2Rmand2Rmand write the Lagrangian of (7.9). L(;b;; ) =1 2>H >e+b>y >+>( C me): (7.31) If we letJ() =1 2>H >ebe the objective function and rJ() = H eits gradient, then taking gradient of the Lagrangian with respect to and setting it to 0 shows that rJ() +by= : (7.32)

7.1 Support Vector Classication 175 Furthermore, by the KKT conditions we have ii= 0 andi(C m i) = 0; (7.33) withi0 andi0. Equations (7.32) and (7.33) can be compactly rewritten as rJ()i+byi0 ifi= 0 (7.34a) rJ()i+byi0 ifi=C m(7.34b) rJ()i+byi= 0 if 0<i<C m: (7.34c) Sinceyi2f 1g, we can further rewrite (7.34) as  yirJ()ibfor alli2Iup  yirJ()ibfor alli2Idown; where the index sets IupandIdown are dened as Iup=fi:i<C m;yi= 1 ori>0;yi= 1g (7.35a) Idown =fi:i<C m;yi= 1 ori>0;yi= 1g: (7.35b) In summary, the KKT conditions imply that is a solution of (7.9) if and only if m()M() where m() = max i2Iup yirJ()iandM() = min i2Idown yirJ()i: (7.36) Therefore, a natural stopping criterion is to stop when the KKT gap falls below a desired tolerance , that is, m()M() +: (7.37) Finally, we turn our attention to the issue of working set selection. The rst order approximation to the objective function J() can be written as J(+d)J() +rJ()>d: Since we are only interested in updating coecients in the working set B we setd>= d> B0 , in which case we can rewrite the above rst order

176 7 Linear Models approximation as rJ()> BdBJ(+d) J(): From among all possible directions dBwe wish to choose one which decreases the objective function the most while maintaining feasibility. This is best expressed as the following optimization problem: min dBrJ()> BdB (7.38a) s.t.y> BdB= 0 (7.38b) di0 ifi= 0 andi2B (7.38c) di0 ifi=C mandi2B (7.38d)  1di1: (7.38e) Here (7.38b) comes from y>(+d) = 0 and y>= 0, while (7.38c) and (7.38d) comes from 0 iC m. Finally, (7.38e) prevents the objective function from diverging to  1. If we specialize (7.38) to SMO, we obtain min i;jrJ()idi+rJ()jdj (7.39a) s.t.yidi+yjdj= 0 (7.39b) dk0 ifk= 0 andk2fi;jg (7.39c) dk0 ifk=C mandk2fi;jg (7.39d)  1dk1 fork2fi;jg: (7.39e) At rst glance, it seems that choosing the optimal iandjfrom the set f1;:::;mgf1;:::mgrequiresO(m2) eort. We now show that O(m) eort suces. Dene new variables ^dk=ykdkfork2fi;jg, and use the observation yk2f 1gto rewrite the objective function as ( yirJ()i+yjrJ()j)^dj: Consider the case  rJ()iyi  rJ()jyj. Because of the constraints (7.39c) and (7.39d) if we choose i2Iupandj2Idown, then ^dj= 1 and ^di= 1 is feasible and the objective function attains a negative value. For all other choices of iandj(i;j2Iup;i;j2Idown;i2Idown andj2Iup) the objective function value of 0 is attained by setting ^di=^dj= 0. The case rJ()jyj rJ()iyiis analogous. In summary, the optimization

7.2 Extensions 177 problem (7.39) boils down to min i2Iup;j2IdownyirJ()i yjrJ()j= min i2IupyirJ()i max j2IdownyjrJ()j; which clearly can be solved in O(m) time. Comparison with (7.36) shows that at every iteration of SMO we choose to update coecients iandj which maximally violate the KKT conditions. 7.2 Extensions 7.2.1 The trick In the soft margin formulation the parameter Cis a trade-o between two conicting requirements namely maximizing the margin and minimizing the training error. Unfortunately, this parameter is rather unintuitive and hence dicult to tune. The -SVM was proposed to address this issue. As Theorem 7.3 below shows, controls the number of support vectors and margin errors. The primal problem for the -SVM can be written as min w;b;;1 2kwk2 +1 mmX i=1i (7.40a) s.t.yi(hw;xii+b) ifor alli (7.40b) i0;and0: (7.40c) As before, if we write the Lagrangian by introducing non-negative Lagrange multipliers, take gradients with respect to the primal variables and set them to zero, and substitute the result back into the Lagrangian we obtain the following dual: min 1 2X i;jyiyjijhxi;xji (7.41a) s.t.mX i=1iyi= 0 (7.41b) mX i=1i1 (7.41c) 0i1 m: (7.41d) It turns out that the dual can be further simplied via the following lemma.

178 7 Linear Models Lemma 7.2 Let2[0;1]and(7.41) be feasible. Then there is at least one solutionwhich satisesP ii= 1. Furthermore, if the nal objective value of(7.41) is non-zero then all solutions satisfyP ii= 1. Proof The feasible region of (7.41) is bounded, therefore if it is feasible then there exists an optimal solution. Let denote this solution and assume thatP ii>1. In this case we can dene =1P jj; and easily check that  is also feasible. As before, let Hdenote amm matrix with Hij=yiyjhxi;xji. Sinceis the optimal solution of (7.41) it follows that 1 2>H1 2>H=  1P jj!21 2>H1 2>H: This implies that either1 2>H= 0, in which case  is an optimal solution with the desired property or1 2>H6= 0, in which case all optimal solutions satisfyP ii= 1. In view of the above theorem one can equivalently replace (7.41) by the following simplied optimization problem with two equality constraints min 1 2X i;jyiyjijhxi;xji (7.42a) s.t.mX i=1iyi= 0 (7.42b) mX i=1i= 1 (7.42c) 0i1 m: (7.42d) The following theorems, which we state without proof, explain the signif- icance ofand the connection between -SVM and the soft margin formu- lation. Theorem 7.3 Suppose we run -SVM with kernel kon some data and obtain>0. Then (i)is an upper bound on the fraction of margin errors, that is points for whichyi(hw;xii+bi)<.

7.2 Extensions 179 (ii)is a lower bound on the fraction of support vectors, that is points for whichyi(hw;xii+bi) =. (iii) Suppose the data (X;Y )were generated iid from a distribution p(x;y) such that neither p(x;y= +1) orp(x;y= 1)contain any discrete components. Moreover, assume that the kernel kis analytic and non- constant. With probability 1, asympotically, equals both the fraction of support vectors and fraction of margin errors. Theorem 7.4 If(7.40) leads to a decision function with >0, then (7.5) withC=1 leads to the same decision function. 7.2.2 Squared Hinge Loss In binary classication, the actual loss which one would like to minimize is the so-called 0-1 loss l(w;x;y ) =( 0 ify(hw;xi+b)1 1 otherwise :(7.43) This loss is dicult to work with because it is non-convex (see Figure 7.4). In y(hw;xi+b)loss Fig. 7.4. The 0-1 loss which is non-convex and intractable is depicted in red. The hinge loss is a convex upper bound to the 0-1 loss and shown in blue. The square hinge loss is a dierentiable convex upper bound to the 0-1 loss and is depicted in green. fact, it has been shown that nding the optimal ( w;b) pair which minimizes the 0-1 loss on a training dataset of mlabeled points is NP hard [BDEL03]. Therefore various proxy functions such as the binary hinge loss (7.13) which we discussed in Section 7.1.1 are used. Another popular proxy is the square

180 7 Linear Models hinge loss: l(w;x;y ) = max(0;1 y(hw;xi+b))2: (7.44) Besides being a proxy for the 0-1 loss, the squared hinge loss, unlike the hinge loss, is also dierentiable everywhere. This sometimes makes the opti- mization in the primal easier. Just like in the case of the hinge loss one can derive the dual of the regularized risk minimization problem and show that it is a quadratic programming problem (problem 7.5). 7.2.3 Ramp Loss The ramp loss l(w;x;y ) = min(1 s;max(0;1 y(hw;xi+b))) (7.45) parameterized by s0 is another proxy for the 0-1 loss (see Figure 7.5). Although not convex, it can be expressed as the dierence of two convex functions lconc(w;x;y ) = max(0;1 y(hw;xi+b)) and lcave(w;x;y ) = max(0;s y(hw;xi+b)): Therefore the Convex-Concave procedure (CCP) we discussed in Section Fig. 7.5. The ramp loss depicted here with s= 0:3 can be viewed as the sum of a convex function namely the binary hinge loss (left) and a concave function min(0;1 y(hw;xi+b)) (right). Viewed alternatively, the ramp loss can be written as the dierence of two convex functions. 3.5.1 can be used to solve the resulting regularized risk minimization problem with the ramp loss. Towards this end write J(w) =1 2kwk2+C mmX i=1lconc(w;xi;yi) | {z } Jconc(w) C mmX i=1lcave(w;xi;yi) |{z} Jcave(w):(7.46)

7.3 Support Vector Regression 181 Recall that at every iteration of the CCP we replace Jcave(w) by its rst order Taylor approximation, computing which requires @wJ(w) =C mmX i=1@wlcave(w;xi;yi): (7.47) This in turn can be computed as @wlcave(w;xi;yi) =iyixiwithi=(  1 ifs>y (hw;xi+b) 0 otherwise :(7.48) Ignoring constant terms, each iteration of the CCP algorithm involves solv- ing the following minimization problem (also see (3.134)) J(w) =1 2kwk2+C mmX i=1lconc(w;xi;yi)   C mmX i=1iyixi! w: (7.49) Letdenote a vector in Rmwith components i. Using the same notation as in (7.9) we can write the following dual optimization problem which is very closely related to the standard SVM dual (7.9) (see problem 7.6) min 1 2>H >e (7.50a) s.t.>y= 0 (7.50b)  C miC m(e ): (7.50c) In fact, this problem can be solved by a SMO solver with minor modica- tions. Putting everything together yields Algorithm 7.1. Algorithm 7.1 CCP for Ramp Loss 1:Initialize0and0 2:repeat 3: Solve (7.50) to nd t+1 4: Computet+1using (7.48) 5:untilt+1=t 7.3 Support Vector Regression As opposed to classication where the labels yiare binary valued, in re- gression they are real valued. Given a tolerance , our aim here is to nd a

182 7 Linear Models y (hw;xi+b)loss  Fig. 7.6. The insensitive loss. All points which lie within the tube shaded in gray incur zero loss while points outside incur a linear loss. hyperplane parameterized by ( w;b) such that jyi (hw;xii+b)j: (7.51) In other words, we want to nd a hyperplane such that all the training data lies within an tube around the hyperplane. We may not always be able to nd such a hyperplane, hence we relax the above condition by introducing slack variables + iand  iand write the corresponding primal problem as min w;b;+; 1 2kwk2+C mmX i=1(+ i+  i) (7.52a) s.t.yi (hw;xii+b)++ ifor alli (7.52b) (hw;xii+b) yi+  ifor alli (7.52c) + i0;and  i0: (7.52d) The Lagrangian can be written by introducing non-negative Lagrange mul- tipliers+ i,  i,+ iand  i: L(w;b;+; ;+; ;+; ) =1 2kwk2+C mmX i=1(+ i+  i) mX i=1(+ i+ i+  i  i) +mX i=1+ i(yi (hw;xii+b)  +) +mX i=1  i((hw;xii+b) yi   ):

7.3 Support Vector Regression 183 Taking gradients with respect to the primal variables and setting them to 0, we obtain the following conditions: w=mX i=1(+ i   i)xi (7.53) mX i=1+ i=mX i=1  i (7.54) + i++ i=C m(7.55)   i+  i=C m: (7.56) Noting that f+; g i;f+; g i0 and substituting the above conditions into the Lagrangian yields the dual min +; 1 2X i;j(+ i   i)(+ j   j)hxi;xji (7.57a) +mX i=1(+ i+  i) mX i=1yi(+ i   i) s.t.mX i=1+ i=mX i=1  i (7.57b) 0+ iC m(7.57c) 0  iC m: (7.57d) This is a quadratic programming problem with one equality constraint, and hence a SMO like decomposition method can be derived for nding the optimal coecients +and (Problem 7.7). As a consequence of (7.53), analogous to the classication case, one can map the data via a feature map into an RKHS with kernel kand recover the decision boundary f(x) =hw;(x)i+bvia f(x) =mX i=1(+ i   i)h(x)i;(x)i+b=mX i=1(+ i   i)k(xi;x) +b:(7.58) Finally, the KKT conditions C m + i + i= 0C m   i   i= 0 and   i((hw;xii+b) yi   ) = 0+ i(yi (hw;xii+b)  +) = 0;

184 7 Linear Models allow us to draw many useful conclusions: Wheneverjyi (hw;xii+b)j< , this implies that + i=  i=+ i=   i= 0. In other words, points which lie inside the tube around the hyperplanehw;xi+bdo not contribute to the solution thus leading to sparse expansions in terms of . If (hw;xii+b) yi>we have  i>0 and therefore   i=C m. On the other hand,+= 0 and+ i= 0. The case yi (hw;xii+b)> is symmetric and yields = 0,+ i>0,+ i=C m, and  i= 0. Finally, if (hw;xii+b) yi=we have  i= 0 and 0  iC m, while += 0 and+ i= 0. Similarly, when yi (hw;xii+b) =we obtain + i= 0, 0+ iC m, = 0 and  i= 0. Note that+ iand  iare never simultaneously non-zero. 7.3.1 Incorporating General Loss Functions Using the same reasoning as in Section 7.1.1 we can deduce from (7.52) that the loss function of support vector regression is given by l(w;x;y ) = max(0;jy hw;xij ): (7.59) It turns out that the support vector regression framework can be easily extended to handle other, more general, convex loss functions such as the ones found in Table 7.1. Dierent losses have dierent properties and hence lead to dierent estimators. For instance, the square loss leads to penalized least squares (LS) regression, while the Laplace loss leads to the penalized least absolute deviations (LAD) estimator. Huber's loss on the other hand is a combination of the penalized LS and LAD estimators, and the pinball loss with parameter 2[0;1] is used to estimate -quantiles. Setting = 0:5 in the pinball loss leads to a scaled version of the Laplace loss. If we dene =y hw;xi, then it is easily veried that all these losses can all be written as l(w;x;y ) =8 >< >:l+( ) if> l (  ) if< 0 if 2[ ;]:(7.60) For all these dierent loss functions, the support vector regression formu-

7.3 Support Vector Regression 185 lation can be written in a unied fashion as follows min w;b;+; 1 2kwk2+C mmX i=1l+(+ i) +l (  i) (7.61a) s.t.yi (hw;xii+b)++ ifor alli (7.61b) (hw;xii+b) yi+  ifor alli (7.61c) + i0;and  i0: (7.61d) The dual in this case is given by min +; 1 2X i;j(+ i   i)(+ j   j)hxi;xji (7.62a)  C mmX i=1T+(+) +T ( ) +mX i=1(+ i+  i) mX i=1yi(+ i   i) s.t.mX i=1+ i=mX i=1  i (7.62b) 0f+; g iC m@lf+; g(f+; g i ) (7.62c) 0f+; g i (7.62d) f+; g i = inf f+; gjC m@lf+; gf+; g i : (7.62e) HereT+() =l+() @l+() andT () =l () @l (). We now show how (7.62) can be specialized to the pinball loss. Clearly, l+() =while l ( ) = ( 1), and hence l () = (1 ). Therefore, T+() = ( 1)  ( 1) = 0. Similarly T () = 0. Since @l+() =and@l () = (1 ) for all0, it follows that the bounds on f+; gcan be computed as 0+ iC mand 0  iC m(1 ). If we denote =+  and Table 7.1. Various loss functions which can be used in support vector regression. For brevity we denote y hw;xiasand write the loss l(w;x;y )in terms of . -insensitive loss max(0 ;jj ) Laplace lossjj Square loss1 2jj2 Huber's robust loss1 22ifjj jj  2otherwise Pinball loss if0 ( 1)otherwise:

186 7 Linear Models observe that = 0 for the pinball loss then (7.62) specializes as follows: min 1 2X i;jijhxi;xji mX i=1yii (7.63a) s.t.mX i=1i= 0 (7.63b) C m( 1)iC m: (7.63c) Similar specializations of (7.62) for other loss functions in Table 7.1 can be derived. 7.3.2 Incorporating the Trick One can also incorporate the trick into support vector regression. The primal problem obtained after incorporating the trick can be written as min w;b;+; ;1 2kwk2+  +1 mmX i=1(+ i+  i)! (7.64a) s.t. (hw;xii+b) yi++ ifor alli (7.64b) yi (hw;xii+b)+  ifor alli (7.64c) + i0;  i0;and0: (7.64d) Proceeding as before we obtain the following simplied dual min +; 1 2X i;j(  i + i)(  j + j)hxi;xji mX i=1yi(  i + i) (7.65a) s.t.mX i=1(  i + i) = 0 (7.65b) mX i=1(  i++ i) = 1 (7.65c) 0+ i1 m(7.65d) 0  i1 m: (7.65e) 7.4 Novelty Detection The large margin approach can also be adapted to perform novelty detection or quantile estimation. Novelty detection is an unsupervised task where one

7.4 Novelty Detection 187 is interested in agging a small fraction of the input X=fx1;:::;xmgas atypical or novel. It can be viewed as a special case of the quantile estimation task, where we are interested in estimating a simple setCsuch thatPr(x2 C)for some2[0;1]. One way to measure simplicity is to use the volume of the set. Formally, if jCjdenotes the volume of a set, then the quantile estimation task is to estimate arginffjCjs.t.Pr(x2C)g: (7.66) Given the input data Xone can compute the empirical density ^p(x) =( 1 mifx2X 0 otherwise ; and estimate its (not necessarily unique) -quantiles. Unfortunately, such estimates are very brittle and do not generalize well to unseen data. One possible way to address this issue is to restrict Cto be simple subsets such as spheres or half spaces. In other words, we estimate simple sets which containfraction of the dataset. For our purposes, we specically work with half-spaces dened by hyperplanes. While half-spaces may seem rather restrictive remember that the kernel trick can be used to map data into a high-dimensional space; half-spaces in the mapped space correspond to non-linear decision boundaries in the input space. Furthermore, instead of explicitly identifying Cwe will learn an indicator function for C, that is, a functionfwhich takes on values  1 inside Cand 1 elsewhere. With1 2kwk2as a regularizer, the problem of estimating a hyperplane such that a large fraction of the points in the input data Xlie on one of its sides can be written as: min w;;1 2kwk2+1 mmX i=1i  (7.67a) s.t.hw;xii ifor alli (7.67b) i0: (7.67c) Clearly, we want to be as large as possible so that the volume of the half- spacehw;xiis minimized. Furthermore, 2[0;1] is a parameter which is analogous to we introduced for the -SVM earlier. Roughly speaking, it denotes the fraction of input data for which hw;xii. An alternative interpretation of (7.67) is to assume that we are separating the data set X from the origin (See Figure 7.7 for an illustration). Therefore, this method is also widely known as the one-class SVM.

188 7 Linear Models Fig. 7.7. The novelty detection problem can be viewed as nding a large margin hyperplane which separates fraction of the data points away from the origin. The Lagrangian of (7.67) can be written by introducing non-negative Lagrange multipliers i, andi: L(w;;;; ) =1 2kwk2+1 mmX i=1i +mX i=1i( i hw;xii) mX i=1ii: By taking gradients with respect to the primal variables and setting them to 0 we obtain w=mX i=1ixi (7.68) i=1 m i1 m(7.69) mX i=1i= 1: (7.70) Noting that i;i0 and substituting the above conditions into the La- grangian yields the dual min 1 2X i;jijhxi;xji (7.71a) s.t. 0i1 m(7.71b) mX i=1i= 1: (7.71c)

7.5 Margins and Probability 189 This can easily be solved by a straightforward modication of the SMO algorithm (see Section 7.1.3 and Problem 7.7). Like in the previous sections, an analysis of the KKT conditions shows that 0 <if and only ifhw;xii; such points are called support vectors. As before, we can replace hxi;xjiby a kernelk(xi;xj) to transform half-spaces in the feature space to non-linear shapes in the input space. The following theorem explains the signicance of the parameter . Theorem 7.5 Assume that the solution of (7.71) satises6= 0, then the following statements hold: (i)is an upper bound on the fraction of support vectors, that is points for whichhw;xii. (ii)Suppose the data Xwere generated independently from a distribution p(x)which does not contain discrete components. Moreover, assume that the kernel kis analytic and non-constant. With probability 1, asympotically, equals the fraction of support vectors. 7.5 Margins and Probability discuss the connection between probabilistic models and linear classiers. issues of consistency, optimization, eciency, etc. 7.6 Beyond Binary Classication In contrast to binary classication where there are only two possible ways to label a training sample, in some of the extensions we discuss below each training sample may be associated with one or more of kpossible labels. Therefore, we will use the decision function y= argmax y2f1;:::;kgf(x;y) wheref(x;y) =h(x;y);wi: (7.72) Recall that the joint feature map (x;y) was introduced in section 7.1.2. One way to interpret the above equation is to view f(x;y) as a compatibility score between instance xand labely; we assign the label with the highest compatibility score to x. There are a number of extensions of the binary hinge loss (7.13) which can be used to estimate this score function. In all these cases the objective function is written as min wJ(w) := 2kwk2+1 mmX i=1l(w;xi;yi): (7.73)

190 7 Linear Models Hereis a scalar which trades o the regularizer1 2kwk2with the empirical risk1 mPm i=1l(w;xi;yi). Plugging in dierent loss functions yields classiers for dierent settings. Two strategies exist for nding the optimal w. Just like in the binary SVM case, one can compute and maximize the dual of (7.73). However, the number of dual variables becomes mjYj, wheremis the number of training points and jYjdenotes the size of the label set. The second strategy is to optimize (7.73) directly. However, the loss functions we discuss below are non-smooth, therefore non-smooth optimization algorithms such as bundle methods (section 3.2.7) need to be used. 7.6.1 Multiclass Classication In multiclass classication a training example is labeled with one of kpos- sible labels, that is, Y=f1;:::;kg. We discuss two dierent extensions of the binary hinge loss to the multiclass setting. It can easily be veried that setting Y=f1gand(x;y) =y 2(x) recovers the binary hinge loss in both cases. 7.6.1.1 Additive Multiclass Hinge Loss A natural generalization of the binary hinge loss is to penalize all labels which have been misclassied. The loss can now be written as l(w;x;y ) =X y06=ymax  0;1 ( (x;y) (x;y0);w ) : (7.74) 7.6.1.2 Maximum Multiclass Hinge Loss Another variant of (7.13) penalizes only the maximally violating label: l(w;x;y ) := max 0;max y06=y(1  (x;y) (x;y0);w ) : (7.75) Note that both (7.74) and (7.75) are zero whenever f(x;y) =h(x;y);wi1 + max y06=y (x;y0);w = 1 + max y06=yf(x;y0):(7.76) In other words, they both ensure an adequate margin of separation, in this case 1, between the score of the true label f(x;y) and every other label f(x;y0). However, they dier in the way they penalize violators, that is, la- belsy06=yfor whichf(x;y)1 +f(x;y0). In one case we linearly penalize the violators and sum up their contributions while in the other case we lin- early penalize only the maximum violator. In fact, (7.75) can be interpreted

7.6 Beyond Binary Classication 191 as the log odds ratio in the exponential family. Towards this end choose  such that log = 1 and rewrite (7.20): logp(yjx;w) maxy06=yp(y0jx;w)= (x;y) max y06=y(x;y0);w 1: Rearranging yields (7.76). 7.6.2 Multilabel Classication In multilabel classication one or more of kpossible labels are assigned to a training example. Just like in the multiclass case two dierent losses can be dened. 7.6.2.1 Additive Multilabel Hinge Loss If we let YxYdenote the labels assigned to x, and generalize the hinge loss to penalize all labels y0=2Yxwhich have been assigned higher score than somey2Yx, then the loss can be written as l(w;x;y ) =X y2Yxandy0=2Yxmax  0;1 ( (x;y) (x;y0);w ) :(7.77) 7.6.2.2 Maximum Multilabel Hinge Loss Another variant only penalizes the maximum violating pair. In this case the loss can be written as l(w;x;y ) = max 0;max y2Yx;y0=2Yx 1   (x;y) (x;y0);w :(7.78) One can immediately verify that specializing the above losses to the mul- ticlass case recovers (7.74) and (7.75) respectively, while the binary case recovers (7.13). The above losses are zero only when min y2Yxf(x;y) = min y2Yxh(x;y);wi1 + max y0=2Yx (x;y0);w = 1 + max y0=2Yxf(x;y0): This can be interpreted as follows: The losses ensure that all the labels assigned to xhave larger scores compared to labels not assigned to xwith the margin of separation of at least 1. Although the above loss functions are compatible with multiple labels, the prediction function argmaxyf(x;y) only takes into account the label with the highest score. This is a signicant drawback of such models, which can be overcome by using a multiclass approach instead. Let jYjbe the size of the label set and z2RjYjdenote a vector with 1 entries. We set

192 7 Linear Models zy= +1 if the y2Yxandzy= 1 otherwise, and use the multiclass loss (7.75) onz. To predict we compute z= argmaxzf(x;z) and assign to x the labels corresponding to components of zwhich are +1. Since zcan take on 2jYjpossible values, this approach is not feasible if jYjis large. To tackle such problems, and to further reduce the computational complexity we assume that the labels correlations are captured via a jYjjYjpositive semi-denite matrix P, and(x;y) can be written as (x) Py. Here denotes the Kronecker product. Furthermore, we express the vector was anjYjmatrixW, wherendenotes the dimension of (x). With these assumptionsh(x) P(z z0);wican be rewritten as D (x)>WP; (z z0)E =X ih (x)>WPi i(zi z0 i); and (7.78) specializes to l(w;x;z ) := max  0;  1 X imin z0 i6=zih (x)>WPi i(zi z0 i)!! :(7.79) A analogous specialization of (7.77) can also be derived wherein the mini- mum is replaced by a summation. Since the minimum (or summation as the case may be) is over jYjpossible labels, computing the loss is tractable even if the set of labels Yis large. 7.6.3 Ordinal Regression and Ranking We can generalize our above discussion to consider slightly more general ranking problems. Denote by Ythe set of all directed acyclic graphs on N nodes. The presence of an edge ( i;j) iny2Yindicates that iis preferred toj. The goal is to nd a function f(x;i) which imposes a total order on f1;:::;Ngwhich is in close agreement with y. Specically, if the estimation error is given by the number of subgraphs of ywhich are in disagreement with the total order imposed by f, then the additive version of the loss can be written as l(w;x;y ) =X G2A(y)max (i;j)2G(0;1 (f(x;i) f(x;j))); (7.80) where A(y) denotes the set of all possible subgraphs of y. The maximum margin version, on the other hand, is given by l(w;x;y ) = max G2A(y)max (i;j)2G(0;1 (f(x;i) f(x;j))): (7.81)

7.7 Large Margin Classiers with Structure 193 In other words, we test for each subgraph Gofyif the ranking imposed by G is satised by f. Selecting specic types of directed acyclic graphs recovers the multiclass and multilabel settings (problem 7.9). 7.7 Large Margin Classiers with Structure 7.7.1 Margin dene margin pictures 7.7.2 Penalized Margin dierent types of loss, rescaling 7.7.3 Nonconvex Losses the max - max loss 7.8 Applications 7.8.1 Sequence Annotation 7.8.2 Matching 7.8.3 Ranking 7.8.4 Shortest Path Planning 7.8.5 Image Annotation 7.8.6 Contingency Table Loss 7.9 Optimization 7.9.1 Column Generation subdierentials 7.9.2 Bundle Methods 7.9.3 Overrelaxation in the Dual when we cannot do things exactly

194 7 Linear Models 7.10 CRFs vs Structured Large Margin Models 7.10.1 Loss Function 7.10.2 Dual Connections 7.10.3 Optimization Problems Problem 7.1 (Deriving the Margin f1g)Show that the distance of a pointxito a hyperplane H=fxjhw;xi+b= 0gis given byjhw;xii+ bj=kwk. Problem 7.2 (SVM without Bias f1g)A homogeneous hyperplane is one which passes through the origin, that is, H=fxjhw;xi= 0g: (7.82) If we devise a soft margin classier which uses the homogeneous hyperplane as a decision boundary, then the corresponding primal optimization problem can be written as follows: min w;1 2kwk2+CmX i=1i (7.83a) s.t.yihw;xii1 ifor alli (7.83b) i0; (7.83c) Derive the dual of (7.83) and contrast it with (7.9) . What changes to the SMO algorithm would you make to solve this dual? Problem 7.3 (Deriving the simplied -SVM dual f2g)In Lemma 7.2 we used (7.41) to show that the constraintP ii1can be replaced byP ii= 1. Show that an equivalent way to arrive at the same conclusion is by arguing that the constraint 0is redundant in the primal (7.40) .Hint: Observe that whenever <0the objective function is always non-negative. On the other hand, setting w==b== 0 yields an objective function value of 0. Problem 7.4 (Fenchel and Lagrange Duals f2g)We derived the La- grange dual of (7.12) in Section 7.1 and showed that it is (7.9) . Derive the Fenchel dual of (7.12) and relate it to (7.9) .Hint: See theorem 3.3.5 of [BL00].

7.10 CRFs vs Structured Large Margin Models 195 Problem 7.5 (Dual of the square hinge loss f1g)The analog of (7.5) when working with the square hinge loss is the following min w;b;1 2kwk2+C mmX i=12 i (7.84a) s.t.yi(hw;xii+b)1 ifor alli (7.84b) i0; (7.84c) Derive the Lagrange dual of the above optimization problem and show that it a Quadratic Programming problem. Problem 7.6 (Dual of the ramp loss f1g)Derive the Lagrange dual of (7.49) and show that it the Quadratic Programming problem (7.50) . Problem 7.7 (SMO for various SVM formulations f2g)Derive an SMO like decomposition algorithm for solving the dual of the following problems: -SVM (7.41) . SV regression (7.57) . SV novelty detection (7.71) . Problem 7.8 (Novelty detection with Balls f2g)In Section 7.4 we as- sumed that we wanted to estimate a halfspace which contains a major frac- tion of the input data. An alternative approach is to use balls, that is, we estimate a ball of small radius in feature space which encloses a majority of the input data. Write the corresponding optimization problem and its dual. Show that if the kernel is translation invariant, that is, k(x;x0)depends only onkx x0kthen the optimization problem with balls is equivalent to (7.71) . Explain why this happens geometrically. Problem 7.9 (Multiclass and Multilabel loss from Ranking Loss f1g) Show how the multiclass (resp. multilabel) losses (7.74) and (7.75) (resp. (7.77) and (7.79) ) can be derived as special cases of (7.80) and (7.81) re- spectively. Problem 7.10 Invariances (basic loss) Problem 7.11 Polynomial transformations - SDP constraints

Appendix 1 Linear Algebra and Functional Analysis A1.1 Johnson Lindenstrauss Lemma Lemma 1.1 (Johnson Lindenstrauss) LetXbe a set ofnpoints in Rd represented as a ndmatrixA. Given; > 0let k4 + 2 2=2 3=3logn (1.1) be a positive integer. Construct a dkrandom matrix Rwith independent standard normal random variables, that is, RijN(0;1), and let E=1p kAR: (1.2) Denef:Rd!Rkas the function which maps the rows of Ato the rows ofE. With probability at least 1 n , for allu;v2Xwe have (1 )ku vk2kf(u) f(v)k2(1 +)ku vk2: (1.3) Our proof presentation by and large follows [ ?]. We rst show that Lemma 1.2 For any arbitrary vector 2Rdletqidenote thei-th compo- nent off(). ThenqiN(0;kk2=k)and hence Eh kf()k2i =kX i=1E q2 i =kk2: (1.4) In other words, the expected length of vectors are preserved even after em- bedding them in a kdimensional space. Next we show that the lengths of the embedded vectors are tightly concentrated around their mean. Lemma 1.3 For any>0and any unit vector 2Rdwe have Pr kf()k2>1 + <exp  k 2  2=2 3=3 (1.5) Pr kf()k2<1  <exp  k 2  2=2 3=3 : (1.6) 197

198 1 Linear Algebra and Functional Analysis Corollary 1.4 If we choose kas in (1.1) then for any 2Rdwe have Pr (1 )kk2kf()k2(1 +)kk2 1 2 n2+: (1.7) Proof Follows immediately from Lemma 1.3 by setting 2 exp  k 2  2=2 3=3 2 n2+; and solving for k. There are n 2 pairs of vectors u;vinX, and their corresponding distances ku vkare preserved within 1 factor as shown by the above lemma. Therefore, the probability of not satisfying (1.3) is bounded by n 2 2 n2+< 1=nas claimed in the Johnson Lindenstrauss Lemma. All that remains is to prove Lemma 1.2 and 1.3. Proof (Lemma 1.2). Since qi=1p kP jRijjis a linear combination of stan- dard normal random variables Rijit follows that qiis normally distributed. To compute the mean note that E[qi] =1p kX jjE[Rij] = 0: SinceRijare independent zero mean unit variance random variables, E[RijRil] = 1 ifj=land 0 otherwise. Using this E q2 i =1 kE0 @dX j=1Rijj1 A2 =1 kdX j=1dX l=1jlE[RijRil] =1 kdX j=12 j=1 kkk2: Proof (Lemma 1.3). Clearly, for all  Prh kf()k2>1 +i =Prh exp kf()k2 >exp((1 +))i :

A1.1 Johnson Lindenstrauss Lemma 199 Using Markov's inequality ( Pr[Xa]E[X]=a) we obtain Prh exp kf()k2 >exp((1 +))i Eh exp kf()k2i exp((1 +)) =Eh exp Pk i=1q2 ii exp((1 +)) =EhQk i=1exp  q2 ii exp((1 +)) =  E exp  q2 i exp  k(1 +)!k : (1.8) The last equality is because the qi's are i.i.d. Since is a unit vector, from the previous lemma qiN(0;1=k). Therefore, kq2 iis a2random variable with moment generating function E exp  q2 i =E exp kkq2 i =1q 1 2 k: Plugging this into (1.8) Prh exp kf()k2 >exp ((1 +))i 0 @exp    k(1 +) q 1 2 k1 Ak : Setting=k 2(1+)in the above inequality and simplifying Prh exp kf()k2 >exp((1 +))i (exp( )(1 +))k=2: Using the inequality log(1 +)< 2=2 +3=3 we can write Prh exp kf()k2 >exp((1 +))i exp  k 2  2=2 3=3 : This proves (1.5). To prove (1.6) we need to repeat the above steps and use the inequality log(1 )<  2=2: This is left as an exercise to the reader.

200 1 Linear Algebra and Functional Analysis A1.2 Spectral Properties of Matrices A1.2.1 Basics A1.2.2 Special Matrices unitary, hermitean, positive semidenite A1.2.3 Normal Forms Jacobi A1.3 Functional Analysis A1.3.1 Norms and Metrics vector space, norm, triangle inequality A1.3.2 Banach Spaces normed vector space, evaluation functionals, examples, dual space A1.3.3 Hilbert Spaces symmetric inner product A1.3.4 Operators spectrum, norm, bounded, unbounded operators A1.4 Fourier Analysis A1.4.1 Basics A1.4.2 Operators

Appendix 2 Conjugate Distributions 201

202 2 Conjugate Distributions Binomial | Beta (x) =x eh(n;n)= (n+ 1) (n(1 ) + 1)  (n+ 2)=B(n+ 1;n(1 ) + 1) In traditional notation one represents the conjugate as p(z;;) = (+)  () ()z 1(1 z) 1 where=n+ 1 and=n(1 b) + 1. Multinomial | Dirichlet (x) =ex eh(n;n)=Qd i=1 (ni+ 1)  (n+d) In traditional notation one represents the conjugate as p(z;) = (Pd i=1i)Qd i=1 (i)dY i=1zi 1 i wherei=ni+ 1 Poisson | Gamma (x) =x eh(n;n)=n n (n) In traditional notation one represents the conjugate as p(z;) =  ()z 1e x where=nand=n. Multinomial / Binomial Gaussian Laplace Poisson Dirichlet Wishart Student-t Beta Gamma

Appendix 3 Loss Functions A3.1 Loss Functions A multitude of loss functions are commonly used to derive seemingly dier- ent algorithms. This often blurs the similarities as well as subtle dierences between them, often for historic reasons: Each new loss is typically accompa- nied by at least one publication dedicated to it. In many cases, the loss is not spelled out explicitly either but instead, it is only given by means of a con- strained optimization problem. A case in point are the papers introducing (binary) hinge loss [BM92, CV95] and structured loss [TGK04, TJHA05]. Likewise, a geometric description obscures the underlying loss function, as in novelty detection [SPST+01]. In this section we give an expository yet unifying presentation of many of those loss functions. Many of them are well known, while others, such as multivariate ranking, hazard regression, or Poisson regression are not commonly used in machine learning. Tables A3.1 and A3.1 contain a choice subset of simple scalar and vectorial losses. Our aim is to put the multitude of loss functions in an unied framework, and to show how these losses and their (sub)gradients can be computed eciently for use in our solver framework. Note that not all losses, while convex, are continuously dierentiable. In this situation we give asubgradient. While this may not be optimal, the convergence rates of our algorithm do not depend on which element of the subdierential we provide: in all cases the rst order Taylor approximation is a lower bound which is tight at the point of expansion. In this setion, with little abuse of notation, viis understood as the i-th component of vector vwhenvis clearly not an element of a sequence or a set. A3.1.1 Scalar Loss Functions It is well known [Wah97] that the convex optimization problem min subject toyhw;xi1 and0 (3.1) 203

204 3 Loss FunctionsScalar loss functions and their derivatives, depending on f:=hw;xi, andy. Loss l(f;y) Derivative l0(f;y) Hinge [BM92] max(0 ;1 yf) 0 if yf1 and yotherwise Squared Hinge [KD05]1 2max(0;1 yf)20 ifyf1 andf yotherwise Exponential [CDLS99] exp(  yf)  yexp( yf) Logistic [CSS00] log(1 + exp(  yf))  y=(1 + exp( yf)) Novelty [SPST+01] max(0 ; f) 0 if fand 1 otherwise Least mean squares [Wil98]1 2(f y)2f y Least absolute deviation jf yj sign(f y) Quantile regression [Koe05] max( (f y);(1 )(y f)) iff >y and 1 otherwise -insensitive [VGS97] max(0 ;jf yj ) 0 if jf yj, else sign(f y) Huber's robust loss [MSR+97]1 2(f y)2ifjf yj1, elsejf yj 1 2f yifjf yj1, else sign( f y) Poisson regression [Cre93] exp( f) yf exp(f) y Vectorial loss functions and their derivatives, depending on the vector f:=Wxand ony. Loss Derivative Soft-Margin Multiclass [TGK04] max y0(fy0 fy+ (y;y0)) ey ey [CS03] where yis the argmax of the loss Scaled Soft-Margin Multiclass max y0 (y;y0)(fy0 fy+ (y;y0))  (y;y0)(ey ey) [TJHA05] where yis the argmax of the loss Softmax Multiclass [CDLS99] logP y0exp(fy0) fyhP y0ey0exp(f0 y)i =P y0exp(f0 y) ey Multivariate Regression1 2(f y)>M(f y) whereM0M(f y)

A3.1 Loss Functions 205 takes on the value max(0 ;1 yhw;xi). The latter is a convex function in wandx. Likewise, we may rewrite the -insensitive loss, Huber's robust loss, the quantile regression loss, and the novelty detection loss in terms of loss functions rather than a constrained optimization problem. In all cases, hw;xiwill play a key role insofar as the loss is convex in terms of the scalar quantityhw;xi. A large number of loss functions fall into this category, as described in Table A3.1. Note that not all functions of this type are continuously dierentiable. In this case we adopt the convention that @xmax(f(x);g(x)) =( @xf(x) iff(x)g(x) @xg(x) otherwise :(3.2) Since we are only interested in obtaining an arbitrary element of the subd- ierential this convention is consistent with our requirements. Let us discuss the issue of ecient computation. For all scalar losses we may writel(x;y;w ) =l(hw;xi;y), as described in Table A3.1. In this case a simple application of the chain rule yields that @wl(x;y;w ) =l0(hw;xi;y)x. For instance, for squared loss we have l(hw;xi;y) =1 2(hw;xi y)2andl0(hw;xi;y) =hw;xi y: Consequently, the derivative of the empirical risk term is given by @wRemp(w) =1 mmX i=1l0(hw;xii;yi)xi: (3.3) This means that if we want to compute land@wlon a large number of observations xi, represented as matrix X, we can make use of fast linear algebra routines to pre-compute the vectors f=Xwandg>Xwheregi=l0(fi;yi): (3.4) This is possible for any of the loss functions listed in Table A3.1, and many other similar losses. The advantage of this unied representation is that im- plementation of each individual loss can be done in very little time. The computational infrastructure for computing Xwandg>Xis shared. Eval- uating l(fi;yi) and l0(fi;yi) for allican be done in O(m) time and it is not time-critical in comparison to the remaining operations. Algorithm 3.1 describes the details. An important but often neglected issue is worth mentioning. Computing f requires us to right multiply the matrix Xwith the vector wwhile computing grequires the leftmultiplication of Xwith the vector g>. IfXis stored in a row major format then Xwcan be computed rather eciently while g>Xis

206 3 Loss Functions Algorithm 3.1 ScalarLoss( w;X;y ) 1:input: Weight vector w, feature matrix X, and labels y 2:Computef=Xw 3:Computer=P il(fi;yi) andg=l0(f;y) 4:g g>X 5:return Riskrand gradient g expensive. This is particularly true if Xcannot t in main memory. Converse is the case when Xis stored in column major format. Similar problems are encountered when Xis a sparse matrix and stored in either compressed row format or in compressed column format. A3.1.2 Structured Loss In recent years structured estimation has gained substantial popularity in machine learning [TJHA05, TGK04, BHS+07]. At its core it relies on two types of convex loss functions: logistic loss: l(x;y;w ) = logX y02Yexp  w;(x;y0)  hw;(x;y)i; (3.5) and soft-margin loss: l(x;y;w ) = max y02Y (y;y0) w;(x;y0) (x;y) + (y;y0): (3.6) Here(x;y) is a joint feature map, ( y;y0)0 describes the cost of mis- classifyingybyy0, and  (y;y0)0 is a scaling term which indicates by how much the large margin property should be enforced. For instance, [TGK04] choose  (y;y0) = 1. On the other hand [TJHA05] suggest  ( y;y0) = (y;y0), which reportedly yields better performance. Finally, [McA07] recently sug- gested generic functions  ( y;y0). The logistic loss can also be interpreted as the negative log-likelihood of a conditional exponential family model: p(yjx;w) := exp(hw;(x;y)i g(wjx)); (3.7) where the normalizing constant g(wjx), often called the log-partition func- tion, reads g(wjx) := logX y02Yexp  w;(x;y0) : (3.8)

A3.1 Loss Functions 207 As a consequence of the Hammersley-Cliord theorem [Jor08] every expo- nential family distribution corresponds to a undirected graphical model. In our case this implies that the labels yfactorize according to an undirected graphical model. A large number of problems have been addressed by this setting, amongst them named entity tagging [LMP01], sequence alignment [TJHA05], segmentation [RSS+07] and path planning [RBZ06]. It is clearly impossible to give examples of all settings in this section, nor would a brief summary do this eld any justice. We therefore refer the reader to the edited volume [BHS+07] and the references therein. If the underlying graphical model is tractable then ecient inference al- gorithms based on dynamic programming can be used to compute (3.5) and (3.6). We discuss intractable graphical models in Section A3.1.2.1, and now turn our attention to the derivatives of the above structured losses. When it comes to computing derivatives of the logistic loss, (3.5), we have @wl(x;y;w ) =P y0(x;y0) exphw;(x;y0)iP y0exphw;(x;y0)i (x;y) (3.9) =Ey0p(y0jx) (x;y0)  (x;y): (3.10) wherep(yjx) is the exponential family model (3.7). In the case of (3.6) we denote by y(x) the argmax of the RHS, that is y(x) := argmax y0 (y;y0) w;(x;y0) (x;y) + (y;y0): (3.11) This allows us to compute the derivative of l(x;y;w ) as @wl(x;y;w ) =  (y;y(x)) [(x;y(x)) (x;y)]: (3.12) In the case where the loss is maximized for more than one distinct value  y(x) we may average over the individual values, since any convex combination of such terms lies in the subdierential. Note that (3.6) majorizes ( y;y), wherey:= argmaxy0hw;(x;y0)i [TJHA05]. This can be seen via the following series of inequalities: (y;y) (y;y)hw;(x;y) (x;y)i+ (y;y)l(x;y;w ): The rst inequality follows because  ( y;y)0 andymaximizeshw;(x;y0)i thus implying that  ( y;y)hw;(x;y) (x;y)i0. The second inequal- ity follows by denition of the loss. We conclude this section with a simple lemma which is at the heart of several derivations of [Joa05]. While the proof in the original paper is far from trivial, it is straightforward in our setting:

208 3 Loss Functions Lemma 3.1 Denote by(y;y0)a loss and let (xi;yi)be a feature map for observations (xi;yi)with 1im. Moreover, denote by X;Y the set of allmpatterns and labels respectively. Finally let (X;Y ) :=mX i=1(xi;yi)and(Y;Y0) :=mX i=1(yi;y0 i): (3.13) Then the following two losses are equivalent: mX i=1max y0 w;(xi;y0) (xi;yi) +(yi;y0)and max Y0 w;(X;Y0) (X;Y ) + (Y;Y0): This is immediately obvious, since both feature map and loss decompose, which allows us to perform maximization over Y0by maximizing each of its mcomponents. In doing so, we showed that aggregating all data and labels into a single feature map and loss yields results identical to minimizing the sum over all individual losses. This holds, in particular, for the sample error loss of [Joa05]. Also note that this equivalence does nothold whenever  (y;y0) is not constant. A3.1.2.1 Intractable Models We now discuss cases where computing l(x;y;w ) itself is too expensive. For instance, for intractable graphical models, the computation ofP yexphw;(x;y)i cannot be computed eciently. [WJ03] propose the use of a convex majoriza- tion of the log-partition function in those cases. In our setting this means that instead of dealing with l(x;y;w ) =g(wjx) hw;(x;y)iwhereg(wjx) := logX yexphw;(x;y)i (3.14) one uses a more easily computable convex upper bound on gvia sup 2MARG(x)hw;i+HGauss (jx): (3.15) Here MARG( x) is an outer bound on the conditional marginal polytope associated with the map (x;y). Moreover, HGauss (jx) is an upper bound on the entropy by using a Gaussian with identical variance. More rened tree decompositions exist, too. The key benet of our approach is that the solutionof the optimization problem (3.15) can immediately be used as a gradient of the upper bound. This is computationally rather ecient.

A3.1 Loss Functions 209 Likewise note that [TGK04] use relaxations when solving structured esti- mation problems of the form l(x;y;w ) = max y0 (y;y0) w;(x;y0) (x;y) + (y;y0); (3.16) by enlarging the domain of maximization with respect to y0. For instance, instead of an integer programming problem we might relax the setting to a linear program which is much cheaper to solve. This, again, provides an upper bound on the original loss function. In summary, we have demonstrated that convex relaxation strategies are well applicable for bundle methods. In fact, the results of the corresponding optimization procedures can be used directly for further optimization steps. A3.1.3 Scalar Multivariate Performance Scores We now discuss a series of structured loss functions and how they can be implemented eciently. For the sake of completeness, we give a concise rep- resentation of previous work on multivariate performance scores and ranking methods. All these loss functions rely on having access to hw;xi, which can be computed eciently by using the same operations as in Section A3.1.1. A3.1.3.1 ROC Score Denote by f=Xwthe vector of function values on the training set. It is well known that the area under the ROC curve is given by AUC(x;y;w ) =1 m+m X yi<yjI(hw;xii<hw;xji); (3.17) wherem+andm are the numbers of positive and negative observations respectively, and I() is indicator function. Directly optimizing the cost 1   AUC(x;y;w ) is dicult as it is not continuous in w. By using max(0 ;1 + hw;xi xji) as the surrogate loss function for all pairs ( i;j) for whichyi<yj we have the following convex multivariate empirical risk Remp(w) =1 m+m X yi<yjmax(0;1 +hw;xi xji) =1 m+m X yi<yjmax(0;1 +fi fj): (3.18) Obviously, we could compute Remp(w) and its derivative by an O(m2) op- eration. However [Joa05] showed that both can be computed in O(mlogm) time using a sorting operation, which we now describe. Denote byc=f 1 2yan auxiliary variable and let iandjbe indices such

210 3 Loss Functions Algorithm 3.2 ROCScore( X;y;w ) 1:input: Feature matrix X, labelsy, and weight vector w 2:initialization: s =m ands+= 0 andl=0mandc=Xw 1 2y 3: f1;:::;mgsorted in ascending order of c 4:fori= 1tomdo 5:ifyi= 1then 6:li s+ands  s  1 7:else 8:li  s ands+ s++ 1 9:end if 10:end for 11:Rescalel l=(m+m ) and compute r=hl;ciandg=l>X. 12:return Riskrand subgradient g thatyi= 1 andyj= 1. It follows that ci cj= 1 +fi fj. The ecient algorithm is due to the observation that there are at most mdistinct terms ck; k= 1;:::;m , each with dierent frequency lkand sign, appear in (3.18). These frequencies lkcan be determined by rst sorting cin ascending order then scanning through the labels according to the sorted order of cand keeping running statistics such as the number s of negative labels yet to encounter, and the number s+of positive labels encountered. When visiting yk, we knowckshould appears s+(ors ) times with positive (or negative) sign in (3.18) if yk= 1 (oryk= 1). Algorithm 3.2 spells out explicitly how to compute Remp(w) and its subgradient. A3.1.3.2 Ordinal Regression Essentially the same preference relationships need to hold for ordinal re- gression. The only dierence is that yineed not take on binary values any more. Instead, we may have an arbitrary number of dierent values yi(e.g., 1 corresponding to 'strong reject' up to 10 corresponding to 'strong accept', when it comes to ranking papers for a conference). That is, we now have yi2f1;:::;ngrather than yi2f 1g. Our goal is to nd some wsuch that hw;xi xji<0 whenever yi< yj. Whenever this relationship is not satis- ed, we incur a cost C(yi;yj) for preferring xitoxj. For examples, C(yi;yj) could be constant i.e.,C(yi;yj) = 1 [Joa06] or linear i.e.,C(yi;yj) =yj yi. Denote by mithe number of xjfor whichyj=i. In this case, there are M=m2 Pn i=1m2 ipairs (yi;yj) for which yi6=yj; this implies that there areM=M=2 pairs (yi;yj) such that yi< yj. Normalizing by the total

A3.1 Loss Functions 211 number of comparisons we may write the overall cost of the estimator as 1 MX yi<yjC(yi;yj)I(hw;xii>hw;xji) whereM=1 2" m2 nX im2 i# :(3.19) Using the same convex majorization as above when we were maximizing the ROC score, we obtain an empirical risk of the form Remp(w) =1 MX yi<yjC(yi;yj) max(0;1 +hw;xi xji) (3.20) Now the goal is to nd an ecient algorithm for obtaining the number of times when the individual losses are nonzero such as to compute both the value and the gradient of Remp(w). The complication arises from the fact that observations xiwith labelyimay appear in either side of the inequality depending on whether yj< yioryj> yi. This problem can be solved as follows: sort f=Xwin ascending order and traverse it while keeping track of how many items with a lower value yjare no more than 1 apart in terms of their value of fi. This way we may compute the count statistics eciently. Algorithm 3.3 describes the details, generalizing the results of [Joa06]. Again, its runtime is O(mlogm), thus allowing for ecient computation. A3.1.3.3 Preference Relations In general, our loss may be described by means of a set of preference relations jifor arbitrary pairs ( i;j)2f1;:::mg2associated with a cost C(i;j) which is incurred whenever iis ranked above j. This set of preferences may or may not form a partial or a total order on the domain of all observations. In these cases ecient computations along the lines of Algorithm 3.3 exist. In general, this is not the case and we need to rely on the fact that the set Pcontaining all preferences is suciently small that it can be enumerated eciently. The risk is then given by 1 jPjX (i;j)2PC(i;j)I(hw;xii>hw;xji) (3.21)

212 3 Loss Functions Algorithm 3.3 OrdinalRegression( X;y;w;C ) 1:input: Feature matrix X, labelsy, weight vector w, and score matrix C 2:initialization: l=0nandui=mi8i2[n] andr= 0 andg=0m 3:Computef=Xwand setc= [f 1 2;f+1 2]2R2m(concatenate the vectors) 4:ComputeM= (m2 Pn i=1m2 i)=2 5:RescaleC C=M 6: f1;:::; 2mgsorted in ascending order of c 7:fori= 1to2mdo 8:j=imodm 9:ifimthen 10: fork= 1toyj 1do 11:r r C(k;yj)ukcj 12:gj gj C(k;yj)uk 13: end for 14:lyj lyj+ 1 15: else 16: fork=yj+ 1tondo 17:r r+C(yj;k)lkcj+m 18:gj gj+C(yj;k)lk 19: end for 20:uyj uyj 1 21: end if 22:end for 23:g g>X 24:return: Riskrand subgradient g Again, the same majorization argument as before allows us to write a convex upper bound Remp(w) =1 jPjX (i;j)2PC(i;j) max (0;1 +hw;xii hw;xji) (3.22) where@wRemp(w) =1 jPjX (i;j)2PC(i;j)( 0 ifhw;xj xii1 xi xjotherwise (3.23) The implementation is straightforward, as given in Algorithm 3.4.

A3.1 Loss Functions 213 Algorithm 3.4 Preference( X;w;C;P ) 1:input: Feature matrix X, weight vector w, score matrix C, and prefer- ence setP 2:initialization: r= 0 andg=0m 3:Computef=Xw 4:while (i;j)2Pdo 5:iffj fi<1then 6:r r+C(i;j)(1 +fi fj) 7:gi gi+C(i;j) andgj gj C(i;j) 8:end if 9:end while 10:g g>X 11:return Riskrand subgradient g A3.1.3.4 Ranking In webpage and document ranking we are often in a situation similar to that described in Section A3.1.3.2, however with the dierence that we do not only care about objects xibeing ranked according to scores yibut moreover that dierent degrees of importance are placed on dierent documents. The information retrieval literature is full with a large number of dier- ent scoring functions. Examples are criteria such as Normalized Discounted Cumulative Gain (NDCG) ,Mean Reciprocal Rank (MRR) ,Precision@n , or Expected Rank Utility (ERU) . They are used to address the issue of evaluat- ing rankers, search engines or recommender sytems [Voo01, JK02, BHK98, BH04]. For instance, in webpage ranking only the rst kretrieved docu- ments that matter, since users are unlikely to look beyond the rst k, say 10, retrieved webpages in an internet search. [LS07] show that these scores can be optimized directly by minimizing the following loss: l(X;y;w ) = max X ici w;x(i) xi +ha a();b(y)i: (3.24) Hereciis a monotonically decreasing sequence, the documents are assumed to be arranged in order of decreasing relevance, is a permutation, the vectorsaandb(y) depend on the choice of a particular ranking measure, and a() denotes the permutation of aaccording to . Pre-computing f=Xw we may rewrite (3.24) as l(f;y) = max h c>f() a()>b(y)i  c>f+a>b(y) (3.25)

214 3 Loss Functions Algorithm 3.5 Ranking(X;y;w ) 1:input: Feature matrix X, relevances y, and weight vector w 2:Compute vectors aandb(y) according to some ranking measure 3:Computef=Xw 4:Compute elements of matrix Cij=cifj biaj 5:= LinearAssignment( C) 6:r=c>(f() f) + (a a())>b 7:g=c( 1) candg g>X 8:return Riskrand subgradient g and consequently the derivative of l(X;y;w ) with respect to wis given by @wl(X;y;w ) = (c( 1) c)>Xwhere = argmax c>f() a()>b(y): (3.26) Here 1denotes the inverse permutation, such that  1= 1. Finding the permutation maximizing c>f() a()>b(y) is a linear assignment problem which can be easily solved by the Hungarian Marriage algorithm, that is, the Kuhn-Munkres algorithm. The original papers by [Kuh55] and [Mun57] implied an algorithm with O(m3) cost in the number of terms. Later, [Kar80] suggested an algorithm with expected quadratic time in the size of the assignment problem (ignor- ing log-factors). Finally, [OL93] propose a linear time algorithm for large problems. Since in our case the number of pages is fairly small (in the order of 50 to 200 per query ) the scaling behavior per query is not too important. We used an existing implementation due to [JV87]. Note also that training sets consist of a collection of ranking problems, that is, we have several ranking problems of size 50 to 200. By means of parallelization we are able to distribute the work onto a cluster of worksta- tions, which is able to overcome the issue of the rather costly computation per collection of queries. Algorithm 3.5 spells out the steps in detail. A3.1.3.5 Contingency Table Scores [Joa05] observed that Fscores and related quantities dependent on a con- tingency table can also be computed eciently by means of structured es- timation. Such scores depend in general on the number of true and false positives and negatives alike. Algorithm 3.6 shows how a corresponding em- pirical risk and subgradient can be computed eciently. As with the pre- vious losses, here again we use convex majorization to obtain a tractable optimization problem.

A3.1 Loss Functions 215 Given a set of labels yand an estimate y0, the numbers of true positives (T+), true negatives ( T ), false positives ( F+), and false negatives ( F ) are determined according to a contingency table as follows: y>0y<0 y0>0T+F+ y0<0F T  In the sequel, we denote by m+=T++F andm =T +F+the numbers of positives and negative labels in y, respectively. We note that Fscore can be computed based on the contingency table [Joa05] as F(T+;T ) =(1 +2)T+ T++m  T +2m+: (3.27) If we want to usehw;xiito estimate the label of observation xi, we may use the following structured loss to \directly" optimize w.r.t. Fscore [Joa05]: l(X;y;w ) = max y0h (y0 y)>f+ (T+;T )i ; (3.28) wheref=Xw, (T+;T ) := 1 F(T+;T ), and (T+;T ) is determined by usingyandy0. Since  does not depend on the specic choice of ( y;y0) but rather just on which sets they disagree, lcan be maximized as follows: Enumerating all possible m+m contingency tables in a way such that given a conguration ( T+;T ),T+(T ) positive (negative) observations xiwith largest (lowest) value of hw;xiiare labeled as positive (negative). This is eectively implemented as a nested loop hence run in O(m2) time. Algorithm 3.6 describes the procedure in details. A3.1.4 Vector Loss Functions Next we discuss \vector" loss functions, i.e.,functions where wis best de- scribed as a matrix (denoted by W) and the loss depends on Wx. Here, we have feature vector x2Rd, labely2Rk, and weight matrix W2Rdk. We also denote feature matrix X2Rmdas a matrix of mfeature vectors xi, and stack up the columns WiofWas a vector w. Some of the most relevant cases are multiclass classication using both the exponential families model and structured estimation, hierarchical mod- els,i.e., ontologies, and multivariate regression. Many of those cases are summarized in Table A3.1.

216 3 Loss Functions Algorithm 3.6 F(X;y;w ) 1:input: Feature matrix X, labelsy, and weight vector w 2:Computef=Xw 3:+ fi:yi= 1gsorted in descending order of f 4:  fi:yi= 1gsorted in ascending order of f 5:Letp0= 0 andpi= 2Pm+ k=if+ k; i= 1;:::;m + 6:Letn0= 0 andni= 2Pm  k=if  k; i= 1;:::;m  7:y0  yandr  1 8:fori= 0tom+do 9:forj= 0tom do 10:rtmp= (i;j) pi+nj 11: ifrtmp>rthen 12:r rtmp 13:T+ iandT  j 14: end if 15: end for 16:end for 17:y0 + i 1; i= 1;:::;T + 18:y0   i  1; i= 1;:::;T  19:g (y0 y)>X 20:return Riskrand subgradient g A3.1.4.1 Unstructured Setting The simplest loss is multivariate regression, where l(x;y;W ) =1 2(y x>W)>M(y  x>W). In this case it is clear that by pre-computing XW subsequent calcu- lations of the loss and its gradient are signicantly accelerated. A second class of important losses is given by plain multiclass classication problems, e.g., recognizing digits of a postal code or categorizing high-level document categories. In this case, (x;y) is best represented by ey x(using a linear model). Clearly we may view hw;(x;y)ias an operation which chooses a column indexed by yfromxW, since all labels ycorrespond to a dierent weight vector Wy. Formally we set hw;(x;y)i= [xW]y. In this case, structured estimation losses can be rewritten as l(x;y;W ) = max y0 (y;y0) Wy0 Wy;x + (y;y0) (3.29) and@Wl(x;y;W ) =  (y;y)(ey ey) x: (3.30) Here   and  are dened as in Section A3.1.2 and ydenotes the value of y0

A3.1 Loss Functions 217 for which the RHS of (3.29) is maximized. This means that for unstructured multiclass settings we may simply compute xW. Since this needs to be per- formed for all observations xiwe may take advantage of fast linear algebra routines and compute f=XW for eciency. Likewise note that comput- ing the gradient over mobservations is now a matrix-matrix multiplication, too: denote by Gthe matrix of rows of gradients  ( yi;y i)(ey i eyi). Then @WRemp(X;y;W ) =G>X. Note that Gis very sparse with at most two nonzero entries per row, which makes the computation of G>Xessentially as expensive as two matrix vector multiplications. Whenever we have many classes, this may yield signicant computational gains. Log-likelihood scores of exponential families share similar expansions. We have l(x;y;W ) = logX y0exp w;(x;y0)  hw;(x;y)i= logX y0exp Wy0;x  hWy;xi (3.31) @Wl(x;y;W ) =P y0(ey0 x) exp Wy0;x P y0exp Wy0;x ey x: (3.32) The main dierence to the soft-margin setting is that the gradients are notsparse in the number of classes. This means that the computation of gradients is slightly more costly. A3.1.4.2 Ontologies Fig. A3.1. Two ontologies. Left: a binary hierarchy with internal nodes f1;:::; 7g and labelsf8;:::15g.Right: a generic directed acyclic graph with internal nodes f1;:::; 6;12gand labelsf7;:::; 11;13;:::; 15g. Note that node 5 has two parents, namely nodes 2 and 3. Moreover, the labels need not be found at the same level of the tree: nodes 14 and 15 are one level lower than the rest of the nodes. Assume that the labels we want to estimate can be found to belong to a directed acyclic graph. For instance, this may be a gene-ontology graph

218 3 Loss Functions [ABB+00] a patent hierarchy [CH04], or a genealogy. In these cases we have a hierarchy of categories to which an element xmay belong. Figure A3.1 gives two examples of such directed acyclic graphs (DAG). The rst example is a binary tree, while the second contains nodes with dierent numbers of children ( e.g., node 4 and 12), nodes at dierent levels having children ( e.g., nodes 5 and 12), and nodes which have more than one parent ( e.g., node 5). It is a well known fundamental property of trees that they have at most as many internal nodes as they have leaf nodes. It is now our goal to build a classier which is able to categorize observa- tions according to which leaf node they belong to (each leaf node is assigned a labely). Denote by k+ 1 the number of nodes in the DAG including the root node. In this case we may design a feature map (y)2Rk[CH04] by associating with every label ythe vector describing the path from the root node toy, ignoring the root node itself. For instance, for the rst DAG in Figure A3.1 we have (8) = (1;0;1;0;0;0;1;0;0;0;0;0;0;0) and(13) = (0;1;0;0;1;0;0;0;0;0;0;1;0;0) Whenever several paths are admissible, as in the right DAG of Figure A3.1 we average over all possible paths. For example, we have (10) = (0:5;0:5;0;1;0;0;0;0;1;0;0;0;0;0) and(15) = (0;1;0;0;1;0;0;0;0;0;0;1;0;0;1): Also note that the lengths of the paths need not be the same ( e.g., to reach 15 it takes a longer path than to reach 13). Likewise, it is natural to assume that ( y;y0),i.e.,the cost for mislabeling yasy0will depend on the similarity of the path. In other words, it is likely that the cost for placing xinto the wrong sub-sub-category is less than getting the main category of the object wrong. To complete the setting, note that for (x;y) =(y) xthe cost of computing all labels is kinner products, since the value of hw;(x;y)ifor a particularycan be obtained by the sum of the contributions for the segments of the path. This means that the values for allterms can be computed by a simple breadth rst traversal through the graph. As before, we may make use of vectorization in our approach, since we may compute xW2Rkto obtain the contributions on all segments of the DAG before performing the graph traversal. Since we have mpatternsxiwe may vectorize matters by pre-computing XW. Also note that (y) (y0) is nonzero only for those edges where the paths foryandy0dier. Hence we only change weights on those parts of the graph where the categorization diers. Algorithm 3.7 describes the subgradient and loss computation for the soft-margin type of loss function.

A3.1 Loss Functions 219 Algorithm 3.7 Ontology(X;y;W ) 1:input: Feature matrix X2Rmd, labelsy, and weight matrix W2 Rdk 2:initialization: G=02Rmkandr= 0 3:Computef=XW and letfi=xiW 4:fori= 1tomdo 5: LetDibe the DAG with edges annotated with the values of fi 6: TraverseDito nd node ythat maximize sum of fivalues on the path plus ( yi;y0) 7:Gi=(y) (yi) 8:r r+zy zyi 9:end for 10:g=G>X 11:return Riskrand subgradient g The same reasoning applies to estimation when using an exponential fam- ilies model. The only dierence is that we need to compute a soft-max over paths rather than exclusively choosing the best path over the ontol- ogy. Again, a breadth-rst recursion suces: each of the leaves yof the DAG is associated with a probability p(yjx). To obtain Eyp(yjx)[(y)] all we need to do is perform a bottom-up traversal of the DAG summing over all probability weights on the path. Wherever a node has more than one parent, we distribute the probability weight equally over its parents.

Bibliography [ABB+00] M. Ashburner, C. A. Ball, J. A. Blake, D. Botstein, H. Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S. Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E. Richard- son, M. Ringwald, G. M. Rubin, and G. Sherlock, Gene ontology: tool for the unication of biology. the gene ontology consortium , Nat Genet 25(2000), 25{ 29. [AGML90] S. F. Altschul, W. Gish, E. W. Myers, and D. J. Lipman, Basic local alignment search tool , Journal of Molecular Biology 215 (1990), no. 3, 403{ 410. [BBL05] O. Bousquet, S. Boucheron, and G. Lugosi, Theory of classication: a sur- vey of recent advances , ESAIM: Probab. Stat. 9(2005), 323{ 375. [BCR84] C. Berg, J. P. R. Christensen, and P. Ressel, Harmonic analysis on semi- groups , Springer, New York, 1984. [BDEL03] S. Ben-David, N. Eiron, and P.M. Long, On the diculty of approximately maximizing agreements , J. Comput. System Sci. 66(2003), no. 3, 496{514. [Bel61] R. E. Bellman, Adaptive control processes , Princeton University Press, Princeton, NJ, 1961. [Bel05] Alexandre Belloni, Introduction to bundle methods , Tech. report, Operation Research Center, M.I.T., 2005. [Ber85] J. O. Berger, Statistical decision theory and Bayesian analysis , Springer, New York, 1985. [BH04] J. Basilico and T. Hofmann, Unifying collaborative and content-based lter- ing, Proc. Intl. Conf. Machine Learning (New York, NY), ACM Press, 2004, pp. 65{72. [BHK98] J. S. Breese, D. Heckerman, and C. Kardie, Empirical analysis of predictive algorithms for collaborative ltering , Proceedings of the 14th Conference on Uncertainty in Articial Intelligence, 1998, pp. 43{52. [BHS+07] G. Bakir, T. Hofmann, B. Sch olkopf, A. Smola, B. Taskar, and S. V. N. Vishwanathan, Predicting structured data , MIT Press, Cambridge, Mas- sachusetts, 2007. [Bil68] Patrick Billingsley, Convergence of probability measures , John Wiley and Sons, 1968. [Bis95] C. M. Bishop, Neural networks for pattern recognition , Clarendon Press, Oxford, 1995. [BK07] R. M. Bell and Y. Koren, Lessons from the netix prize challenge , SIGKDD Explorations 9(2007), no. 2, 75{79. [BKL06] A. Beygelzimer, S. Kakade, and J. Langford, Cover trees for nearest neigh- bor, International Conference on Machine Learning, 2006. [BL00] J. M. Borwein and A. S. Lewis, Convex analysis and nonlinear optimization: Theory and examples , CMS books in Mathematics, Canadian Mathematical Society, 2000. 221

222 3 Bibliography [BM92] K. P. Bennett and O. L. Mangasarian, Robust linear programming discrimi- nation of two linearly inseparable sets , Optim. Methods Softw. 1(1992), 23{34. [BNJ03] D. Blei, A. Ng, and M. Jordan, Latent Dirichlet allocation , Journal of Ma- chine Learning Research 3(2003), 993{1022. [BT03] D.P. Bertsekas and J.N. Tsitsiklis, Introduction to probability , Athena Sci- entic, 2003. [BV04] S. Boyd and L. Vandenberghe, Convex optimization , Cambridge University Press, Cambridge, England, 2004. [CDLS99] R. Cowell, A. Dawid, S. Lauritzen, and D. Spiegelhalter, Probabilistic networks and expert sytems , Springer, New York, 1999. [CH04] Lijuan Cai and T. Hofmann, Hierarchical document categorization with sup- port vector machines , Proceedings of the Thirteenth ACM conference on Infor- mation and knowledge management (New York, NY, USA), ACM Press, 2004, pp. 78{87. [Cra46] H. Cram er, Mathematical methods of statistics , Princeton University Press, 1946. [Cre93] N. A. C. Cressie, Statistics for spatial data , John Wiley and Sons, New York, 1993. [CS03] K. Crammer and Y. Singer, Ultraconservative online algorithms for multi- class problems , Journal of Machine Learning Research 3(2003), 951{991. [CSS00] M. Collins, R. E. Schapire, and Y. Singer, Logistic regression, AdaBoost and Bregman distances , Proc. 13th Annu. Conference on Comput. Learning Theory, Morgan Kaufmann, San Francisco, 2000, pp. 158{169. [CV95] Corinna Cortes and V. Vapnik, Support vector networks , Machine Learning 20(1995), no. 3, 273{297. [DG03] S. Dasgupta and A. Gupta, An elementary proof of a theorem of johnson and lindenstrauss , Random Struct. Algorithms 22(2003), no. 1, 60{65. [DG08] J. Dean and S. Ghemawat, MapReduce: simplied data processing on large clusters , CACM 51(2008), no. 1, 107{113. [DGL96] L. Devroye, L. Gy or, and G. Lugosi, A probabilistic theory of pattern recognition , Applications of mathematics, vol. 31, Springer, New York, 1996. [Fel71] W. Feller, An introduction to probability theory and its applications , 2 ed., John Wiley and Sons, New York, 1971. [FJ95] A. Frieze and M. Jerrum, An analysis of a monte carlo algorithm for esti- mating the permanent , Combinatorica 15(1995), no. 1, 67{83. [FS99] Y. Freund and R. E. Schapire, Large margin classication using the percep- tron algorithm , Machine Learning 37(1999), no. 3, 277{296. [FT94] L. Fahrmeir and G. Tutz, Multivariate statistical modelling based on gener- alized linear models , Springer, 1994. [GIM99] A. Gionis, P. Indyk, and R. Motwani, Similarity search in high dimensions via hashing , Proceedings of the 25th VLDB Conference (Edinburgh, Scotland) (M. P. Atkinson, M. E. Orlowska, P. Valduriez, S. B. Zdonik, and M. L. Brodie, eds.), Morgan Kaufmann, 1999, pp. 518{529. [GS04] T.L. Griths and M. Steyvers, Finding scientic topics , Proceedings of the National Academy of Sciences 101(2004), 5228{5235. [GW92] P. Groeneboom and J. A. Wellner, Information bounds and nonparametric maximum likelihood estimation , DMV, vol. 19, Springer, 1992. [Hal92] P. Hall, The bootstrap and edgeworth expansions , Springer, New York, 1992. [Hay98] S. Haykin, Neural networks : A comprehensive foundation , Macmillan, New York, 1998, 2nd edition.

Bibliography 223 [Heb49] D. O. Hebb, The organization of behavior , John Wiley and Sons, New York, 1949. [Hoe63] W. Hoeding, Probability inequalities for sums of bounded random variables , Journal of the American Statistical Association 58(1963), 13{30. [HUL93] J.B. Hiriart-Urruty and C. Lemar echal, Convex analysis and minimization algorithms, I and II , vol. 305 and 306, Springer-Verlag, 1993. [IM98] P. Indyk and R. Motawani, Approximate nearest neighbors: Towards remov- ing the curse of dimensionality , Proceedings of the 30thSymposium on Theory of Computing, 1998, pp. 604{613. [JK02] K. Jarvelin and J. Kekalainen, IR evaluation methods for retrieving highly relevant documents , ACM Special Interest Group in Information Retrieval (SI- GIR), New York: ACM, 2002, pp. 41{48. [Joa05] T. Joachims, A support vector method for multivariate performance mea- sures , Proc. Intl. Conf. Machine Learning (San Francisco, California), Morgan Kaufmann Publishers, 2005, pp. 377{384. [Joa06] ,Training linear SVMs in linear time , Proc. ACM Conf. Knowledge Discovery and Data Mining (KDD), ACM, 2006. [Jor08] M. I. Jordan, An introduction to probabilistic graphical models , MIT Press, 2008, To Appear. [JV87] R. Jonker and A. Volgenant, A shortest augmenting path algorithm for dense and sparse linear assignment problems , Computing 38(1987), 325{340. [Kar80] R.M. Karp, An algorithm to solve the mnassignment problem in expected timeO(mnlogn), Networks 10(1980), no. 2, 143{152. [KD05] S. S. Keerthi and D. DeCoste, A modied nite Newton method for fast solution of large scale linear SVMs , J. Mach. Learn. Res. 6(2005), 341{361. [Kel60] J. E. Kelly, The cutting-plane method for solving convex programs , Journal of the Society for Industrial and Applied Mathematics 8(1960), no. 4, 703{712. [Kiw90] Krzysztof C. Kiwiel, Proximity control in bundle methods for convex non- dierentiable minimization , Mathematical Programming 46(1990), 105{122. [KM00] Paul Komarek and Andrew Moore, A dynamic adaptation of AD-trees for ecient machine learning on large data sets , Proc. Intl. Conf. Machine Learn- ing, Morgan Kaufmann, San Francisco, CA, 2000, pp. 495{502. [Koe05] R. Koenker, Quantile regression , Cambridge University Press, 2005. [Kuh55] H.W. Kuhn, The Hungarian method for the assignment problem , Naval Re- search Logistics Quarterly 2(1955), 83{97. [Lew98] D. D. Lewis, Naive (Bayes) at forty: The independence assumption in in- formation retrieval , Proceedings of ECML-98, 10th European Conference on Machine Learning (Chemnitz, DE) (C. N edellec and C. Rouveirol, eds.), no. 1398, Springer Verlag, Heidelberg, DE, 1998, pp. 4{15. [LK03] C. Leslie and R. Kuang, Fast kernels for inexact string matching , Proc. Annual Conf. Computational Learning Theory, 2003. [LMP01] J. D. Laerty, A. McCallum, and F. Pereira, Conditional random elds: Probabilistic modeling for segmenting and labeling sequence data , Proceedings of International Conference on Machine Learning (San Francisco, CA), vol. 18, Morgan Kaufmann, 2001, pp. 282{289. [LNN95] Claude Lemar echal, Arkadii Nemirovskii, and Yurii Nesterov, New variants of bundle methods , Mathematical Programming 69(1995), 111{147. [LS07] Q. Le and A.J. Smola, Direct optimization of ranking measures , J. Mach. Learn. Res. (2007), submitted. [LT92] Z. Q. Luo and P. Tseng, On the convergence of coordinate descent method

224 3 Bibliography for convex dierentiable minimization , Journal of Optimization Theory and Applications 72(1992), no. 1, 7{35. [Lue84] D. G. Luenberger, Linear and nonlinear programming , second ed., Addison- Wesley, Reading, May 1984. [Mar61] M.E. Maron, Automatic indexing: An experimental inquiry , Journal of the Association for Computing Machinery 8(1961), 404{417. [McA07] David McAllester, Generalization bounds and consistency for structured labeling , Predicting Structured Data (Cambridge, Massachusetts), MIT Press, 2007. [McD89] C. McDiarmid, On the method of bounded dierences , Survey in Combina- torics, Cambridge University Press, 1989, pp. 148{188. [Mit97] T. M. Mitchell, Machine learning , McGraw-Hill, New York, 1997. [MN83] P. McCullagh and J. A. Nelder, Generalized linear models , Chapman and Hall, London, 1983. [MSR+97] K.-R. M uller, A. J. Smola, G. R atsch, B. Sch olkopf, J. Kohlmorgen, and V. Vapnik, Predicting time series with support vector machines , Articial Neu- ral Networks ICANN'97 (Berlin) (W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, eds.), Lecture Notes in Comput. Sci., vol. 1327, Springer-Verlag, 1997, pp. 999{1004. [Mun57] J. Munkres, Algorithms for the assignment and transportation problems , Journal of SIAM 5(1957), no. 1, 32{38. [MYA94] N. Murata, S. Yoshizawa, and S. Amari, Network information criterion | determining the number of hidden units for articial neural network models , IEEE Transactions on Neural Networks 5(1994), 865{872. [Nad65] E. A. Nadaraya, On nonparametric estimates of density functions and re- gression curves , Theory of Probability and its Applications 10(1965), 186{190. [NW99] J. Nocedal and S. J. Wright, Numerical optimization , Springer Series in Operations Research, Springer, 1999. [OL93] J.B. Orlin and Y. Lee, Quickmatch: A very fast algorithm for the assignment problem , Working Paper 3547-93, Sloan School of Management, Massachusetts Institute of Technology, Cambridge, MA, March 1993. [Pap62] A. Papoulis, The fourier integral and its applications , McGraw-Hill, New York, 1962. [Pla99] J. Platt, Fast training of support vector machines using sequential minimal optimization , Advances in Kernel Methods | Support Vector Learning (Cam- bridge, MA) (B. Sch olkopf, C. J. C. Burges, and A. J. Smola, eds.), MIT Press, 1999, pp. 185{208. [PTVF94] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery, Numerical recipes in c. the art of scientic computation , Cambridge University Press, Cambridge, UK, 1994. [Rao73] C. R. Rao, Linear statistical inference and its applications , John Wiley and Sons, New York, 1973. [RBZ06] N. Ratli, J. Bagnell, and M. Zinkevich, Maximum margin planning , Inter- national Conference on Machine Learning, July 2006. [Ros58] F. Rosenblatt, The perceptron: A probabilistic model for information storage and organization in the brain , Psychological Review 65(1958), no. 6, 386{408. [RPB06] M. Richardson, A. Prakash, and E. Brill, Beyond pagerank: machine learn- ing for static ranking , Proceedings of the 15th international conference on World Wide Web, WWW (L. Carr, D. De Roure, A. Iyengar, C.A. Goble, and M. Dahlin, eds.), ACM, 2006, pp. 707{715.

Bibliography 225 [RSS+07] G. R atsch, S. Sonnenburg, J. Srinivasan, H. Witte, K.-R. M uller, R. J. Sommer, and B. Sch olkopf, Improving the Caenorhabditis elegans genome an- notation using machine learning , PLoS Computational Biology 3(2007), no. 2, e20 doi:10.1371/journal.pcbi.0030020. [Rud73] W. Rudin, Functional analysis , McGraw-Hill, New York, 1973. [Sil86] B. W. Silverman, Density estimation for statistical and data analysis , Mono- graphs on statistics and applied probability, Chapman and Hall, London, 1986. [SPST+01] B. Sch olkopf, J. Platt, J. Shawe-Taylor, A. J. Smola, and R. C. Williamson, Estimating the support of a high-dimensional distribution , Neu- ral Comput. 13(2001), no. 7, 1443{1471. [SS02] B. Sch olkopf and A. Smola, Learning with kernels , MIT Press, Cambridge, MA, 2002. [SW86] G.R. Shorack and J.A. Wellner, Empirical processes with applications to statistics , Wiley, New York, 1986. [SZ92] Helga Schramm and Jochem Zowe, A version of the bundle idea for minimiz- ing a nonsmooth function: Conceptual idea, convergence analysis, numerical results , SIAM J. Optimization 2(1992), 121{152. [TGK04] B. Taskar, C. Guestrin, and D. Koller, Max-margin Markov networks , Advances in Neural Information Processing Systems 16 (Cambridge, MA) (S. Thrun, L. Saul, and B. Sch olkopf, eds.), MIT Press, 2004, pp. 25{32. [TJHA05] I. Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun, Large margin methods for structured and interdependent output variables , J. Mach. Learn. Res.6(2005), 1453{1484. [Vap82] V. Vapnik, Estimation of dependences based on empirical data , Springer, Berlin, 1982. [Vap95] ,The nature of statistical learning theory , Springer, New York, 1995. [Vap98] ,Statistical learning theory , John Wiley and Sons, New York, 1998. [vdG00] S. van de Geer, Empirical processes in M-estimation , Cambridge University Press, 2000. [vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical processes , Springer, 1996. [VGS97] V. Vapnik, S. Golowich, and A. J. Smola, Support vector method for func- tion approximation, regression estimation, and signal processing , Advances in Neural Information Processing Systems 9 (Cambridge, MA) (M. C. Mozer, M. I. Jordan, and T. Petsche, eds.), MIT Press, 1997, pp. 281{287. [Voo01] E. Voorhees, Overview of the TRECT 2001 question answering track , TREC, 2001. [VS04] S. V. N. Vishwanathan and A. J. Smola, Fast kernels for string and tree matching , Kernel Methods in Computational Biology (Cambridge, MA) (B. Sch olkopf, K. Tsuda, and J. P. Vert, eds.), MIT Press, 2004, pp. 113{130. [VSV07] S. V. N. Vishwanathan, A. J. Smola, and R. Vidal, Binet-Cauchy kernels on dynamical systems and its application to the analysis of dynamic scenes , International Journal of Computer Vision 73(2007), no. 1, 95{119. [Wah97] G. Wahba, Support vector machines, reproducing kernel Hilbert spaces and the randomized GACV , Tech. Report 984, Department of Statistics, University of Wisconsin, Madison, 1997. [Wat64] G. S. Watson, Smooth regression analysis , Sankhya A 26(1964), 359{372. [Wil98] C. K. I. Williams, Prediction with Gaussian processes: From linear regression to linear prediction and beyond , Learning and Inference in Graphical Models (M. I. Jordan, ed.), Kluwer Academic, 1998, pp. 599{621.

226 3 Bibliography [WJ03] M. J. Wainwright and M. I. Jordan, Graphical models, exponential fami- lies, and variational inference , Tech. Report 649, UC Berkeley, Department of Statistics, September 2003.